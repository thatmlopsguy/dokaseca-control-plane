{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"\"Just as ships are built in dry docks, platforms are crafted in DoKa Seca\""},{"location":"#introduction","title":"\ud83d\udc4b Introduction","text":"<p>Welcome to DoKa Seca - a comprehensive framework for bootstrapping cloud-native platforms using Kubernetes in Docker (Kind)! The name \"DoKa Seca\" is a playful Portuguese phrase where \"DoKa\" incorporates the \"K\" from Kubernetes (representing the containerized orchestration at the heart of this project), and \"Seca\" means \"dry\" - drawing inspiration from the concept of a dry dock.</p> <p>Just as ships are built, repaired, and maintained in dry docks - controlled, isolated environments where all the necessary infrastructure and tooling are readily available - DoKa Seca provides a \"dry dock\" for Kubernetes platforms. It creates an isolated, controlled environment where entire cloud-native platforms can be rapidly assembled, configured, and tested before being deployed to production waters.</p> <p>DoKa Seca provides an opinionated, production-ready framework that automates the entire platform bootstrap process using Kind clusters. Rather than just being a collection of configurations, it's a complete platform engineering solution that provisions infrastructure, installs essential tooling, configures GitOps workflows, and sets up observability - all with a single command, in your local \"dry dock\" environment.</p> <p>This project serves as both a personal learning journey into modern DevOps practices and a comprehensive resource for platform engineers and developers interested in rapidly spinning up production-grade Kubernetes environments. Here you'll find real-world implementations of GitOps workflows, infrastructure as code, observability stacks, and cloud-native security practices - all designed to run efficiently in local development or homelab environments while following enterprise-grade patterns and best practices.</p> <p>Prerequisites</p> <ul> <li>Docker</li> <li>Terraform or opentofu</li> <li>Kind, k0s and/or k3d</li> <li>jq</li> <li>Helm</li> <li>Kubectl</li> <li>kustomize</li> <li>k9s or freelens (optional, if you'd like to inspect your cluster visually)</li> </ul> <p>Optional tools</p> <ul> <li>argocd</li> <li>vcluster</li> <li>falcoctl</li> <li>karmor</li> <li>clusteradm</li> <li>cosign</li> <li>velero</li> <li>vault</li> <li>minio client (mc)</li> </ul> <pre><code>$ kubectl version\nClient Version: v1.31.0\nKustomize Version: v5.4.2\nServer Version: v1.30.0\n\n$ kind version\nkind v0.27.0 go1.23.6 linux/amd64\n\n$ k3d --version\nk3d version v5.8.3\nk3s version v1.31.5-k3s1 (default)\n\n$ k0s version\nv1.32.4+k0s.0\n\n$ helm version\nversion.BuildInfo{Version:\"v3.16.1\", GitCommit:\"v3.16.1\", GitTreeState:\"\", GoVersion:\"go1.22.7\"}\n</code></pre>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<pre><code># Deploy control plane cluster\n./scripts/terraform.sh hub dev apply\n</code></pre> <p>You can inspect the deployed clusters by typing:</p> <pre><code>$ kind get clusters\nhub-dev\nspoke-dev\nspoke-prod\nspoke-stg\n</code></pre> <p>If you enable in <code>terraform.tfvars</code> the gitops bridge by setting <code>enable_gitops_bridge = true</code>, then argocd will be also installed and all the enabled addons. You can see that terraform will add GitOps Bridge Metadata to the ArgoCD secret. The annotations contain metadata for the addons' Helm charts and ArgoCD ApplicationSets.</p> <pre><code>kubectl get secret -n argocd -l argocd.argoproj.io/secret-type=cluster -o json | jq '.items[0].metadata.annotations'\n</code></pre> <p>The output looks like the following:</p> <pre><code>{\n  \"addons_extras_repo_basepath\": \"stable\",\n  \"addons_extras_repo_revision\": \"main\",\n  \"addons_extras_repo_url\": \"https://github.com/thatmlopsguy/helm-charts\",\n  \"addons_repo_basepath\": \"argocd\",\n  \"addons_repo_path\": \"appsets\",\n  \"addons_repo_revision\": \"main\",\n  \"addons_repo_url\": \"https://github.com/thatmlopsguy/dokaseca-addons\",\n  \"cluster_name\": \"hub-dev\",\n  \"cluster_repo_basepath\": \"argocd\",\n  \"cluster_repo_path\": \"clusters\",\n  \"cluster_repo_revision\": \"dev\",\n  \"cluster_repo_url\": \"https://github.com/thatmlopsguy/dokaseca-clusters\",\n  \"environment\": \"dev\",\n  \"workload_repo_basepath\": \"argocd\",\n  \"workload_repo_path\": \"workloads\",\n  \"workload_repo_revision\": \"dev\",\n  \"workload_repo_url\": \"https://github.com/thatmlopsguy/dokaseca-workloads\"\n}\n</code></pre> <p>The labels offer a straightforward way to enable or disable an addon in ArgoCD for the cluster.</p> <pre><code>kubectl get secret -n argocd -l argocd.argoproj.io/secret-type=cluster -o json | jq '.items[0].metadata.labels'\n</code></pre> <p>The output looks like the following:</p> <pre><code>{\n  \"argocd.argoproj.io/secret-type\": \"cluster\",\n  \"cloud_provider\": \"local\",\n  \"cluster_name\": \"hub-dev\",\n  \"enable_alloy\": \"false\",\n  \"enable_argo_cd\": \"true\",\n  \"enable_argo_cd_image_updater\": \"false\",\n  \"enable_argo_events\": \"false\",\n  \"enable_argo_rollouts\": \"false\",\n  \"enable_argo_workflows\": \"false\",\n  \"enable_trivy\": \"false\",\n  \"enable_vault\": \"false\",\n  \"enable_vcluster\": \"false\",\n  \"enable_vector\": \"false\",\n  \"enable_victoria_metrics_k8s_stack\": \"true\",\n  \"enable_zipkin\": \"false\",\n  \"environment\": \"dev\",\n  \"k8s_cluster_name\": \"hub-dev\",\n  \"k8s_domain_name\": \"dokaseca.local\",\n  \"kubernetes_version\": \"1.31.2\"\n}\n</code></pre>"},{"location":"#destroy-infrastructure","title":"\ud83d\udca5 Destroy Infrastructure","text":"<p>To tear down all the resources and the kind cluster(s), run the following command:</p> <pre><code>make clean-infra\n</code></pre>"},{"location":"#faq","title":"\u2692\ufe0f FAQ","text":"<p><code>ERROR: failed to create cluster: could not find a log line that matches \"Reached target .*Multi-User System.*|detected cgroup v1\"</code></p> <p>To increase these limits temporarily run the following commands on the host:</p> <pre><code>sudo sysctl fs.inotify.max_user_watches=1048576\nsudo sysctl fs.inotify.max_user_instances=8192\n</code></pre> <p>Source: Pod errors due to \u201ctoo many open files\u201d</p>"},{"location":"#resources","title":"\ud83d\udcda Resources","text":"<p>User documentation can be found on our user docs site.</p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>All contributors are warmly welcome. If you want to become a new contributor, we are so happy! Just, before doing it, read our contributing guidelines.</p>"},{"location":"#roadmap","title":"\ud83d\uddfa\ufe0f Roadmap","text":"<p>Want to know about the features to come? Check out the project roadmap for more information.</p>"},{"location":"#license","title":"\ud83d\udd16 License","text":"<p>DoKa Seca is licensed under Apache License, Version 2.0</p>"},{"location":"#note","title":"\u26a0\ufe0f Note","text":"<p>DoKa Seca is still in relatively early development. At this time, do not use Doka Seca for critical production systems.</p>"},{"location":"backup/","title":"Backup","text":""},{"location":"backup/#velero-kubernetes-backup-restore","title":"Velero: Kubernetes Backup &amp; Restore","text":"<p>Velero is an open source tool to back up and restore Kubernetes cluster resources and persistent volumes. It supports public cloud platforms and on-premises environments.</p>"},{"location":"backup/#key-features","title":"Key Features","text":"<ul> <li>Take scheduled or on-demand backups of your cluster and restore in case of loss</li> <li>Migrate cluster resources to other clusters</li> <li>Replicate production clusters to development/testing</li> <li>Back up both Kubernetes resources and persistent volumes</li> </ul>"},{"location":"backup/#installation","title":"Installation","text":""},{"location":"backup/#cli","title":"CLI","text":"<p>Install the Velero CLI:</p> <pre><code>curl -L https://github.com/vmware-tanzu/velero/releases/latest/download/velero-linux-amd64.tar.gz | tar -xz\nsudo mv velero-linux-amd64/velero /usr/local/bin/\n</code></pre>"},{"location":"backup/#helm-recommended","title":"Helm (Recommended)","text":"<pre><code>helm repo add vmware-tanzu https://vmware-tanzu.github.io/helm-charts\nhelm repo update\n</code></pre>"},{"location":"backup/#storage-configuration","title":"Storage Configuration","text":"<p>Velero supports S3-compatible storage (AWS S3, MinIO, etc). Example: MinIO (self-hosted S3):</p> <ol> <li>Deploy MinIO</li> <li>Create a bucket for Velero backups</li> <li>Create a Kubernetes secret with your S3 credentials:</li> </ol> <pre><code>kubectl create secret generic velero-creds \\\n  --namespace velero \\\n  --from-literal=aws_access_key_id=&lt;MINIO_ACCESS_KEY&gt; \\\n  --from-literal=aws_secret_access_key=&lt;MINIO_SECRET_KEY&gt;\n</code></pre> <ol> <li>Install Velero with Helm:</li> </ol> <pre><code>helm install velero vmware-tanzu/velero \\\n  --namespace velero --create-namespace \\\n  --set configuration.provider=aws \\\n  --set configuration.backupStorageLocation.name=default \\\n  --set configuration.backupStorageLocation.bucket=&lt;YOUR_BUCKET&gt; \\\n  --set configuration.backupStorageLocation.config.region=minio \\\n  --set configuration.backupStorageLocation.config.s3ForcePathStyle=true \\\n  --set configuration.backupStorageLocation.config.s3Url=http://minio.minio.svc:9000 \\\n  --set credentials.existingSecret=velero-creds\n</code></pre>"},{"location":"backup/#usage","title":"Usage","text":""},{"location":"backup/#create-a-backup","title":"Create a Backup","text":"<pre><code>velero backup create my-backup --include-namespaces my-namespace\n</code></pre>"},{"location":"backup/#list-backups","title":"List Backups","text":"<pre><code>velero backup get\n</code></pre>"},{"location":"backup/#restore-from-backup","title":"Restore from Backup","text":"<pre><code>velero restore create --from-backup my-backup\n</code></pre>"},{"location":"backup/#schedule-regular-backups","title":"Schedule Regular Backups","text":"<pre><code>velero schedule create daily-backup --schedule=\"0 2 * * *\" --include-namespaces my-namespace\n</code></pre>"},{"location":"backup/#references","title":"References","text":"<ul> <li>Velero Documentation</li> </ul>"},{"location":"catalog/","title":"Catalog","text":"<p>The Catalog is a library of curated Helm charts to create Kubernetes resources via gitops bridge using argocd.</p> Category Tool Namespace Description URL (where applicable) Portal Backstage backstage Auth Provider Keycloak Keycloak CI Tool GitHub Actions - CD Tool ArgoCD argocd GitOps Continuous Delivery CD Tool Argo Rollouts argo-rollouts Progressive Delivery Tool CD Tool Kargo kargo GitOps Continuous Promotion CD Tool GitOps promoter promoter-system Facilitates environment promotion for config managed via GitOps Object Storage Minio minio High performance Object Storage compatible with Amazon S3 cloud storage service Security Cert Manager cert-manager Security External Secrets external-secrets Security Vault vault Security Trivy trivy-system Compliance Kyverno kyverno Observability Grafana monitoring Visualize metrics, logs, and traces from multiple sources Observability Loki monitoring Collecting container application logs Observability Victoria Logs monitoring Observability Otel collector monitoring Observability alloy monitoring Observability fluentbit monitoring Observability vector monitoring Observability Victoria Metrics monitoring Observability Prometheus monitoring Observability Jaeger monitoring Observability Zipkin monitoring Observability Kiali monitoring Compliance kyverno kyverno Kubernetes native policy management Networking MetalLB metallb-system Networking Kube-vip kube-system Networking Nginx Ingress Controller ingress-nginx Networking Traefik Ingress Controller traefik Networking ngrok Networking Istio istio-system The service mesh framework with end-to-end transit encryption Chaos Engineering Chaos Mesh chaos-mesh Chaos Engineering Litmus litmus Disaster Recovery Velero velero"},{"location":"chaos_engineering/","title":"Chaos Engineering","text":"<p>DoKa Seca provides comprehensive chaos engineering capabilities to help teams build resilient cloud-native applications and validate system reliability through controlled failure injection. The platform integrates with industry-leading chaos engineering tools to enable systematic resilience testing.</p>"},{"location":"chaos_engineering/#overview","title":"Overview","text":"<p>Chaos engineering is the practice of experimenting on a system to build confidence in its capability to withstand turbulent conditions in production. DoKa Seca implements chaos engineering through two powerful platforms:</p> <ul> <li>Chaos Mesh - Cloud-native chaos engineering platform for Kubernetes</li> <li>Litmus - Open-source chaos engineering framework</li> </ul> <p>These tools enable teams to proactively identify weaknesses in their systems before they manifest as outages in production.</p>"},{"location":"chaos_engineering/#chaos-engineering-principles","title":"Chaos Engineering Principles","text":"<p>DoKa Seca's chaos engineering implementation follows core principles:</p> <ol> <li>Hypothesis-Driven: Define clear hypotheses about system behavior under failure conditions</li> <li>Production-Safe: Minimize blast radius and impact on real users</li> <li>Automated: Integrate chaos experiments into CI/CD pipelines</li> <li>Measurable: Monitor system behavior and collect meaningful metrics</li> <li>Gradual: Start with small experiments and gradually increase complexity</li> </ol>"},{"location":"chaos_engineering/#chaos-mesh-integration","title":"Chaos Mesh Integration","text":"<p>Chaos Mesh is DoKa Seca's primary chaos engineering platform, providing a rich set of fault injection capabilities specifically designed for Kubernetes environments.</p>"},{"location":"chaos_engineering/#key-features","title":"Key Features","text":"<ul> <li>Native Kubernetes Integration: Chaos experiments defined as Kubernetes CRDs</li> <li>Rich Fault Types: Network, Pod, IO, Kernel, and Time chaos experiments</li> <li>Web Dashboard: Intuitive interface for managing and monitoring experiments</li> <li>Safety Controls: Built-in safeguards to prevent excessive damage</li> <li>Observability: Integration with monitoring systems for experiment tracking</li> </ul>"},{"location":"chaos_engineering/#supported-chaos-experiments","title":"Supported Chaos Experiments","text":""},{"location":"chaos_engineering/#pod-chaos","title":"Pod Chaos","text":"<ul> <li>PodKill: Randomly kill pods to test recovery mechanisms</li> <li>PodFailure: Make pods fail for specified durations</li> <li>Container Kill: Target specific containers within pods</li> </ul>"},{"location":"chaos_engineering/#network-chaos","title":"Network Chaos","text":"<ul> <li>Network Partition: Simulate network splits and partitions</li> <li>Network Delay: Introduce network latency</li> <li>Network Loss: Simulate packet loss</li> <li>Network Corrupt: Corrupt network packets</li> <li>Bandwidth Limit: Restrict network bandwidth</li> </ul>"},{"location":"chaos_engineering/#stress-testing","title":"Stress Testing","text":"<ul> <li>Stress CPU: Generate CPU load on target pods</li> <li>Stress Memory: Consume memory resources</li> <li>Stress IO: Generate disk I/O stress</li> </ul>"},{"location":"chaos_engineering/#time-chaos","title":"Time Chaos","text":"<ul> <li>Time Skew: Modify system time on target pods</li> <li>Clock Drift: Simulate clock synchronization issues</li> </ul>"},{"location":"chaos_engineering/#example-chaos-mesh-experiment","title":"Example Chaos Mesh Experiment","text":"<pre><code>apiVersion: chaos-mesh.org/v1alpha1\nkind: PodChaos\nmetadata:\n  name: pod-kill-example\n  namespace: chaos-engineering\nspec:\n  action: pod-kill\n  mode: fixed\n  value: \"1\"\n  selector:\n    namespaces:\n      - production\n    labelSelectors:\n      app: web-service\n  duration: \"30s\"\n  scheduler:\n    cron: \"@every 10m\"\n</code></pre>"},{"location":"chaos_engineering/#litmus-integration","title":"Litmus Integration","text":"<p>Litmus complements Chaos Mesh by providing a comprehensive chaos engineering framework with extensive experiment libraries and GitOps integration.</p>"},{"location":"chaos_engineering/#litmus-features","title":"Litmus Features","text":"<ul> <li>ChaosHub: Extensive library of pre-built chaos experiments</li> <li>Workflow Engine: Orchestrate complex chaos scenarios</li> <li>GitOps Support: Version-controlled chaos experiments</li> <li>Multi-Cloud: Support for various cloud providers and platforms</li> <li>Analytics: Advanced experiment analytics and reporting</li> </ul>"},{"location":"chaos_engineering/#litmus-components","title":"Litmus Components","text":""},{"location":"chaos_engineering/#chaoscenter","title":"ChaosCenter","text":"<ul> <li>Centralized Management: Single pane of glass for chaos experiments</li> <li>User Management: Role-based access control</li> <li>Team Collaboration: Multi-tenant experiment management</li> <li>Visualization: Real-time experiment monitoring and analytics</li> </ul>"},{"location":"chaos_engineering/#chaoshub","title":"ChaosHub","text":"<ul> <li>Experiment Library: Ready-to-use chaos experiments</li> <li>Custom Experiments: Create and share custom chaos scenarios</li> <li>Community Contributions: Access to community-maintained experiments</li> <li>Version Control: Git-based experiment management</li> </ul>"},{"location":"chaos_engineering/#chaosengine","title":"ChaosEngine","text":"<ul> <li>Experiment Execution: Kubernetes-native experiment runner</li> <li>Safety Controls: Steady-state hypothesis validation</li> <li>Result Analysis: Automated verdict generation</li> <li>Integration: Works with existing monitoring and alerting systems</li> </ul>"},{"location":"chaos_engineering/#example-litmus-experiment","title":"Example Litmus Experiment","text":"<pre><code>apiVersion: litmuschaos.io/v1alpha1\nkind: ChaosEngine\nmetadata:\n  name: nginx-chaos\n  namespace: default\nspec:\n  engineState: 'active'\n  appinfo:\n    appns: 'default'\n    applabel: 'app=nginx'\n    appkind: 'deployment'\n  chaosServiceAccount: litmus-admin\n  experiments:\n    - name: pod-delete\n      spec:\n        components:\n          env:\n            - name: TOTAL_CHAOS_DURATION\n              value: '30'\n            - name: CHAOS_INTERVAL\n              value: '10'\n            - name: FORCE\n              value: 'false'\n</code></pre>"},{"location":"chaos_engineering/#doka-seca-implementation","title":"DoKa Seca Implementation","text":""},{"location":"chaos_engineering/#installation-and-setup","title":"Installation and Setup","text":"<p>DoKa Seca includes both Chaos Mesh and Litmus as optional components that can be enabled during platform bootstrap by configuring Terraform variables:</p> <pre><code># terraform/tfvars/dev.tfvars\nenable_chaos_mesh = true\nenable_litmus = true\n</code></pre>"},{"location":"chaos_engineering/#integration-with-platform-components","title":"Integration with Platform Components","text":""},{"location":"chaos_engineering/#gitops-integration","title":"GitOps Integration","text":"<ul> <li>Chaos experiments stored in Git repositories</li> <li>ArgoCD manages experiment lifecycle</li> <li>Version-controlled experiment definitions</li> <li>Automated experiment deployment</li> </ul>"},{"location":"chaos_engineering/#observability-integration","title":"Observability Integration","text":"<ul> <li>Grafana dashboards for chaos experiment monitoring</li> <li>Prometheus metrics collection during experiments</li> <li>Alert integration for experiment failures</li> <li>Victoria Metrics for long-term experiment data storage</li> </ul>"},{"location":"chaos_engineering/#security-integration","title":"Security Integration","text":"<ul> <li>Kyverno policies for chaos experiment validation</li> <li>RBAC controls for experiment execution</li> <li>Admission controllers for safety enforcement</li> <li>Cosign verification for experiment artifacts</li> </ul>"},{"location":"chaos_engineering/#best-practices","title":"Best Practices","text":""},{"location":"chaos_engineering/#experiment-design","title":"Experiment Design","text":"<ol> <li>Start Small: Begin with low-impact experiments</li> <li>Clear Hypotheses: Define expected system behavior</li> <li>Monitoring: Ensure comprehensive observability</li> <li>Safety Nets: Implement automatic experiment termination</li> <li>Documentation: Record experiment procedures and results</li> </ol>"},{"location":"chaos_engineering/#safety-measures","title":"Safety Measures","text":"<ul> <li>Blast Radius Control: Limit experiment scope</li> <li>Time Bounds: Set maximum experiment duration</li> <li>Health Checks: Monitor system health during experiments</li> <li>Rollback Procedures: Define quick recovery mechanisms</li> <li>Approval Workflows: Require approvals for high-impact experiments</li> </ul>"},{"location":"chaos_engineering/#team-practices","title":"Team Practices","text":"<ul> <li>Regular Game Days: Schedule chaos engineering exercises</li> <li>Cross-Team Collaboration: Include development and operations teams</li> <li>Learning Culture: Focus on learning from experiments</li> <li>Continuous Improvement: Iterate based on experiment results</li> <li>Knowledge Sharing: Document and share findings</li> </ul>"},{"location":"chaos_engineering/#getting-started","title":"Getting Started","text":""},{"location":"chaos_engineering/#prerequisites","title":"Prerequisites","text":"<ul> <li>DoKa Seca platform deployed and configured</li> <li>Kubernetes cluster with appropriate permissions</li> <li>Monitoring and observability stack operational</li> <li>Team training on chaos engineering principles</li> </ul>"},{"location":"chaos_engineering/#basic-workflow","title":"Basic Workflow","text":"<ol> <li>Hypothesis Formation: Define what you expect to happen</li> <li>Experiment Design: Create chaos experiment specifications</li> <li>Safety Review: Validate safety controls and blast radius</li> <li>Execution: Run the experiment in a controlled manner</li> <li>Observation: Monitor system behavior during the experiment</li> <li>Analysis: Analyze results and validate hypothesis</li> <li>Documentation: Record findings and lessons learned</li> <li>Iteration: Improve systems based on learnings</li> </ol>"},{"location":"chaos_engineering/#example-scenarios","title":"Example Scenarios","text":""},{"location":"chaos_engineering/#microservices-resilience","title":"Microservices Resilience","text":"<ul> <li>Test service mesh fault tolerance</li> <li>Validate circuit breaker functionality</li> <li>Verify graceful degradation patterns</li> <li>Test load balancing under failures</li> </ul>"},{"location":"chaos_engineering/#data-layer-testing","title":"Data Layer Testing","text":"<ul> <li>Database failover scenarios</li> <li>Cache invalidation testing</li> <li>Storage failure simulation</li> <li>Backup and recovery validation</li> </ul>"},{"location":"chaos_engineering/#infrastructure-resilience","title":"Infrastructure Resilience","text":"<ul> <li>Node failure simulation</li> <li>Network partition testing</li> <li>Resource exhaustion scenarios</li> <li>Security policy validation</li> </ul>"},{"location":"chaos_engineering/#monitoring-and-metrics","title":"Monitoring and Metrics","text":""},{"location":"chaos_engineering/#key-metrics-to-track","title":"Key Metrics to Track","text":"<ul> <li>System Availability: Uptime during chaos experiments</li> <li>Recovery Time: Time to restore normal operations</li> <li>Error Rates: Application error rates during experiments</li> <li>Performance Impact: Latency and throughput changes</li> <li>Resource Utilization: CPU, memory, and storage usage</li> </ul>"},{"location":"compliance/","title":"Compliance","text":"<p>DoKa Seca implements a comprehensive compliance framework using Kyverno for policy enforcement and Cosign for supply chain security. This combination provides automated policy validation, resource mutation, and container image signing to ensure workloads meet security and compliance requirements throughout the platform lifecycle.</p> <p>Warning</p> <p>Documentation coming soon!</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for taking the time to make a contribution to this project!</p> <p>The following document provides guidelines and instructions to help you contribute effectively.</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":"<ul> <li>Discuss your idea: Before making changes, please open an issue to discuss your proposed contribution.</li> <li>Fork the repository: Create a personal fork and branch for your work.</li> <li>Follow code style: Match the formatting and conventions used in the project.</li> <li>Write clear commit messages: Use descriptive and concise commit messages.</li> <li>Test your changes: Ensure your changes do not break existing functionality. Add tests if possible.</li> <li>Keep pull requests focused: Submit small, focused pull requests for easier review.</li> </ul>"},{"location":"contributing/#devops-tooling","title":"DevOps &amp; Tooling","text":"<ul> <li>Automation:</li> <li>Use provided Makefile targets or scripts in the <code>scripts/</code> directory for common tasks.</li> <li>Secrets &amp; Credentials:</li> <li>Never commit secrets, credentials, or sensitive data to the repository.</li> <li>Use environment variables or secret management tools as described in the documentation.</li> </ul>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":"<ol> <li>Fork the repository and create your branch from <code>main</code>.</li> <li>Make your changes, following the guidelines above.</li> <li>Test your changes locally.</li> <li>Submit a pull request with a clear description of your changes and reference any related issues.</li> </ol>"},{"location":"contributing/#need-help","title":"Need Help?","text":"<p>If you have questions, open an issue or start a discussion. We're happy to help!</p> <p>Thank you for helping make this project better!</p>"},{"location":"cost/","title":"Cost Management","text":"<p>DoKa Seca integrates Kubecost (also known as OpenCost) to provide comprehensive cost visibility and management for Kubernetes workloads. Kubecost offers real-time cost allocation, resource optimization recommendations, and budget alerts to help teams understand and control their infrastructure spending across the platform.</p> <p>Warning</p> <p>Documentation coming soon!</p>"},{"location":"database/","title":"Database Setup for DoKa Seca Platform Services","text":"<p>This document outlines the database configuration for platform services in the DoKa Seca project.</p>"},{"location":"database/#overview","title":"Overview","text":"<p>The platform uses a single PostgreSQL database container with multiple databases for different platform services:</p> <ul> <li>Keycloak: Identity and access management</li> <li>DevLake: DevOps metrics platform</li> <li>Temporal: Workflow orchestration</li> <li>Backstage: Developer portal</li> <li>LiteLLM: LLM proxy service</li> <li>Langfuse: LLM observability service</li> </ul>"},{"location":"database/#database-structure","title":"Database Structure","text":"<p>Each service has:</p> <ul> <li>Its own dedicated database</li> <li>A unique user with appropriate permissions</li> <li>Password-based authentication</li> </ul>"},{"location":"database/#connection-information","title":"Connection Information","text":"Service Database Username Default Password Connection String Keycloak keycloak keycloak keycloak_password jdbc:postgresql://postgres:5432/keycloak DevLake devlake devlake devlake_password jdbc:postgresql://postgres:5432/devlake Temporal temporal temporal temporal_password jdbc:postgresql://postgres:5432/temporal Backstage backstage backstage backstage_password jdbc:postgresql://postgres:5432/backstage LiteLLM litellm litellm litellm_password jdbc:postgresql://postgres:5432/litellm Langfuse langfuse langfuse langfuse_password jdbc:postgresql://postgres:5432/langfuse"},{"location":"database/#configuration","title":"Configuration","text":"<p>Database credentials are configured via environment variables in the <code>.env</code> file. Copy <code>.env.example</code> to <code>.env</code> and adjust as needed:</p> <pre><code>cp .env.example .env\n</code></pre>"},{"location":"database/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>PG_USER</code> &amp; <code>PG_PASSWORD</code>: PostgreSQL admin credentials</li> <li>Service-specific credentials:</li> <li><code>KC_DB_USERNAME</code> &amp; <code>KC_DB_PASSWORD</code> (Keycloak)</li> <li><code>DL_DB_USERNAME</code> &amp; <code>DL_DB_PASSWORD</code> (DevLake)</li> <li>etc.</li> </ul>"},{"location":"database/#accessing-postgresql","title":"Accessing PostgreSQL","text":""},{"location":"database/#via-command-line","title":"Via Command Line","text":"<p>Connect to the PostgreSQL container:</p> <pre><code>docker compose exec postgres psql -U postgres\n</code></pre> <p>List databases:</p> <pre><code>\\l\n</code></pre> <p>Connect to a specific database:</p> <pre><code>\\c keycloak\n</code></pre>"},{"location":"database/#via-external-tool","title":"Via External Tool","text":"<p>Connect using your preferred database client:</p> <ul> <li>Host: localhost</li> <li>Port: 5432</li> <li>User/Password: As specified in .env file</li> </ul>"},{"location":"database/#integration-with-vault","title":"Integration with Vault","text":"<p>Database credentials are stored in Vault following the pattern described in the secrets documentation:</p> <pre><code>vault/platform/postgres/common/backstage\nvault/platform/postgres/common/keycloak\nvault/platform/postgres/common/devlake\nvault/platform/postgres/common/litellm\nvault/platform/postgres/common/langfuse\n</code></pre>"},{"location":"database/#backup-and-recovery","title":"Backup and Recovery","text":"<p>Database data is persisted in the <code>postgres-data</code> Docker volume.</p>"},{"location":"documentation/","title":"Documentation","text":"<p>DoKa Seca uses MkDocs with the Material theme to generate and publish comprehensive platform documentation. This guide provides detailed instructions for configuring, writing, and deploying documentation for the DoKa Seca platform.</p>"},{"location":"documentation/#overview","title":"Overview","text":"<p>The documentation system in DoKa Seca provides:</p> <ul> <li>Static Site Generation: Uses MkDocs to convert Markdown files into a static website</li> <li>Material Design: Clean, responsive theme with dark/light mode support</li> <li>Automated Deployment: GitHub Actions workflow for automatic publishing to GitHub Pages</li> <li>Live Development: Local development server with hot reloading</li> <li>Enhanced Markdown: Support for advanced features like tabs, code blocks, and diagrams</li> </ul>"},{"location":"documentation/#architecture","title":"Architecture","text":"<p>The documentation system consists of several key components:</p> <pre><code>docs/                          # Documentation source files\n\u251c\u2500\u2500 index.md                   # Homepage\n\u251c\u2500\u2500 getting_started/           # Getting started guides\n\u251c\u2500\u2500 adr/                      # Architecture Decision Records\n\u2514\u2500\u2500 *.md                      # Feature documentation\n\nmkdocs.yaml                   # MkDocs configuration\nrequirements/docs.txt         # Python dependencies\n.github/workflows/docs.yml    # Automated deployment\n</code></pre>"},{"location":"documentation/#prerequisites","title":"Prerequisites","text":"<p>Before working with documentation, ensure you have:</p> <ul> <li>Python 3.13 or higher</li> <li>Git</li> <li>Text editor or IDE</li> <li>Basic Markdown knowledge</li> </ul>"},{"location":"documentation/#local-development","title":"Local Development","text":""},{"location":"documentation/#quick-start","title":"Quick Start","text":"<ol> <li>Install dependencies using the provided Makefile:</li> </ol> <pre><code>make docs-install\n</code></pre> <ol> <li>Start the development server:</li> </ol> <pre><code>make docs-serve\n</code></pre> <ol> <li>Access the documentation at <code>http://localhost:8000</code></li> </ol> <p>The development server provides live reloading, so changes to Markdown files will be automatically reflected in your browser.</p>"},{"location":"documentation/#manual-installation","title":"Manual Installation","text":"<p>If you prefer manual installation:</p> <ol> <li>Create a virtual environment:</li> </ol> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre> <ol> <li>Install dependencies:</li> </ol> <pre><code>pip install -U pip\npip install -r requirements/docs.txt\n</code></pre> <ol> <li>Start the server:</li> </ol> <pre><code>mkdocs serve\n</code></pre>"},{"location":"documentation/#writing-documentation","title":"Writing Documentation","text":""},{"location":"documentation/#file-organization","title":"File Organization","text":"<ul> <li>Place new documentation files in the <code>docs/</code> directory</li> <li>Use descriptive filenames with lowercase and hyphens (e.g., <code>cluster-api.md</code>)</li> <li>Organize related content in subdirectories (e.g., <code>getting_started/</code>)</li> <li>Follow the existing naming conventions</li> </ul>"},{"location":"documentation/#markdown-guidelines","title":"Markdown Guidelines","text":"<p>DoKa Seca documentation uses extended Markdown with the following features:</p>"},{"location":"documentation/#code-blocks","title":"Code Blocks","text":"<p>Use fenced code blocks with language specification:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: example\ndata:\n  key: value\n</code></pre>"},{"location":"documentation/#admonitions","title":"Admonitions","text":"<p>Use admonitions for important information:</p> <pre><code>!!! note \"Installation Note\"\n    This requires administrative privileges.\n\n!!! warning \"Security Warning\"\n    Never commit secrets to version control.\n\n!!! tip \"Pro Tip\"\n    Use the Makefile for consistent commands.\n</code></pre>"},{"location":"documentation/#tabs","title":"Tabs","text":"<p>Group related content using tabs:</p> <pre><code>=== \"Kubernetes\"\n    ```yaml\n    apiVersion: apps/v1\n    kind: Deployment\n    ```\n\n=== \"Docker\"\n    ```bash\n    docker run nginx\n    ```\n</code></pre>"},{"location":"documentation/#tables","title":"Tables","text":"<p>Use standard Markdown tables:</p> <pre><code>| Component | Purpose    | Port |\n|-----------|------------|------|\n| Grafana   | Monitoring | 3000 |\n| ArgoCD    | GitOps     | 8080 |\n</code></pre>"},{"location":"documentation/#navigation-structure","title":"Navigation Structure","text":"<p>Update the navigation in <code>mkdocs.yaml</code> when adding new pages:</p> <pre><code>nav:\n  - Overview: index.md\n  - Getting Started:\n    - Installation: getting_started/install.md\n    - Your New Page: getting_started/new-page.md\n</code></pre>"},{"location":"documentation/#page-metadata","title":"Page Metadata","text":"<p>Include frontmatter for better organization:</p> <pre><code>---\ntitle: Page Title\ndescription: Brief description of the page content\n---\n\n# Page Title\n\nContent starts here...\n</code></pre>"},{"location":"documentation/#configuration","title":"Configuration","text":""},{"location":"documentation/#mkdocs-configuration","title":"MkDocs Configuration","text":"<p>The main configuration is in <code>mkdocs.yaml</code>:</p> <ul> <li>Site Information: Name, description, and repository links</li> <li>Navigation: Page hierarchy and organization  </li> <li>Theme: Material theme configuration with color schemes</li> <li>Extensions: Enhanced Markdown features</li> <li>Plugins: Additional functionality</li> </ul> <p>Key configuration sections:</p> <pre><code># Site metadata\nsite_name: Doka Seca\nsite_description: A comprehensive framework for bootstrapping cloud-native platforms\n\n# Theme configuration\ntheme:\n  name: material\n  palette:\n    - scheme: default      # Light mode\n    - scheme: slate        # Dark mode\n\n# Markdown extensions\nmarkdown_extensions:\n  - pymdownx.superfences  # Enhanced code blocks\n  - pymdownx.tabbed      # Tab support\n  - pymdownx.details     # Collapsible sections\n</code></pre>"},{"location":"documentation/#dependencies","title":"Dependencies","text":"<p>Documentation dependencies are managed in <code>requirements/docs.txt</code>:</p> <ul> <li><code>mkdocs</code>: Core static site generator</li> <li><code>mkdocs-material</code>: Material Design theme</li> <li><code>mkdocs-material-extensions</code>: Additional theme features</li> </ul>"},{"location":"documentation/#deployment","title":"Deployment","text":""},{"location":"documentation/#automatic-deployment","title":"Automatic Deployment","text":"<p>DoKa Seca uses GitHub Actions for automatic documentation deployment:</p> <ol> <li>Trigger: Pushes to the <code>main</code> branch automatically trigger deployment</li> <li>Build: GitHub Actions installs dependencies and builds the site</li> <li>Deploy: The built site is deployed to GitHub Pages</li> <li>Access: Documentation is available at the configured GitHub Pages URL</li> </ol> <p>The workflow (<code>.github/workflows/docs.yml</code>) handles:</p> <ul> <li>Python environment setup</li> <li>Dependency installation</li> <li>Site building with <code>mkdocs gh-deploy</code></li> <li>Caching for faster builds</li> </ul>"},{"location":"documentation/#manual-deployment","title":"Manual Deployment","text":"<p>For manual deployment:</p> <pre><code># Build the site locally\nmkdocs build\n\n# Deploy to GitHub Pages\nmkdocs gh-deploy --force\n</code></pre>"},{"location":"documentation/#build-output","title":"Build Output","text":"<p>The built documentation is generated in the <code>site/</code> directory:</p> <pre><code>site/\n\u251c\u2500\u2500 index.html\n\u251c\u2500\u2500 getting_started/\n\u251c\u2500\u2500 assets/\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"documentation/#best-practices","title":"Best Practices","text":""},{"location":"documentation/#content-guidelines","title":"Content Guidelines","text":"<ol> <li>Clear Structure: Use descriptive headings and logical organization</li> <li>Consistent Style: Follow established patterns and terminology</li> <li>Code Examples: Include practical, working examples</li> <li>Cross-References: Link to related documentation sections</li> <li>Regular Updates: Keep documentation current with platform changes</li> </ol>"},{"location":"documentation/#writing-style","title":"Writing Style","text":"<ul> <li>Use active voice and clear, concise language</li> <li>Include prerequisites and assumptions</li> <li>Provide step-by-step instructions</li> <li>Use consistent terminology across documents</li> <li>Include troubleshooting sections where appropriate</li> </ul>"},{"location":"documentation/#version-control","title":"Version Control","text":"<ul> <li>Make atomic commits for documentation changes</li> <li>Use descriptive commit messages</li> <li>Review changes in pull requests</li> <li>Test locally before committing</li> </ul>"},{"location":"documentation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"documentation/#common-issues","title":"Common Issues","text":"<p>MkDocs server won't start:</p> <ul> <li>Check Python version (3.7+ required)</li> <li>Verify virtual environment activation</li> <li>Ensure dependencies are installed: <code>make docs-install</code></li> </ul> <p>Navigation not updating:</p> <ul> <li>Check <code>mkdocs.yaml</code> syntax</li> <li>Verify file paths in navigation section</li> <li>Restart the development server</li> </ul> <p>Build failures:</p> <ul> <li>Check Markdown syntax</li> <li>Verify image and link paths</li> <li>Review MkDocs logs for specific errors</li> </ul> <p>GitHub Pages not updating:</p> <ul> <li>Check GitHub Actions workflow status</li> <li>Verify repository settings for GitHub Pages</li> <li>Ensure proper branch permissions</li> </ul>"},{"location":"documentation/#getting-help","title":"Getting Help","text":"<ul> <li>Check MkDocs documentation</li> <li>Review Material theme docs</li> <li>Examine existing DoKa Seca documentation for patterns</li> <li>Open issues in the project repository for platform-specific questions</li> </ul>"},{"location":"documentation/#contributing","title":"Contributing","text":"<p>When contributing to documentation:</p> <ol> <li>Fork the repository</li> <li>Create a feature branch for your changes</li> <li>Write or update documentation following these guidelines</li> <li>Test locally using <code>make docs-serve</code></li> <li>Submit a pull request with clear description of changes</li> </ol> <p>See the Contributing Guide for detailed contribution workflows and standards.</p>"},{"location":"gitops/","title":"GitOps","text":"<p>DoKa Seca implements a comprehensive GitOps strategy that provides automated deployment, promotion, and lifecycle management for cloud-native applications. The platform integrates multiple GitOps tools to create a robust, scalable, and secure deployment pipeline that follows GitOps principles and best practices.</p>"},{"location":"gitops/#overview","title":"Overview","text":"<p>GitOps is a operational framework that takes DevOps best practices used for application development such as version control, collaboration, compliance, and CI/CD, and applies them to infrastructure automation. DoKa Seca's GitOps implementation provides:</p> <ul> <li>Declarative Infrastructure: All infrastructure and application configurations stored in Git</li> <li>Automated Deployments: Continuous deployment based on Git repository changes</li> <li>Progressive Delivery: Sophisticated promotion workflows across environments</li> <li>Security and Compliance: Policy-driven deployments with audit trails</li> <li>Observability: Comprehensive monitoring and alerting for deployment pipelines</li> </ul>"},{"location":"gitops/#gitops-bridge-architecture","title":"GitOps Bridge Architecture","text":"<p>The GitOps Bridge is a key component that enables automated deployment and management of both cluster addons and workloads using ArgoCD.</p> <p>Here's how it works:</p> <pre><code>graph TB\n    subgraph \"Infrastructure Layer\"\n        TF[Terraform]\n        k8s[k8s Cluster]\n        TF --&gt; k8s\n    end\n\n    subgraph \"GitOps Bridge Bootstrap\"\n        GB[GitOps Bridge Module]\n        ArgoCD[ArgoCD Installation]\n        GB --&gt; ArgoCD\n    end\n\n    subgraph \"Git Repositories\"\n        AddonsRepo[Addons Repository]\n        WorkloadsRepo[Workloads Repository]\n        ClustersRepo[Clusters Repository]\n    end\n\n    subgraph \"ArgoCD Applications\"\n        AddonsAppSet[Addons ApplicationSet]\n        WorkloadsAppSet[Workloads ApplicationSet]\n        ClustersAppSet[Clusters ApplicationSet]\n    end\n\n    subgraph \"Kubernetes Resources\"\n        Addons[Cluster Addons&lt;br/&gt;- Metrics Server&lt;br/&gt;- Nginx Controller&lt;br/&gt;- etc.]\n        Workloads[Application Workloads&lt;br/&gt;- Team A&lt;br/&gt;- Team B&lt;br/&gt;- Team C]\n        Clusters[Application Clusters&lt;br/&gt;- dev&lt;br/&gt;- stg&lt;br/&gt;- prod]\n    end\n\n    TF --&gt; GB\n    k8s --&gt; ArgoCD\n\n    ArgoCD --&gt; AddonsAppSet\n    ArgoCD --&gt; WorkloadsAppSet\n    ArgoCD --&gt; ClustersAppSet\n\n    AddonsAppSet --&gt; AddonsRepo\n    WorkloadsAppSet --&gt; WorkloadsRepo\n    ClustersAppSet --&gt; ClustersRepo\n\n    AddonsAppSet --&gt; Addons\n    WorkloadsAppSet --&gt; Workloads\n    ClustersAppSet --&gt; Clusters\n\n    AddonsRepo -.-&gt;|Watches for changes| AddonsAppSet\n    WorkloadsRepo -.-&gt;|Watches for changes| WorkloadsAppSet\n    ClustersRepo -.-&gt;|Watches for changes| ClustersAppSet\n\n    classDef terraform fill:#623CE4,stroke:#333,stroke-width:2px,color:#fff\n    classDef gitops fill:#FF6B35,stroke:#333,stroke-width:2px,color:#fff\n    classDef argocd fill:#00D2FF,stroke:#333,stroke-width:2px,color:#fff\n    classDef k8s fill:#326CE5,stroke:#333,stroke-width:2px,color:#fff\n\n    class TF terraform\n    class GB,AddonsRepo,WorkloadsRepo,ClustersRepo gitops\n    class ArgoCD,AddonsAppSet,WorkloadsAppSet,ClustersAppSet argocd\n    class k8s,Addons,Workloads,Clusters k8s</code></pre>"},{"location":"gitops/#gitops-bridge-workflow","title":"GitOps Bridge Workflow","text":"<ol> <li>Bootstrap Phase:</li> <li>Terraform deploys the kubernetes cluster</li> <li>GitOps Bridge module installs ArgoCD on the cluster</li> <li> <p>Creates initial ApplicationSets for addons and workloads</p> </li> <li> <p>Addons Management:</p> </li> <li><code>addons.yaml</code> ApplicationSet monitors the addons repository</li> <li>Automatically deploys cluster-level components (metrics-server, Metallb, etc.)</li> <li> <p>Uses cluster annotations to determine which addons to deploy</p> </li> <li> <p>Workloads Management:</p> </li> <li><code>workloads.yaml</code> ApplicationSet monitors the workloads repository</li> <li>Deploys application workloads (Team A, Team-b, Team-c)</li> <li> <p>Supports environment-specific configurations</p> </li> <li> <p>Clusters Management:</p> </li> <li><code>clusters.yaml</code> ApplicationSet monitors the clusters repository</li> <li> <p>Deploys application clusters (dev, stg, prod)</p> </li> <li> <p>Continuous Sync:</p> </li> <li>ArgoCD continuously monitors Git repositories for changes</li> <li>Automatically applies updates to the cluster</li> <li>Provides drift detection and self-healing capabilities</li> </ol>"},{"location":"gitops/#gitops-architecture","title":"GitOps Architecture","text":"<p>DoKa Seca implements GitOps through a multi-tool approach that provides comprehensive coverage of the deployment lifecycle:</p> <pre><code>graph TB\n    Git[Git Repository] --&gt; ArgoCD[ArgoCD]\n    Git --&gt; Kargo[Kargo]\n    Git --&gt; Promoter[GitOps Promoter]\n\n    ArgoCD --&gt; ImageUpdater[ArgoCD Image Updater]\n    ImageUpdater --&gt; Registry[Container Registry]\n\n    subgraph \"Promotion Pipeline\"\n        Dev[Development] --&gt; Kargo\n        Kargo --&gt; Staging[Staging]\n        Kargo --&gt; Prod[Production]\n    end\n\n    subgraph \"Deployment Engine\"\n        ArgoCD --&gt; K8s[Kubernetes Clusters]\n        Promoter --&gt; K8s\n    end\n\n    Registry --&gt; ImageUpdater\n    Kargo --&gt; ArgoCD\n    Promoter --&gt; ArgoCD</code></pre>"},{"location":"gitops/#core-gitops-components-for-team-workloads","title":"Core GitOps Components for Team Workloads","text":""},{"location":"gitops/#argocd-continuous-deployment","title":"ArgoCD - Continuous Deployment","text":"<p>ArgoCD serves as the primary GitOps continuous deployment tool in DoKa Seca:</p>"},{"location":"gitops/#argocd-features","title":"ArgoCD Features","text":"<ul> <li>Application Deployment: Declarative GitOps application management</li> <li>Multi-Cluster Support: Deploy to multiple Kubernetes clusters</li> <li>RBAC Integration: Role-based access control for teams</li> <li>Sync Policies: Automated and manual synchronization strategies</li> <li>Health Monitoring: Application health status and drift detection</li> </ul>"},{"location":"gitops/#argocd-configuration-example","title":"ArgoCD Configuration Example","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: web-application\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/org/dokaseca-workloads\n    targetRevision: HEAD\n    path: applications/web-app\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: production\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n    - CreateNamespace=true\n</code></pre>"},{"location":"gitops/#argocd-image-updater-automated-image-updates","title":"ArgoCD Image Updater - Automated Image Updates","text":"<p>ArgoCD Image Updater automates container image updates in GitOps workflows:</p>"},{"location":"gitops/#image-updater-features","title":"Image Updater Features","text":"<ul> <li>Automatic Image Updates: Monitor registries for new image versions</li> <li>Semantic Versioning: Intelligent version constraint handling</li> <li>Git Integration: Automated Git commits for image updates</li> <li>Webhook Support: Trigger updates via registry webhooks</li> <li>Multi-Registry Support: Works with various container registries</li> </ul>"},{"location":"gitops/#image-updater-configuration-example","title":"Image Updater Configuration Example","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: api-service\n  annotations:\n    argocd-image-updater.argoproj.io/image-list: api=ghcr.io/org/api-service\n    argocd-image-updater.argoproj.io/api.update-strategy: semver\n    argocd-image-updater.argoproj.io/api.allow-tags: regexp:^v[0-9]+\\.[0-9]+\\.[0-9]+$\n    argocd-image-updater.argoproj.io/api.ignore-tags: latest,main,develop\n    argocd-image-updater.argoproj.io/write-back-method: git:secret:argocd/git-creds\nspec:\n  # ... application specification\n</code></pre>"},{"location":"gitops/#kargo-progressive-delivery","title":"Kargo - Progressive Delivery","text":"<p>Kargo provides sophisticated promotion workflows and progressive delivery capabilities:</p>"},{"location":"gitops/#key-features","title":"Key Features","text":"<ul> <li>Multi-Stage Promotions: Complex promotion pipelines across environments</li> <li>Approval Workflows: Human and automated approval gates</li> <li>Rollback Capabilities: Safe rollback to previous versions</li> <li>Canary Deployments: Gradual traffic shifting strategies</li> <li>Integration Points: Works with ArgoCD and other GitOps tools</li> </ul>"},{"location":"gitops/#configuration-example","title":"Configuration Example","text":"<pre><code>apiVersion: kargo.akuity.io/v1alpha1\nkind: Project\nmetadata:\n  name: web-application\nspec:\n  promotionPolicies:\n  - stage: staging\n    autoPromotionEnabled: true\n  - stage: production\n    autoPromotionEnabled: false\n---\napiVersion: kargo.akuity.io/v1alpha1\nkind: Stage\nmetadata:\n  name: production\n  namespace: web-application\nspec:\n  subscriptions:\n    upstreamStages:\n    - name: staging\n  promotionMechanisms:\n    gitRepoUpdates:\n    - repoURL: https://github.com/org/dokaseca-workloads\n      writeBranch: main\n      kustomize:\n        images:\n        - image: ghcr.io/org/web-app\n          path: applications/web-app/production\n</code></pre>"},{"location":"gitops/#gitops-promoter-environment-promotion","title":"GitOps Promoter - Environment Promotion","text":"<p>GitOps Promoter handles automated promotion between environments:</p>"},{"location":"gitops/#key-features_1","title":"Key Features","text":"<ul> <li>Environment Progression: Automated promotion through environment stages</li> <li>Policy-Based Promotion: Promotion rules and constraints</li> <li>Integration: Works seamlessly with ArgoCD applications</li> <li>Audit Trail: Complete promotion history and rollback capabilities</li> <li>Custom Promotion Logic: Extensible promotion strategies</li> </ul>"},{"location":"gitops/#configuration-example_1","title":"Configuration Example","text":"<pre><code>apiVersion: promoter.argoproj.io/v1alpha1\nkind: PromotionStrategy\nmetadata:\n  name: web-app-promotion\nspec:\n  environments:\n  - name: development\n    branch: main\n    autoPromote: true\n  - name: staging\n    branch: staging\n    autoPromote: true\n    promotionPolicies:\n    - name: health-check\n      successCondition: \"app.status.health.status == 'Healthy'\"\n  - name: production\n    branch: production\n    autoPromote: false\n    approvalRequired: true\n    promotionPolicies:\n    - name: staging-success\n      successCondition: \"staging.status == 'Succeeded'\"\n    - name: security-scan\n      successCondition: \"securityScan.passed == true\"\n</code></pre>"},{"location":"gitops/#doka-seca-gitops-implementation","title":"DoKa Seca GitOps Implementation","text":""},{"location":"gitops/#multi-repository-gitops-structure","title":"Multi-Repository GitOps Structure","text":"<p>DoKa Seca implements GitOps across multiple repositories for separation of concerns:</p>"},{"location":"gitops/#control-plane-repository","title":"Control Plane Repository","text":"<ul> <li>Platform infrastructure configurations</li> <li>ArgoCD application definitions</li> <li>Bootstrap configurations</li> <li>Cross-environment policies</li> </ul>"},{"location":"gitops/#addons-repository","title":"Addons Repository","text":"<ul> <li>Platform addon ApplicationSets</li> <li>Addon-specific configurations</li> <li>Environment-specific value overrides</li> <li>Addon promotion workflows</li> </ul>"},{"location":"gitops/#workloads-repository","title":"Workloads Repository","text":"<ul> <li>Application deployment manifests</li> <li>Application-specific configurations</li> <li>Environment progression definitions</li> <li>Application promotion pipelines</li> </ul>"},{"location":"gitops/#clusters-repository","title":"Clusters Repository","text":"<ul> <li>Cluster-specific configurations</li> <li>Environment definitions</li> <li>Resource quotas and limits</li> <li>Cluster policies and governance</li> </ul>"},{"location":"gitops/#gitops-promotion-workflows","title":"GitOps Promotion Workflows","text":"<p>DoKa Seca implements sophisticated promotion workflows using the integrated toolchain:</p>"},{"location":"gitops/#development-to-staging-promotion","title":"Development to Staging Promotion","text":"<ol> <li>Image Update Detection: ArgoCD Image Updater monitors for new images</li> <li>Automatic Deployment: New images automatically deployed to development</li> <li>Health Validation: Automated health checks and validation</li> <li>Kargo Promotion: Automatic promotion to staging upon successful validation</li> <li>GitOps Promoter: Updates staging environment configurations</li> </ol>"},{"location":"gitops/#staging-to-production-promotion","title":"Staging to Production Promotion","text":"<ol> <li>Manual Approval: Human approval required for production promotion</li> <li>Security Validation: Additional security scans and compliance checks</li> <li>Kargo Orchestration: Controlled promotion with rollback capabilities</li> <li>Progressive Deployment: Canary or blue-green deployment strategies</li> <li>Monitoring Integration: Continuous monitoring during promotion</li> </ol>"},{"location":"gitops/#configuration-examples","title":"Configuration Examples","text":""},{"location":"gitops/#argocd-applicationset-for-multi-environment-deployment","title":"ArgoCD ApplicationSet for Multi-Environment Deployment","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: web-application-environments\n  namespace: argocd\nspec:\n  generators:\n  - clusters:\n      selector:\n        matchLabels:\n          environment: web-app\n  template:\n    metadata:\n      name: 'web-app-{{name}}'\n      annotations:\n        argocd-image-updater.argoproj.io/image-list: app=ghcr.io/org/web-app\n        argocd-image-updater.argoproj.io/app.update-strategy: semver\n        argocd-image-updater.argoproj.io/app.allow-tags: 'regexp:^v[0-9]+\\.[0-9]+\\.[0-9]+$'\n    spec:\n      project: default\n      source:\n        repoURL: https://github.com/org/dokaseca-workloads\n        targetRevision: HEAD\n        path: 'applications/web-app/{{metadata.labels.environment}}'\n      destination:\n        server: '{{server}}'\n        namespace: web-app\n      syncPolicy:\n        automated:\n          prune: true\n          selfHeal: true\n</code></pre>"},{"location":"gitops/#kargo-project-configuration","title":"Kargo Project Configuration","text":"<pre><code>apiVersion: kargo.akuity.io/v1alpha1\nkind: Project\nmetadata:\n  name: platform-services\nspec:\n  promotionPolicies:\n  - stage: development\n    autoPromotionEnabled: true\n  - stage: staging\n    autoPromotionEnabled: true\n    requirements:\n    - healthChecks: true\n    - securityScans: true\n  - stage: production\n    autoPromotionEnabled: false\n    requirements:\n    - manualApproval: true\n    - stagingSuccess: true\n    - complianceCheck: true\n</code></pre>"},{"location":"gitops/#integration-with-platform-components","title":"Integration with Platform Components","text":""},{"location":"gitops/#observability-integration","title":"Observability Integration","text":"<p>GitOps operations are fully integrated with DoKa Seca's observability stack:</p> <ul> <li>Grafana Dashboards: GitOps deployment metrics and trends</li> <li>Prometheus Metrics: ArgoCD, Kargo, and promotion metrics</li> <li>Alert Manager: Automated alerts for deployment failures</li> <li>Victoria Metrics: Long-term storage of GitOps metrics</li> </ul>"},{"location":"gitops/#security-integration","title":"Security Integration","text":"<p>GitOps workflows incorporate security best practices:</p> <ul> <li>Kyverno Policies: Policy validation for GitOps deployments</li> <li>Cosign Verification: Image signature verification</li> <li>RBAC Controls: Fine-grained access control for GitOps operations</li> <li>Audit Logging: Complete audit trail of all GitOps activities</li> </ul>"},{"location":"gitops/#compliance-integration","title":"Compliance Integration","text":"<p>Automated compliance checking throughout the GitOps pipeline:</p> <ul> <li>Policy Validation: Automated policy compliance checking</li> <li>Security Scanning: Container and configuration security scans</li> <li>Approval Workflows: Required approvals for sensitive environments</li> <li>Documentation: Automated documentation of promotion decisions</li> </ul>"},{"location":"gitops/#best-practices","title":"Best Practices","text":""},{"location":"gitops/#repository-management","title":"Repository Management","text":"<ol> <li>Clear Structure: Organize repositories with clear separation of concerns</li> <li>Branch Strategy: Use appropriate branching strategies for environments</li> <li>Access Control: Implement proper RBAC for repository access</li> <li>Backup Strategy: Regular backups of Git repositories</li> <li>Documentation: Maintain comprehensive GitOps documentation</li> </ol>"},{"location":"gitops/#deployment-strategies","title":"Deployment Strategies","text":"<ol> <li>Progressive Delivery: Use progressive deployment strategies</li> <li>Health Checks: Implement comprehensive health validation</li> <li>Rollback Plans: Maintain clear rollback procedures</li> <li>Monitoring: Continuous monitoring during deployments</li> <li>Testing: Automated testing at each stage</li> </ol>"},{"location":"gitops/#security-practices","title":"Security Practices","text":"<ol> <li>Image Scanning: Automated vulnerability scanning</li> <li>Policy Enforcement: Automated policy validation</li> <li>Secret Management: Secure handling of sensitive data</li> <li>Access Logging: Complete audit trails</li> <li>Approval Gates: Required approvals for production</li> </ol>"},{"location":"gitops/#troubleshooting","title":"Troubleshooting","text":""},{"location":"gitops/#common-issues","title":"Common Issues","text":"<p>Application Sync Failures:</p> <ul> <li>Check Git repository accessibility</li> <li>Verify RBAC permissions</li> <li>Validate manifest syntax</li> <li>Review ArgoCD logs</li> </ul> <p>Image Update Failures:</p> <ul> <li>Verify registry credentials</li> <li>Check image update annotations</li> <li>Review image updater logs</li> <li>Validate semantic versioning patterns</li> </ul> <p>Promotion Failures:</p> <ul> <li>Check Kargo stage configurations</li> <li>Verify promotion policies</li> <li>Review approval requirements</li> <li>Validate target environment health</li> </ul>"},{"location":"gitops/#monitoring-and-alerting","title":"Monitoring and Alerting","text":"<p>DoKa Seca provides comprehensive monitoring for GitOps operations:</p> <pre><code># Example Prometheus alert for GitOps failures\ngroups:\n- name: gitops-alerts\n  rules:\n  - alert: ArgoCD Application Sync Failed\n    expr: argocd_app_health_status{health_status!=\"Healthy\"} == 1\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"ArgoCD application {{ $labels.name }} sync failed\"\n      description: \"Application {{ $labels.name }} has been unhealthy for more than 5 minutes\"\n\n  - alert: Image Update Failed\n    expr: increase(argocd_image_updater_images_updated_errors_total[5m]) &gt; 0\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Image updater failed to update images\"\n      description: \"ArgoCD Image Updater has failed to update images\"\n</code></pre>"},{"location":"glossary/","title":"Glossary","text":"<p>This glossary is a collection of terms related to Doca Seca. The glossary is intended to assist in consistency in usage of terminology across all documentation and represents the meanings accepted by the majority of the community and overall high tech industry. This glossary includes only some most-commonly used terms from these fields.</p> <ul> <li>Recovery point objective (RPO)</li> <li>Recovery time objective (RTO)</li> </ul>"},{"location":"glossary/#reference","title":"Reference","text":"<ul> <li>Mirantis Glossary</li> <li>Devtron Glossary</li> <li>External Secrets Glossary</li> </ul>"},{"location":"identity/","title":"Identity and Access Management","text":"<p>This document describes the identity and access management (IAM) setup for the DoKa Seca platform, including Keycloak integration with various platform components.</p>"},{"location":"identity/#overview","title":"Overview","text":"<p>The DoKa Seca platform uses Keycloak as the central identity provider (IdP) to provide:</p> <ul> <li>Single Sign-On (SSO) across all platform components</li> <li>Multi-tenant user management with role-based access control</li> <li>OIDC/SAML integration with platform services</li> <li>Identity federation with external providers</li> <li>Audit logging for compliance requirements</li> </ul>"},{"location":"identity/#architecture","title":"Architecture","text":"<pre><code>graph TB\n    User[User] --&gt; KC[Keycloak]\n    KC --&gt; Vault[HashiCorp Vault]\n    KC --&gt; ArgoCD[ArgoCD]\n    KC --&gt; Kargo[Kargo]\n    KC --&gt; Grafana[Grafana]\n    KC --&gt; Backstage[Backstage]\n\n    KC --&gt; DB[(PostgreSQL)]\n\n    subgraph \"Identity Sources\"\n        LDAP[LDAP/AD]\n        GitHub[GitHub]\n        Google[Google]\n    end\n\n    LDAP --&gt; KC\n    GitHub --&gt; KC\n    Google --&gt; KC\n\n    subgraph \"Platform Services\"\n        Vault\n        ArgoCD\n        Kargo\n        Grafana\n        Backstage\n    end</code></pre>"},{"location":"identity/#authentication","title":"Authentication","text":""},{"location":"identity/#keycloak-setup","title":"Keycloak Setup","text":"<p>Keycloak serves as the central identity provider with the following configuration:</p> <ul> <li>Database: PostgreSQL for persistent storage</li> <li>Realm: <code>dokaseca</code> - main realm for all platform users</li> <li>Protocols: OIDC/OAuth2 and SAML 2.0 support</li> <li>Federation: Integration with external identity providers</li> </ul>"},{"location":"identity/#getting-started","title":"Getting Started","text":"<p>Access admin console: http://localhost:8080</p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>admin_password</code></li> </ul>"},{"location":"identity/#authorization","title":"Authorization","text":""},{"location":"identity/#user-roles","title":"User Roles","text":"Role Description Permissions <code>platform-admin</code> Platform administrators Full access to all platform components and infrastructure <code>tenant-admin</code> Tenant administrators Administrative access within their tenant namespace <code>developer</code> Development team members Read/write access to tenant applications and resources <code>viewer</code> Read-only users View-only access to tenant resources and dashboards"},{"location":"identity/#groups-and-multi-tenancy","title":"Groups and Multi-Tenancy","text":"<p>Groups provide the primary mechanism for multi-tenancy:</p> <ul> <li><code>platform-team</code>: Platform engineering team with <code>platform-admin</code> role</li> <li><code>tenant-a</code>: Tenant A team members with <code>tenant-admin</code> and <code>developer</code> roles</li> <li><code>tenant-b</code>: Tenant B team members with <code>tenant-admin</code> and <code>developer</code> roles</li> <li><code>tenant-c</code>: Tenant C team members with <code>tenant-admin</code> and <code>developer</code> roles</li> </ul>"},{"location":"identity/#service-integration","title":"Service Integration","text":""},{"location":"identity/#hashicorp-vault","title":"HashiCorp Vault","text":"<p>Vault integrates with Keycloak using the OIDC auth method:</p> <pre><code># Enable OIDC auth method\nvault auth enable oidc\n\n# Configure OIDC\nvault write auth/oidc/config \\\n    oidc_discovery_url=\"https://keycloak.dokaseca.local/realms/dokaseca\" \\\n    oidc_client_id=\"vault\" \\\n    oidc_client_secret=\"vault-client-secret\" \\\n    default_role=\"default\"\n</code></pre>"},{"location":"identity/#argocd","title":"ArgoCD","text":"<p>ArgoCD integrates with Keycloak for SSO and RBAC:</p> <pre><code>oidc.config: |\n  name: Keycloak\n  issuer: https://keycloak.dokaseca.local/realms/dokaseca\n  clientId: argocd\n  clientSecret: argocd-client-secret\n  requestedScopes: [\"openid\", \"profile\", \"email\", \"groups\"]\n\npolicy.csv: |\n  p, role:platform-admin, applications, *, */*, allow\n  p, role:platform-admin, clusters, *, *, allow\n  g, platform-team, role:platform-admin\n</code></pre>"},{"location":"identity/#security-configuration","title":"Security Configuration","text":""},{"location":"identity/#password-policies","title":"Password Policies","text":"<ul> <li>Minimum 12 characters</li> <li>Mixed case letters, numbers, and special characters</li> <li>Username/email restrictions</li> <li>Brute force protection enabled</li> </ul>"},{"location":"identity/#session-management","title":"Session Management","text":"<ul> <li>SSO session timeout: 30 minutes</li> <li>Maximum session lifespan: 10 hours</li> <li>Access token lifespan: 5 minutes</li> <li>Offline session timeout: 30 days</li> </ul>"},{"location":"identity/#federation-and-external-providers","title":"Federation and External Providers","text":""},{"location":"identity/#github-integration","title":"GitHub Integration","text":"<p>Configure GitHub as an identity provider for developer authentication:</p> <pre><code>{\n  \"providerId\": \"github\",\n  \"alias\": \"github\",\n  \"displayName\": \"GitHub\",\n  \"config\": {\n    \"clientId\": \"${github.client.id}\",\n    \"clientSecret\": \"${github.client.secret}\",\n    \"defaultScope\": \"user:email\"\n  }\n}\n</code></pre>"},{"location":"identity/#ldapactive-directory","title":"LDAP/Active Directory","text":"<p>Enterprise LDAP integration for corporate users:</p> <pre><code>{\n  \"providerId\": \"ldap\",\n  \"alias\": \"ldap\",\n  \"displayName\": \"Corporate LDAP\",\n  \"config\": {\n    \"vendor\": \"ad\",\n    \"connectionUrl\": \"ldaps://ldap.company.com:636\",\n    \"usersDn\": \"ou=users,dc=company,dc=com\"\n  }\n}\n</code></pre>"},{"location":"identity/#references","title":"References","text":"<ul> <li>Keycloak Documentation</li> <li>OIDC Specification</li> <li>OAuth 2.0 Security Best Practices</li> <li>Vault OIDC Auth Method</li> </ul>"},{"location":"machine_learning/","title":"Machine Learning","text":"<p>Doka Seca provides a robust machine learning platform built on Kubernetes, integrating best-in-class tools for distributed training, LLM request routing, and observability.</p>"},{"location":"machine_learning/#components-overview","title":"Components Overview","text":"<p>Doka Seca's machine learning stack consists of three core components:</p> <ol> <li>Ray Operator - For distributed training workloads</li> <li>LiteLLM - For routing LLM requests</li> <li>Langfuse - For LLM observability</li> </ol>"},{"location":"machine_learning/#ray-operator","title":"Ray Operator","text":"<p>Ray is an open-source unified framework for scaling AI and Python applications. Doka Seca leverages the Ray Operator for Kubernetes to manage distributed training workloads.</p>"},{"location":"machine_learning/#ray-capabilities","title":"Ray Capabilities","text":"<ul> <li>Distributed Training: Scale machine learning workloads across multiple nodes</li> <li>Resource Management: Efficiently allocate CPU, GPU, and memory resources</li> <li>Fault Tolerance: Automatically recover from node failures</li> <li>Dynamic Scaling: Scale resources up or down based on workload demands</li> </ul>"},{"location":"machine_learning/#usage","title":"Usage","text":"<pre><code>apiVersion: ray.io/v1\nkind: RayCluster\nmetadata:\n  name: ray-cluster\nspec:\n  rayVersion: '2.9.0'\n  headGroupSpec:\n    rayStartParams: {}\n    template:\n      spec:\n        containers:\n        - name: ray-head\n          image: rayproject/ray:2.9.0\n          resources:\n            limits:\n              cpu: \"2\"\n              memory: \"4Gi\"\n  workerGroupSpecs:\n  - groupName: worker-group\n    replicas: 3\n    rayStartParams: {}\n    template:\n      spec:\n        containers:\n        - name: ray-worker\n          image: rayproject/ray:2.9.0\n          resources:\n            limits:\n              cpu: \"1\"\n              memory: \"2Gi\"\n</code></pre>"},{"location":"machine_learning/#litellm","title":"LiteLLM","text":"<p>LiteLLM provides a unified interface for working with various LLM providers. Doka Seca uses LiteLLM to standardize API calls across different LLM services and implement intelligent request routing.</p>"},{"location":"machine_learning/#litellm-capabilities","title":"LiteLLM Capabilities","text":"<ul> <li>Provider Agnostic: Seamless switching between OpenAI, Anthropic, HuggingFace, and other LLM providers</li> <li>Load Balancing: Distribute requests across multiple models and providers</li> <li>Fallbacks: Automatically retry failed requests with alternative models</li> <li>Cost Management: Track and optimize LLM usage costs</li> </ul>"},{"location":"machine_learning/#configuration","title":"Configuration","text":"<p>LiteLLM is deployed as a proxy service in Kubernetes:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: litellm-proxy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: litellm-proxy\n  template:\n    metadata:\n      labels:\n        app: litellm-proxy\n    spec:\n      containers:\n      - name: litellm\n        image: ghcr.io/berriai/litellm:main\n        ports:\n        - containerPort: 8000\n        env:\n        - name: LITELLM_CONFIG_PATH\n          value: \"/config/litellm.yaml\"\n        volumeMounts:\n        - name: config\n          mountPath: /config\n      volumes:\n      - name: config\n        configMap:\n          name: litellm-config\n</code></pre>"},{"location":"machine_learning/#langfuse","title":"Langfuse","text":"<p>Langfuse is an open-source observability and analytics platform for LLM applications. Doka Seca integrates Langfuse to provide comprehensive monitoring and tracing of LLM requests.</p>"},{"location":"machine_learning/#langfuse-capabilities","title":"Langfuse Capabilities","text":"<ul> <li>Request Tracing: Track the flow of requests through your LLM application</li> <li>Performance Monitoring: Measure latency, token usage, and cost metrics</li> <li>Quality Evaluation: Evaluate responses against defined criteria</li> <li>Analytics Dashboard: Visualize patterns and identify optimization opportunities</li> </ul>"},{"location":"multicluster/","title":"Multi cluster","text":"<p>Kubernetes multi-cluster setups are increasingly common for scaling, isolation, and resilience. In this project, we use one cluster per environment: dev, stg, and prod. This approach provides clear separation and reduces blast radius between environments.</p>"},{"location":"multicluster/#pets-vs-cattle","title":"Pets vs Cattle","text":"<p>When managing clusters, it's important to treat them as cattle, not pets. This means clusters should be reproducible, disposable, and managed declaratively (e.g., via GitOps), rather than being unique, hand-crafted, or irreplaceable. This mindset enables automation, scalability, and reliability.</p>"},{"location":"multicluster/#multi-cluster-topologies","title":"Multi-Cluster Topologies","text":"<p>There are several ways to organize and connect multiple Kubernetes clusters:</p> <ul> <li>Standalone/Distributed: Each cluster operates independently, with its own control plane and management. This is simple and works well for clear environment separation (as in our current setup).</li> <li>Centralized (Hub/Spoke): A central \"hub\" cluster manages or coordinates multiple \"spoke\" clusters. This can simplify policy management, observability, and cross-cluster workflows, but adds complexity and a potential single point of failure.</li> </ul> <p>Note: Currently, we use the standalone/distributed topology with one cluster per environment. Future enhancements may explore centralized management for advanced use cases.</p>"},{"location":"multicluster/#gitops-bridge-topologies","title":"GitOps Bridge Topologies","text":"<p>When using GitOps to manage multiple clusters, there are two main approaches:</p> <ul> <li>Standalone/Distributed GitOps: Each cluster has its own GitOps controller (e.g., Argo CD or Flux), managing only its resources. This matches our current topology and is simple to operate.</li> <li>Centralized (Hub/Spoke) GitOps: A central GitOps controller manages resources across multiple clusters. This can enable global policies, shared services, and easier cross-cluster coordination, but requires careful RBAC and security design.</li> </ul>"},{"location":"multicluster/#cluster-creation-and-management","title":"Cluster Creation and Management","text":"<p>DoKa Seca is planning to enhance multi-cluster capabilities through automated cluster provisioning and management using two complementary approaches: Cluster API with KRO, and Crossplane.</p>"},{"location":"multicluster/#cluster-api-capi","title":"Cluster API (CAPI)","text":"<p>Cluster API provides declarative APIs and tooling to simplify provisioning, upgrading, and operating multiple Kubernetes clusters.</p>"},{"location":"multicluster/#planned-implementation","title":"Planned Implementation","text":"<p>DoKa Seca will utilize Cluster API for:</p> <ul> <li>Infrastructure-agnostic cluster creation: Using CAPI providers for AWS, Azure, GCP, and on-premises (via vSphere and bare metal)</li> <li>Standardized cluster templates: Creating reusable cluster blueprints with pre-configured settings for each environment</li> <li>Lifecycle management: Enabling seamless upgrades and scaling operations across clusters</li> </ul>"},{"location":"multicluster/#kro-kubernetes-resource-orchestration","title":"KRO (Kubernetes Resource Orchestration)","text":"<p>KRO will be used to simplify cross-cluster resource management, enabling DoKa Seca to:</p> <ul> <li>Coordinate multi-cluster deployments: Manage application lifecycles across cluster boundaries</li> <li>Handle dependencies between clusters: Ensure proper sequencing of resource creation</li> <li>Implement advanced rollout strategies: Control the deployment flow across multiple environments</li> </ul>"},{"location":"multicluster/#planned-integration","title":"Planned Integration","text":"<p>KRO will work alongside ArgoCD to provide more sophisticated orchestration between clusters, especially for complex application topologies spanning multiple clusters.</p>"},{"location":"multicluster/#crossplane","title":"Crossplane","text":"<p>Crossplane will enable DoKa Seca to define and use higher-level abstractions for both infrastructure and applications, making it easier to manage complex deployments.</p>"},{"location":"multicluster/#planned-capabilities","title":"Planned Capabilities","text":"<ul> <li>Infrastructure provisioning: Create cloud resources (VPCs, databases, etc.) alongside Kubernetes clusters</li> <li>Composition of complex resources: Define reusable templates for complete application stacks</li> <li>Self-service platform: Allow teams to provision environments via simple custom resources</li> </ul>"},{"location":"multicluster/#integration-strategy","title":"Integration Strategy","text":"<p>DoKa Seca plans to integrate these tools in a complementary fashion:</p> <ol> <li>Cluster API will handle the core cluster provisioning and lifecycle management</li> <li>KRO will manage application deployment orchestration across clusters</li> <li>Crossplane will provide higher-level abstractions and additional infrastructure resources</li> </ol> <p>The initial implementation will focus on standardizing cluster creation for dev, staging, and production environments using Cluster API, with KRO and Crossplane capabilities being integrated in subsequent phases.</p>"},{"location":"multicluster/#references","title":"References","text":"<ul> <li>Building a Bridge between Terraform and ArgoCD</li> <li>Cluster API Documentation</li> <li>KRO Project</li> <li>Crossplane Documentation</li> </ul>"},{"location":"networking/","title":"Networking","text":"<p>This document provides an overview of the networking components used in our Kubernetes homelab setup, including Container Network Interfaces (CNIs), load balancers, ingress controllers, API gateways, and service meshes.</p>"},{"location":"networking/#container-network-interfaces-cni","title":"Container Network Interfaces (CNI)","text":"<p>CNIs provide networking for pod-to-pod communication within the cluster.</p>"},{"location":"networking/#cilium","title":"Cilium","text":"<p>Cilium is our primary CNI, leveraging eBPF for high-performance, secure networking with additional observability features.</p>"},{"location":"networking/#installation","title":"Installation","text":"<p>For KinD clusters:</p> <pre><code># Install Cilium CLI\nmake install-cilium-cli\n\n# Deploy Cilium on KinD\ncilium install --version 1.14.3 \\\n  --set kubeProxyReplacement=strict \\\n  --set k8sServiceHost=control-plane-dev-control-plane \\\n  --set k8sServicePort=6443\n</code></pre>"},{"location":"networking/#validation","title":"Validation","text":"<pre><code># Check Cilium status\ncilium status\n\n# Run connectivity test\ncilium connectivity test\n</code></pre>"},{"location":"networking/#multi-cluster","title":"Multi-cluster","text":"<p>For multi-cluster networking with Cilium:</p> <pre><code># Enable cluster mesh\ncilium clustermesh enable --service-type NodePort\n</code></pre>"},{"location":"networking/#load-balancer","title":"Load Balancer","text":""},{"location":"networking/#metallb","title":"MetalLB","text":"<p>MetalLB provides a network load balancer implementation for bare-metal Kubernetes clusters.</p>"},{"location":"networking/#installing-metallb","title":"Installing MetalLB","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.14.3/config/manifests/metallb-native.yaml\n</code></pre>"},{"location":"networking/#configuration","title":"Configuration","text":"<p>To complete the layer2 configuration, you need to provide MetalLB with a range of IP addresses it controls, which should be on the docker kind network. To find the IP address range, run:</p> <pre><code>docker network inspect -f '{{.IPAM.Config}}' kind\n</code></pre> <p>Then apply a configuration like this:</p> <pre><code>apiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: first-pool\n  namespace: metallb-system\nspec:\n  addresses:\n  - 172.18.255.200-172.18.255.250\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: l2-advert\n  namespace: metallb-system\n</code></pre>"},{"location":"networking/#kube-vip","title":"Kube-vip","text":"<p>Kube-vip provides high availability for Kubernetes control plane and services.</p>"},{"location":"networking/#installing-kube-vip","title":"Installing Kube-vip","text":"<pre><code># Deploy as DaemonSet for control plane HA\nkubectl apply -f https://raw.githubusercontent.com/kube-vip/kube-vip-manifests/main/control-plane/daemonset.yaml\n</code></pre>"},{"location":"networking/#ingress-controllers","title":"Ingress Controllers","text":"<p>Ingress controllers manage external access to services within the cluster.</p>"},{"location":"networking/#nginx-ingress-controller","title":"NGINX Ingress Controller","text":"<pre><code># Install NGINX Ingress Controller\nkubectl apply -f kubernetes/ingress-nginx/deploy.yaml\n\n# Verify installation\nkubectl get pods -n ingress-nginx\n</code></pre>"},{"location":"networking/#traefik","title":"Traefik","text":"<p>Traefik is a modern HTTP reverse proxy and load balancer.</p> <pre><code># Install Traefik using Helm\nhelm repo add traefik https://helm.traefik.io/traefik\nhelm install traefik traefik/traefik -n traefik --create-namespace\n</code></pre>"},{"location":"networking/#service-mesh","title":"Service Mesh","text":"<p>Service meshes provide advanced networking features like traffic management, security, and observability.</p>"},{"location":"networking/#istio","title":"Istio","text":"<p>Istio is a popular service mesh that enhances security, observability, and traffic management.</p> <pre><code># Install Istio using istioctl\nistioctl install --set profile=demo -y\n\n# Enable automatic sidecar injection for a namespace\nkubectl label namespace default istio-injection=enabled\n</code></pre>"},{"location":"networking/#advanced-networking-patterns","title":"Advanced Networking Patterns","text":""},{"location":"networking/#east-west-traffic-control","title":"East-West Traffic Control","text":"<p>Implement network policies to control pod-to-pod communication:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-specific-app\nspec:\n  podSelector:\n    matchLabels:\n      app: backend-api\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n</code></pre>"},{"location":"networking/#multi-cluster-networking","title":"Multi-cluster Networking","text":"<p>For connecting multiple clusters:</p> <ol> <li>Use Cilium Cluster Mesh for direct pod-to-pod communication across clusters</li> <li>Configure service discovery with multi-cluster services</li> <li>Implement cross-cluster load balancing with Skupper or similar tools</li> </ol>"},{"location":"networking/#skupper","title":"Skupper","text":"<p>Install cli</p> <pre><code>$ curl https://skupper.io/install.sh | sh\n$ skupper version\nclient version                 1.8.3\ntransport version              not-found (no configuration has been provided)\ncontroller version             not-found (no configuration has been provided)\n</code></pre>"},{"location":"networking/#references","title":"References","text":"<ul> <li>Cilium Documentation</li> <li>MetalLB Configuration</li> <li>Kube-vip: Deploying KIND</li> <li>NGINX Ingress Controller</li> <li>Istio Documentation</li> <li>chmodshubham/cilium</li> <li>Kubernetes Multicluster with Kind and Cilium</li> <li>Kubernetes Multicluster Load Balancing with Skupper</li> <li>Simplifying multi-clusters in Kubernetes</li> </ul>"},{"location":"observability/","title":"Observability","text":"<p>Continuous Monitoring &amp; Observability increases agility, improves customer experience and reduces risk of the cloud environment. According to Wikipedia, Observability is a measure of how well internal states of a system can be inferred from the knowledge of its external outputs. The term observability itself originates from the field of control theory, where it basically means that you can infer the internal state of the components in a system by learning about the external signals/outputs it is producing.</p> <p>The difference between Monitoring and Observability is that Monitoring tells you whether a system is working or not, while Observability tells you why the system isn't working. Monitoring is usually a reactive measure whereas the goal of Observability is to be able to improve your Key Performance Indicators in a proactive manner. A system cannot be controlled or optimized unless it is observed. Instrumenting workloads through collection of metrics, logs, or traces and gaining meaningful insights &amp; detailed context using the right monitoring and observability tools help customers control and optimize the environment.</p>"},{"location":"observability/#doka-seca-observability-stack","title":"DoKa Seca Observability Stack","text":"<p>DoKa Seca provides a comprehensive observability stack that covers the three pillars of observability: Metrics, Logs, and Traces. The platform offers multiple observability solutions that can be enabled based on your requirements.</p>"},{"location":"observability/#primary-observability-stack","title":"Primary Observability Stack","text":""},{"location":"observability/#victoria-metrics-ecosystem","title":"Victoria Metrics Ecosystem","text":"<p>DoKa Seca's default observability stack is built around the Victoria Metrics ecosystem, providing a high-performance, cost-effective alternative to Prometheus:</p>"},{"location":"observability/#victoria-metrics-k8s-stack-enable_victoria_metrics_k8s_stack","title":"Victoria Metrics K8s Stack (<code>enable_victoria_metrics_k8s_stack</code>)","text":"<ul> <li>Purpose: Complete monitoring solution for Kubernetes</li> <li>Components:</li> <li>Victoria Metrics Single for metrics storage</li> <li>VMAgent for metrics collection</li> <li>VMAlert for alerting</li> <li>VMAlertmanager for alert management</li> <li>Grafana for visualization</li> <li>Features:</li> <li>PromQL compatibility</li> <li>High compression ratios</li> <li>Multi-tenancy support</li> <li>Long-term storage capabilities</li> </ul>"},{"location":"observability/#victoria-logs-enable_victoria_logs","title":"Victoria Logs (<code>enable_victoria_logs</code>)","text":"<ul> <li>Purpose: High-performance log management system</li> <li>Features:</li> <li>LogQL compatibility</li> <li>High compression and query performance</li> <li>Integration with Grafana</li> <li>Cost-effective log storage</li> <li>Components:</li> <li>Victoria Logs Single server</li> <li>Vector for log collection and shipping</li> <li>Grafana integration for log visualization</li> </ul>"},{"location":"observability/#grafana-included-with-vm-stack","title":"Grafana (Included with VM Stack)","text":"<ul> <li>Purpose: Unified visualization and dashboards</li> <li>Features:</li> <li>Pre-configured dashboards for Kubernetes</li> <li>Victoria Metrics and Victoria Logs datasources</li> <li>Custom dashboard support</li> <li>Alert visualization</li> </ul>"},{"location":"observability/#data-collection-and-shipping","title":"Data Collection and Shipping","text":""},{"location":"observability/#alloy-enable_alloy","title":"Alloy (<code>enable_alloy</code>)","text":"<ul> <li>Purpose: Grafana's distribution of OpenTelemetry Collector</li> <li>Capabilities:</li> <li>Metrics, logs, and traces collection</li> <li>Data transformation and routing</li> <li>Multi-destination shipping</li> <li>Low resource footprint</li> </ul>"},{"location":"observability/#vector-enable_vector","title":"Vector (<code>enable_vector</code>)","text":"<ul> <li>Purpose: High-performance log and metrics router</li> <li>Features:</li> <li>Real-time data transformation</li> <li>Multiple input/output formats</li> <li>Built-in error handling</li> <li>Memory-efficient processing</li> </ul>"},{"location":"observability/#alternative-observability-solutions","title":"Alternative Observability Solutions","text":""},{"location":"observability/#prometheus-ecosystem","title":"Prometheus Ecosystem","text":""},{"location":"observability/#kube-prometheus-stack-enable_kube_prometheus_stack","title":"Kube Prometheus Stack (<code>enable_kube_prometheus_stack</code>)","text":"<ul> <li>Components:</li> <li>Prometheus for metrics storage</li> <li>Alertmanager for alert management</li> <li>Grafana for visualization</li> <li>Node Exporter for node metrics</li> <li>Kube State Metrics for Kubernetes metrics</li> <li>Features:</li> <li>Industry-standard monitoring solution</li> <li>Extensive ecosystem</li> <li>Rich alerting capabilities</li> </ul>"},{"location":"observability/#metrics-server-enable_metrics_server","title":"Metrics Server (<code>enable_metrics_server</code>)","text":"<ul> <li>Purpose: Core resource metrics for Kubernetes</li> <li>Function: Provides CPU/Memory metrics for HPA and VPA</li> </ul>"},{"location":"observability/#prometheus-adapter-enable_prometheus_adapter","title":"Prometheus Adapter (<code>enable_prometheus_adapter</code>)","text":"<ul> <li>Purpose: Custom metrics API for Kubernetes</li> <li>Function: Exposes custom metrics for HPA scaling</li> </ul>"},{"location":"observability/#tracing-solutions","title":"Tracing Solutions","text":""},{"location":"observability/#tempo-enable_tempo","title":"Tempo (<code>enable_tempo</code>)","text":"<ul> <li>Purpose: High-scale distributed tracing backend</li> <li>Features:</li> <li>OpenTelemetry native</li> <li>Object storage backend</li> <li>Grafana integration</li> <li>Cost-effective trace storage</li> </ul>"},{"location":"observability/#jaeger-enable_jaeger","title":"Jaeger (<code>enable_jaeger</code>)","text":"<ul> <li>Purpose: End-to-end distributed tracing</li> <li>Components:</li> <li>Jaeger Agent</li> <li>Jaeger Collector</li> <li>Jaeger Query UI</li> <li>Storage backend</li> </ul>"},{"location":"observability/#zipkin-enable_zipkin","title":"Zipkin (<code>enable_zipkin</code>)","text":"<ul> <li>Purpose: Distributed tracing system</li> <li>Features:</li> <li>Simple deployment model</li> <li>Web UI for trace visualization</li> <li>Service dependency mapping</li> </ul>"},{"location":"observability/#profiling-solutions","title":"Profiling Solutions","text":""},{"location":"observability/#pyroscope-enable_pyroscope","title":"Pyroscope (<code>enable_pyroscope</code>)","text":"<ul> <li>Purpose: Continuous profiling platform</li> <li>Features:</li> <li>Application performance profiling</li> <li>Resource usage optimization</li> <li>Integration with Grafana</li> <li>Multi-language support</li> </ul>"},{"location":"observability/#specialized-monitoring","title":"Specialized Monitoring","text":""},{"location":"observability/#opentelemetry-operator-enable_opentelemetry_operator","title":"OpenTelemetry Operator (<code>enable_opentelemetry_operator</code>)","text":"<ul> <li>Purpose: Kubernetes operator for OpenTelemetry</li> <li>Features:</li> <li>Auto-instrumentation</li> <li>Collector management</li> <li>Custom resource definitions</li> </ul>"},{"location":"observability/#grafana-operator-enable_grafana_operator","title":"Grafana Operator (<code>enable_grafana_operator</code>)","text":"<ul> <li>Purpose: Kubernetes-native Grafana management</li> <li>Features:</li> <li>GitOps-friendly dashboard management</li> <li>Multi-tenant Grafana instances</li> <li>Custom resource definitions</li> </ul>"},{"location":"observability/#k8s-monitoring-enable_k8s_monitoring","title":"K8s Monitoring (<code>enable_k8s_monitoring</code>)","text":"<ul> <li>Purpose: Simplified Kubernetes monitoring setup</li> <li>Features:</li> <li>Pre-configured monitoring stack</li> <li>Best practices implementation</li> <li>Automated discovery</li> </ul>"},{"location":"observability/#service-mesh-observability","title":"Service Mesh Observability","text":""},{"location":"observability/#kiali-enable_kiali","title":"Kiali (<code>enable_kiali</code>)","text":"<ul> <li>Purpose: Service mesh observability (Istio)</li> <li>Features:</li> <li>Service topology visualization</li> <li>Traffic flow analysis</li> <li>Configuration validation</li> <li>Performance metrics</li> </ul>"},{"location":"observability/#dashboard-solutions","title":"Dashboard Solutions","text":""},{"location":"observability/#kubernetes-dashboard-enable_kubernetes_dashboard","title":"Kubernetes Dashboard (<code>enable_kubernetes_dashboard</code>)","text":"<ul> <li>Purpose: Web-based Kubernetes user interface</li> <li>Features:</li> <li>Cluster resource management</li> <li>Workload visualization</li> <li>Resource monitoring</li> </ul>"},{"location":"observability/#headlamp-enable_headlamp","title":"Headlamp (<code>enable_headlamp</code>)","text":"<ul> <li>Purpose: Modern Kubernetes dashboard</li> <li>Features:</li> <li>Real-time cluster monitoring</li> <li>Resource management</li> <li>Plugin ecosystem</li> </ul>"},{"location":"observability/#helm-dashboard-enable_helm_dashboard","title":"Helm Dashboard (<code>enable_helm_dashboard</code>)","text":"<ul> <li>Purpose: Web UI for Helm releases</li> <li>Features:</li> <li>Release management</li> <li>Chart exploration</li> <li>Installation monitoring</li> </ul>"},{"location":"observability/#altinity-dashboard-enable_altinity_dashboard","title":"Altinity Dashboard (<code>enable_altinity_dashboard</code>)","text":"<ul> <li>Purpose: ClickHouse cluster monitoring</li> <li>Features:</li> <li>ClickHouse-specific metrics</li> <li>Query performance monitoring</li> <li>Cluster health visualization</li> </ul>"},{"location":"observability/#log-management-alternatives","title":"Log Management Alternatives","text":""},{"location":"observability/#cortex-enable_cortex","title":"Cortex (<code>enable_cortex</code>)","text":"<ul> <li>Purpose: Horizontally scalable Prometheus</li> <li>Features:</li> <li>Multi-tenancy</li> <li>Long-term storage</li> <li>High availability</li> </ul>"},{"location":"observability/#thanos-enable_thanos","title":"Thanos (<code>enable_thanos</code>)","text":"<ul> <li>Purpose: Prometheus long-term storage</li> <li>Features:</li> <li>Global query view</li> <li>Object storage integration</li> <li>Data deduplication</li> </ul>"},{"location":"observability/#logging-operator-enable_logging_operator","title":"Logging Operator (<code>enable_logging_operator</code>)","text":"<ul> <li>Purpose: Kubernetes-native log management</li> <li>Features:</li> <li>Fluentd/Fluent Bit integration</li> <li>Log routing and filtering</li> <li>Multiple output destinations</li> </ul>"},{"location":"observability/#configuration-examples","title":"Configuration Examples","text":""},{"location":"observability/#enabling-victoria-metrics-stack","title":"Enabling Victoria Metrics Stack","text":"<pre><code>addons = {\n  enable_victoria_metrics_k8s_stack = true\n  enable_victoria_logs = true\n  enable_alloy = true\n}\n</code></pre>"},{"location":"observability/#enabling-prometheus-stack","title":"Enabling Prometheus Stack","text":"<pre><code>addons = {\n  enable_kube_prometheus_stack = true\n  enable_metrics_server = true\n  enable_prometheus_adapter = true\n}\n</code></pre>"},{"location":"observability/#enabling-distributed-tracing","title":"Enabling Distributed Tracing","text":"<pre><code>addons = {\n  enable_tempo = true\n  enable_opentelemetry_operator = true\n  enable_jaeger = true\n}\n</code></pre>"},{"location":"observability/#access-instructions","title":"Access Instructions","text":""},{"location":"observability/#grafana-ui","title":"Grafana UI","text":"<pre><code># For Victoria Metrics stack\nkubectl port-forward svc/victoria-metrics-k8s-stack-grafana -n monitoring 3000:80\n\n# For Prometheus stack\nkubectl port-forward svc/kube-prometheus-stack-grafana -n monitoring 3000:80\n\n# Get admin password\nkubectl get secret -n monitoring &lt;grafana-secret-name&gt; -o jsonpath=\"{.data.admin-password}\" | base64 --decode\n</code></pre>"},{"location":"observability/#victoria-metrics-ui","title":"Victoria Metrics UI","text":"<pre><code>kubectl port-forward svc/vmsingle-victoria-metrics-k8s-stack -n monitoring 8429:8429\n</code></pre>"},{"location":"observability/#victoria-logs-ui","title":"Victoria Logs UI","text":"<pre><code>kubectl port-forward svc/victoria-logs-single-server -n monitoring 9428:9428\n</code></pre>"},{"location":"observability/#prometheus-ui","title":"Prometheus UI","text":"<pre><code>kubectl port-forward svc/kube-prometheus-stack-prometheus -n monitoring 9090:9090\n</code></pre>"},{"location":"observability/#jaeger-ui","title":"Jaeger UI","text":"<pre><code>kubectl port-forward svc/jaeger-query -n monitoring 16686:80\n</code></pre>"},{"location":"observability/#best-practices","title":"Best Practices","text":""},{"location":"observability/#1-resource-management","title":"1. Resource Management","text":"<ul> <li>Configure appropriate resource limits for monitoring components</li> <li>Use persistent volumes for long-term metric storage</li> <li>Implement data retention policies</li> </ul>"},{"location":"observability/#2-security","title":"2. Security","text":"<ul> <li>Enable RBAC for monitoring components</li> <li>Use ServiceMonitor labels for metric discovery</li> <li>Implement network policies for monitoring namespace</li> </ul>"},{"location":"observability/#3-performance-optimization","title":"3. Performance Optimization","text":"<ul> <li>Configure scrape intervals based on requirements</li> <li>Use metric relabeling to reduce cardinality</li> <li>Implement recording rules for complex queries</li> </ul>"},{"location":"observability/#4-high-availability","title":"4. High Availability","text":"<ul> <li>Deploy monitoring components across multiple nodes</li> <li>Use anti-affinity rules for critical components</li> <li>Implement backup strategies for monitoring data</li> </ul>"},{"location":"observability/#monitoring-integration","title":"Monitoring Integration","text":"<p>DoKa Seca automatically configures ServiceMonitors and dashboards for:</p> <ul> <li>ArgoCD: GitOps metrics and dashboards</li> <li>Kyverno: Policy enforcement metrics</li> <li>External Secrets: Secret synchronization metrics</li> <li>Cert-Manager: Certificate metrics</li> <li>MetalLB: Load balancer metrics</li> <li>Ingress Controllers: Traffic metrics</li> </ul> <p>This comprehensive observability stack ensures that your DoKa Seca platform provides deep insights into application and infrastructure performance, enabling proactive monitoring and troubleshooting capabilities.</p>"},{"location":"portal/","title":"Developer Portal","text":"<p>DoKa Seca integrates Backstage as its developer portal, providing a unified interface for developers to discover services, access documentation, manage deployments, and interact with the platform ecosystem. Backstage serves as the central hub for developer experience within the DoKa Seca platform.</p> <p>Warning</p> <p>Documentation coming soon!</p>"},{"location":"references/","title":"References","text":"<p>DoKa Seca draws inspiration from numerous cloud-native platform engineering projects and initiatives. This page acknowledges the innovative work that has influenced the design and implementation of our platform bootstrapping framework.</p>"},{"location":"references/#platform-engineering-inspirations","title":"Platform Engineering Inspirations","text":"<p>DoKa Seca is inspired by the following projects and platforms that have shaped modern cloud-native platform engineering:</p>"},{"location":"references/#internal-developer-platforms-idps","title":"Internal Developer Platforms (IDPs)","text":"<ul> <li>Janus IDP - Red Hat's open-source Internal Developer Platform built on Backstage, providing enterprise-ready developer portal capabilities</li> <li>choreo-idp - Cloud-native platform for building and deploying applications with strong GitOps integration</li> <li>Mia-Platform - Comprehensive platform for fast software delivery with microservices architecture</li> <li>Devtron - Open-source software delivery workflow for Kubernetes with CI/CD and GitOps</li> </ul>"},{"location":"references/#cloud-native-operating-systems","title":"Cloud-Native Operating Systems","text":"<ul> <li>Deckhouse - Kubernetes platform that provides a seamless experience for deploying and managing containerized applications</li> <li>SIGHUP Distribution - Production-ready Kubernetes distribution focused on security, observability, and disaster recovery</li> </ul>"},{"location":"references/#platform-frameworks-and-stacks","title":"Platform Frameworks and Stacks","text":"<ul> <li>CNOE (Cloud Native Operational Excellence) - Open-source reference implementation for cloud-native platform engineering</li> <li>BACK Stack - Modern infrastructure stack combining Backstage, ArgoCD, Crossplane, and Kubernetes</li> <li>Harmonix - AWS-native platform engineering framework for building developer platforms</li> <li>Krateo PlatformOps - Cloud-native platform for managing infrastructure and applications through a unified control plane with strong GitOps practices</li> <li>Azure AKS Platform Engineering - Microsoft's reference implementation for building platform engineering solutions on Azure Kubernetes Service with GitOps and developer self-service capabilities</li> </ul>"},{"location":"references/#configuration-and-automation","title":"Configuration and Automation","text":"<ul> <li>Kusion - Programmable configuration techstack for building reliable applications at scale</li> <li>mlinfra - Infrastructure-as-code platform specifically designed for machine learning workloads</li> </ul>"},{"location":"references/#cloud-provider-platforms","title":"Cloud Provider Platforms","text":"<ul> <li>Akamai App Platform - Cloud-native application platform providing edge computing capabilities</li> <li>Data on EKS - AWS Labs' comprehensive guide and patterns for running data workloads on Amazon EKS, including big data, analytics, and machine learning platforms</li> <li>AI on EKS - AWS Labs' reference implementations and best practices for deploying artificial intelligence and machine learning workloads on Amazon EKS</li> </ul>"},{"location":"references/#technical-references","title":"Technical References","text":""},{"location":"references/#backup-and-disaster-recovery","title":"Backup and Disaster Recovery","text":"<ul> <li>Integrate Velero to back up and restore Karmada resources - Best practices for implementing backup strategies in multi-cluster environments</li> </ul>"},{"location":"references/#platform-engineering-talks-and-presentations","title":"Platform Engineering Talks and Presentations","text":"<ul> <li>AWS re:Invent 2023 - Platform engineering with Amazon EKS (CON311) (Presentation) - Comprehensive session on building developer platforms using Amazon EKS, covering best practices for platform team organization, standardized infrastructure patterns, and developer self-service capabilities</li> </ul>"},{"location":"references/#acknowledgments","title":"Acknowledgments","text":"<p>We extend our gratitude to the maintainers and contributors of these projects. Their innovative approaches to platform engineering, developer experience, and cloud-native operations have significantly influenced DoKa Seca's architecture and implementation.</p> <p>The cloud-native ecosystem thrives on collaboration and shared knowledge. DoKa Seca aims to contribute back to this ecosystem by providing a practical, production-ready platform bootstrapping framework that incorporates lessons learned from these exemplary projects.</p>"},{"location":"references/#contributing-references","title":"Contributing References","text":"<p>If you know of other projects or resources that have influenced platform engineering practices and should be included in this list, please contribute by submitting a pull request or opening an issue.</p>"},{"location":"releases/","title":"Release Notes","text":"<p>Warning</p> <p>Documentation coming soon!</p>"},{"location":"repository/","title":"Artifact Repositories","text":"<p>DoKa Seca leverages GitHub Container Registry (GHCR) as the default artifact repository for storing and distributing container images, Helm charts, and other platform artifacts. This integration provides seamless CI/CD workflows with built-in security scanning, access control, and package management capabilities.</p>"},{"location":"repository/#overview","title":"Overview","text":"<p>Artifact repositories are essential components of modern platform engineering, providing centralized storage for container images, Helm charts, and other deployment artifacts. DoKa Seca's integration with GitHub Container Registry offers:</p> <ul> <li>Unified Platform: Single platform for source code and artifacts</li> <li>Integrated Security: Built-in vulnerability scanning and security policies</li> <li>Access Control: Fine-grained permissions aligned with repository access</li> <li>Cost Effective: Free tier for public repositories and competitive pricing for private</li> <li>GitOps Ready: Native integration with GitOps workflows and ArgoCD</li> </ul>"},{"location":"repository/#authentication-and-security","title":"Authentication and Security","text":""},{"location":"repository/#github-personal-access-tokens","title":"GitHub Personal Access Tokens","text":"<p>DoKa Seca uses GitHub Personal Access Tokens for registry authentication:</p> <pre><code># Create token with required scopes\n# - read:packages\n# - write:packages (for pushing)\n# - delete:packages (for cleanup)\n\n# Login to GHCR\necho $GITHUB_TOKEN | docker login ghcr.io -u USERNAME --password-stdin\n</code></pre>"},{"location":"repository/#kubernetes-secret-management","title":"Kubernetes Secret Management","text":"<p>Registry credentials are managed as Kubernetes secrets:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: ghcr-credentials\n  namespace: default\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: |\n    {\n      \"auths\": {\n        \"ghcr.io\": {\n          \"username\": \"your-username\",\n          \"password\": \"your-token\",\n          \"auth\": \"base64-encoded-credentials\"\n        }\n      }\n    }\n</code></pre>"},{"location":"repository/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"repository/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<p>DoKa Seca includes GitHub Actions workflows for automated image building and publishing:</p> <pre><code>name: Build and Push Container Images\n\non:\n  push:\n    branches: [main]\n    tags: ['v*']\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Login to GitHub Container Registry\n      uses: docker/login-action@v3\n      with:\n        registry: ghcr.io\n        username: ${{ github.actor }}\n        password: ${{ secrets.GITHUB_TOKEN }}\n\n    - name: Extract metadata\n      id: meta\n      uses: docker/metadata-action@v5\n      with:\n        images: ghcr.io/${{ github.repository }}\n        tags: |\n          type=ref,event=branch\n          type=ref,event=pr\n          type=semver,pattern={{version}}\n          type=semver,pattern={{major}}.{{minor}}\n\n    - name: Build and push\n      uses: docker/build-push-action@v5\n      with:\n        context: .\n        push: true\n        tags: ${{ steps.meta.outputs.tags }}\n        labels: ${{ steps.meta.outputs.labels }}\n        platforms: linux/amd64,linux/arm64\n</code></pre>"},{"location":"repository/#argocd-image-updater-integration","title":"ArgoCD Image Updater Integration","text":"<p>Automated image updates in GitOps workflows:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: my-app\n  annotations:\n    argocd-image-updater.argoproj.io/image-list: myapp=ghcr.io/your-org/my-app\n    argocd-image-updater.argoproj.io/myapp.update-strategy: semver\n    argocd-image-updater.argoproj.io/myapp.allow-tags: regexp:^v[0-9]+\\.[0-9]+\\.[0-9]+$\nspec:\n  # ... application specification\n</code></pre>"},{"location":"repository/#package-management","title":"Package Management","text":""},{"location":"repository/#package-visibility-and-access","title":"Package Visibility and Access","text":"<p>GHCR supports different visibility levels:</p> <ul> <li>Public: Accessible to everyone</li> <li>Private: Restricted to organization members</li> <li>Internal: Accessible to organization and enterprise members</li> </ul>"},{"location":"repository/#package-metadata","title":"Package Metadata","text":"<p>Rich metadata support for better package management:</p> <pre><code># Dockerfile with metadata labels\nLABEL org.opencontainers.image.title=\"DoKa Seca Control Plane\"\nLABEL org.opencontainers.image.description=\"Platform engineering framework for Kubernetes\"\nLABEL org.opencontainers.image.source=\"https://github.com/your-org/dokaseca-control-plane\"\nLABEL org.opencontainers.image.documentation=\"https://dokaseca.dev/docs\"\nLABEL org.opencontainers.image.version=\"1.0.0\"\n</code></pre>"},{"location":"repository/#vulnerability-scanning","title":"Vulnerability Scanning","text":"<p>GHCR provides built-in security scanning:</p> <ul> <li>Automatic Scanning: All pushed images are automatically scanned</li> <li>Vulnerability Reports: Detailed security vulnerability reports</li> <li>Security Advisories: Integration with GitHub security advisories</li> <li>Policy Enforcement: Block deployments of vulnerable images</li> </ul>"},{"location":"repository/#best-practices","title":"Best Practices","text":""},{"location":"repository/#image-management","title":"Image Management","text":"<ol> <li>Immutable Tags: Use semantic versioning for production images</li> <li>Security Scanning: Enable vulnerability scanning for all images</li> <li>Multi-Stage Builds: Use multi-stage Dockerfiles to reduce image size</li> <li>Base Image Updates: Regularly update base images for security patches</li> <li>Image Signing: Use Cosign for image signing and verification</li> </ol>"},{"location":"repository/#repository-organization","title":"Repository Organization","text":"<ol> <li>Naming Conventions: Follow consistent naming patterns</li> <li>Visibility Settings: Set appropriate visibility levels for packages</li> <li>Access Control: Use GitHub teams for repository and package access</li> <li>Documentation: Maintain comprehensive package documentation</li> <li>Lifecycle Policies: Implement retention policies for old packages</li> </ol>"},{"location":"repository/#cicd-integration_1","title":"CI/CD Integration","text":"<ol> <li>Automated Builds: Trigger builds on code changes</li> <li>Testing: Include security and functionality testing in pipelines</li> <li>Promotion: Use promotion workflows between environments</li> <li>Rollback: Maintain rollback capabilities for failed deployments</li> <li>Monitoring: Monitor registry usage and performance</li> </ol>"},{"location":"repository/#troubleshooting","title":"Troubleshooting","text":""},{"location":"repository/#common-issues","title":"Common Issues","text":"<p>Authentication Failures:</p> <ul> <li>Verify GitHub token has required scopes</li> <li>Check token expiration date</li> <li>Ensure correct username/token combination</li> </ul> <p>Image Pull Errors:</p> <ul> <li>Verify image exists and is accessible</li> <li>Check network connectivity to registry</li> <li>Validate image tag and repository name</li> </ul> <p>Build and Push Failures:</p> <ul> <li>Check Docker daemon status</li> <li>Verify disk space for image building</li> <li>Validate Dockerfile syntax</li> </ul>"},{"location":"repository/#getting-help","title":"Getting Help","text":"<ul> <li>Review GitHub Container Registry documentation</li> <li>Check DoKa Seca registry configuration examples</li> <li>Examine platform logs for registry-related errors</li> <li>Engage with the platform engineering team for support</li> </ul>"},{"location":"repository/#migration-guide","title":"Migration Guide","text":""},{"location":"repository/#migrating-from-other-registries","title":"Migrating from Other Registries","text":"<p>DoKa Seca provides migration tools for transitioning from other container registries:</p> <pre><code># Migration script example\n#!/bin/bash\nSOURCE_REGISTRY=\"docker.io/myorg\"\nTARGET_REGISTRY=\"ghcr.io/myorg\"\n\n# Pull, retag, and push images\nfor image in $(docker images --format \"{{.Repository}}:{{.Tag}}\" | grep $SOURCE_REGISTRY); do\n  new_image=${image/$SOURCE_REGISTRY/$TARGET_REGISTRY}\n  docker tag $image $new_image\n  docker push $new_image\ndone\n</code></pre>"},{"location":"roadmap/","title":"DoKa Seca Roadmap","text":"<p>This roadmap outlines the planned features, improvements, and integrations for the DoKa Seca platform. Items are organized by category and priority, with completed tasks marked with green checkmark.</p>"},{"location":"roadmap/#platform-infrastructure","title":"Platform Infrastructure","text":""},{"location":"roadmap/#observability-monitoring","title":"Observability &amp; Monitoring","text":"<ul> <li> Victoria Logs Integration - Deploy Victoria Logs via ArgoCD for centralized log aggregation and analysis</li> <li> Enhanced Metrics Collection - Expand Prometheus metrics collection for platform components</li> <li> Custom Dashboards - Develop additional Grafana dashboards for platform-specific monitoring</li> </ul>"},{"location":"roadmap/#gitops-deployment","title":"GitOps &amp; Deployment","text":"<ul> <li>\u2705 GitOps Promoter - Automated promotion workflows between environments via Kustomize</li> <li>\u2705 Atlas Operator - Database schema management and migrations via GitOps</li> <li> Advanced GitOps Workflows - Enhanced promotion strategies and rollback mechanisms</li> </ul>"},{"location":"roadmap/#progressive-delivery","title":"Progressive Delivery","text":"<ul> <li>\u2705 Flagger Integration - Automated canary deployments and progressive delivery via FluxCD</li> <li> Traffic Management - Advanced traffic splitting and A/B testing capabilities</li> <li> Deployment Strategies - Blue/green and canary deployment patterns</li> </ul>"},{"location":"roadmap/#networking-connectivity","title":"Networking &amp; Connectivity","text":""},{"location":"roadmap/#container-network-interface-cni","title":"Container Network Interface (CNI)","text":"<ul> <li> Cilium CNI - Deploy Cilium for advanced networking, security, and observability</li> <li> Linkerd Service Mesh - Install Linkerd via FluxCD for service-to-service communication</li> <li> Network Policies - Implement comprehensive network security policies</li> </ul>"},{"location":"roadmap/#load-balancing-ingress","title":"Load Balancing &amp; Ingress","text":"<ul> <li>\u2705 KubeVIP - Load balancer for Kubernetes control plane via Helm chart</li> <li> Gateway API - Install and configure Kubernetes Gateway API for next-generation ingress management</li> <li> Traefik Gateway - Install and configure Traefik to test Gateway API functionality</li> <li> Ingress Optimization - Performance tuning and SSL/TLS automation</li> </ul>"},{"location":"roadmap/#multi-cluster-networking","title":"Multi-Cluster Networking","text":"<ul> <li> Submariner Integration - Connect multiple Kind clusters with different CNI configurations</li> <li> Cross-Cluster Service Discovery - Enable service discovery across cluster boundaries</li> <li> Multi-Cluster Security - Implement security policies for cross-cluster communication</li> </ul>"},{"location":"roadmap/#platform-integrations","title":"Platform Integrations","text":""},{"location":"roadmap/#developer-experience","title":"Developer Experience","text":"<ul> <li> Enhanced Backstage Integration - Improved developer portal with additional plugins</li> <li> Self-Service Capabilities - Automated resource provisioning and management</li> </ul>"},{"location":"roadmap/#security-compliance","title":"Security &amp; Compliance","text":"<ul> <li> Enhanced Policy Management - Advanced Kyverno policies for security and compliance</li> <li> Secret Management - Implementation of External Secrets Operator with Vault integration</li> <li> Image Security - Enhanced container image scanning and vulnerability management</li> </ul>"},{"location":"roadmap/#data-storage","title":"Data &amp; Storage","text":"<ul> <li> Persistent Storage Solutions - Enhanced storage classes and backup strategies</li> <li> Database Management - Automated database provisioning and lifecycle management</li> <li> Data Pipeline Integration - Support for data processing and analytics workloads</li> </ul>"},{"location":"roadmap/#platform-engineering","title":"Platform Engineering","text":""},{"location":"roadmap/#infrastructure-as-code","title":"Infrastructure as Code","text":"<ul> <li> Terraform Modules - Additional reusable Terraform modules for cloud resources</li> <li> Infrastructure Testing - Automated testing for infrastructure changes</li> </ul>"},{"location":"roadmap/#automation-tooling","title":"Automation &amp; Tooling","text":"<ul> <li> CI/CD Enhancements - Improved build and deployment pipelines</li> <li> Quality Gates - Automated quality checks and security scanning</li> <li> Documentation Automation - Automated documentation generation and updates</li> </ul>"},{"location":"roadmap/#monitoring-operations","title":"Monitoring &amp; Operations","text":"<ul> <li> Enhanced Alerting - Intelligent alerting with reduced noise and better correlation</li> <li> Capacity Planning - Automated resource planning and optimization</li> <li> Performance Optimization - Platform performance tuning and optimization</li> </ul>"},{"location":"roadmap/#future-enhancements","title":"Future Enhancements","text":""},{"location":"roadmap/#advanced-features","title":"Advanced Features","text":"<ul> <li> Multi-Tenancy - Enhanced tenant isolation and resource management</li> <li> Cost Optimization - Advanced cost tracking and optimization recommendations</li> <li> AI/ML Integration - Support for machine learning workloads and model serving</li> </ul>"},{"location":"roadmap/#ecosystem-integration","title":"Ecosystem Integration","text":"<ul> <li> Cloud Native Tools - Integration with additional CNCF projects</li> <li> Vendor Solutions - Integration with enterprise-grade solutions</li> <li> Custom Extensions - Framework for custom platform extensions</li> </ul>"},{"location":"roadmap/#contributing-to-the-roadmap","title":"Contributing to the Roadmap","text":"<p>We welcome community input on our roadmap! Here's how you can contribute:</p>"},{"location":"roadmap/#vote-on-features","title":"\ud83d\uddf3\ufe0f Vote on Features","text":"<ul> <li>Comment on GitHub issues to express interest in specific features</li> <li>Participate in community discussions about priorities</li> </ul>"},{"location":"roadmap/#suggest-new-features","title":"\ud83d\udca1 Suggest New Features","text":"<ul> <li>Open GitHub issues with feature requests</li> <li>Provide detailed use cases and requirements</li> </ul>"},{"location":"roadmap/#contribute-implementation","title":"\ud83e\udd1d Contribute Implementation","text":"<ul> <li>Pick up roadmap items and submit pull requests</li> <li>Review the Contributing Guide for development guidelines</li> </ul>"},{"location":"roadmap/#roadmap-process","title":"\ud83d\udccb Roadmap Process","text":"<ol> <li>Community Input - Gather feedback and requirements from users</li> <li>Technical Design - Create Architecture Decision Records (ADRs) for major features</li> <li>Implementation - Develop features following platform standards</li> <li>Testing &amp; Documentation - Comprehensive testing and documentation updates</li> <li>Release - Deploy to platform and update roadmap status</li> </ol> <p>Note: This roadmap is subject to change based on community feedback, technical considerations, and project priorities. Check back regularly for updates!</p> <p>For questions about the roadmap or to discuss specific features, please open an issue in our GitHub repository.</p>"},{"location":"security/","title":"Security","text":"<p>The DoKa Seca platform implements a defense-in-depth approach to Kubernetes security, leveraging both vulnerability scanning and runtime threat detection.</p>"},{"location":"security/#trivy-vulnerability-misconfiguration-scanning","title":"Trivy: Vulnerability &amp; Misconfiguration Scanning","text":"<p>Trivy is used for:</p> <ul> <li>Scanning container images for vulnerabilities (CVEs)</li> <li>Auditing Kubernetes manifests for misconfigurations</li> <li>Integrating with CI/CD pipelines for early detection</li> <li>Generating <code>ConfigAuditReports</code> and <code>VulnerabilityReports</code> in-cluster</li> </ul> <p>Example: Scan a running deployment</p> <pre><code>kubectl create deployment nginx --image nginx:1.16\nkubectl get configauditreports -o wide\n</code></pre> <p>Sample output:</p> <pre><code>NAME                          SCANNER   AGE     CRITICAL   HIGH   MEDIUM   LOW\nreplicaset-nginx-599c4f6679   Trivy     3m16s   0          2      3        10\n</code></pre> <p>Trivy is integrated via the KubeSec Operator for continuous cluster scanning.</p>"},{"location":"security/#falco-runtime-threat-detection","title":"Falco: Runtime Threat Detection","text":"<p>Falco is used for:</p> <ul> <li>Real-time detection of suspicious activity in Kubernetes nodes and containers</li> <li>Alerting on unexpected process execution, file access, privilege escalation, and network activity</li> <li>Enforcing runtime security policies with custom rules</li> <li>Integrating with alerting systems (Slack, Prometheus, etc)</li> </ul>"},{"location":"security/#example-view-falco-alerts","title":"Example: View Falco alerts","text":"<pre><code>kubectl logs -n falco -l app=falco\n</code></pre> <p>Common Falco rules include:</p> <ul> <li>Detecting shell in a container</li> <li>Detecting changes to sensitive files</li> <li>Detecting privilege escalation attempts</li> </ul>"},{"location":"security/#security-best-practices","title":"Security Best Practices","text":"<ul> <li>All images are scanned before deployment</li> <li>Cluster is continuously monitored for runtime threats</li> <li>Alerts are integrated with platform observability</li> <li>Security events are audited and reviewed regularly</li> </ul>"},{"location":"security/#references","title":"References","text":"<ul> <li>Trivy Documentation</li> <li>Falco Documentation</li> <li>Kubernetes Security Best Practices</li> </ul>"},{"location":"storage/","title":"Storage","text":"<p>This document describes the storage architecture used in the DoKa Seca, focusing on how MinIO running in Docker is used to provide S3-compatible object storage for various components.</p>"},{"location":"storage/#minio-object-storage","title":"MinIO Object Storage","text":"<p>MinIO is a high-performance, S3-compatible object storage system that we deploy to provide persistent storage for several critical components.</p>"},{"location":"storage/#architecture","title":"Architecture","text":"<p>We run MinIO as a Docker container outside the Kubernetes cluster, providing S3-compatible storage that our Kubernetes services connect to:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Docker Host     \u2502           \u2502 Kubernetes Cluster        \u2502\n\u2502                 \u2502           \u2502                           \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  access   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502 \u2502 MinIO Server\u2502\u25c4\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u25ba\u25c4\u2502 Service \u2502 \u2502 Service \u2502  \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502           \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502\n        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502             S3 Buckets                     \u2502\n\u2502                                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  velero  \u2502 \u2502 loki  \u2502 \u2502 tempo \u2502 \u2502  vm  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"storage/#installation","title":"Installation","text":"<p>To install MinIO using Docker:</p> <pre><code># Create directories for MinIO data\nmkdir -p data/minio/velero-dev\nmkdir -p data/minio/velero-stg\nmkdir -p data/minio/velero-prod\nmkdir -p data/minio/loki\nmkdir -p data/minio/tempo\nmkdir -p data/minio/vm\n\n\n# Start MinIO\ndocker compose up -d\n</code></pre> <p>This will start a MinIO server accessible at:</p> <ul> <li>API endpoint: <code>http://localhost:9000</code> (for S3 clients)</li> <li>Web Console: <code>http://localhost:9001</code> (for administration)</li> </ul>"},{"location":"storage/#creating-buckets","title":"Creating Buckets","text":"<p>After installation, create the required buckets:</p> <pre><code># Install MinIO client\ncurl https://dl.min.io/client/mc/release/linux-amd64/mc \\\n  --create-dirs \\\n  -o $HOME/bin/mc &amp;&amp; \\\n  chmod +x $HOME/bin/mc\n\n# Configure MinIO client\nmc alias set local http://localhost:9000 minioadmin minioadmin\n\n# Create buckets\nmc mb local/velero-dev\nmc mb local/velero-stg\nmc mb local/velero-prod\nmc mb local/loki\nmc mb local/tempo\nmc mb local/vm\n</code></pre>"},{"location":"storage/#bucket-configuration","title":"Bucket Configuration","text":"<p>MinIO is configured with dedicated buckets for each service:</p> Service Bucket Name Purpose Velero velero-{env} Kubernetes backup and restore Loki loki Log storage and querying Tempo tempo Distributed tracing storage Victoria Metrics vm Long-term metrics storage"},{"location":"storage/#service-integrations","title":"Service Integrations","text":""},{"location":"storage/#velero-backup-and-restore","title":"Velero Backup and Restore","text":"<p>Velero uses MinIO for storing Kubernetes cluster backups:</p> <pre><code># Install Velero with MinIO storage\nvelero install \\\n  --provider aws \\\n  --plugins velero/velero-plugin-for-aws:v1.7.0 \\\n  --bucket velero-dev \\\n  --secret-file ./credentials-velero \\\n  --use-volume-snapshots=false \\\n  --backup-location-config region=minio,s3ForcePathStyle=\"true\",s3Url=http://&lt;docker-host-ip&gt;:9000\n</code></pre>"},{"location":"storage/#loki-log-storage","title":"Loki Log Storage","text":"<p>Loki is configured to use MinIO for persistent log storage:</p> <pre><code>loki:\n  storage:\n    bucketNames:\n      chunks: loki\n      ruler: loki\n      admin: loki\n    type: s3\n    s3:\n      endpoint: &lt;docker-host-ip&gt;:9000\n      region: minio\n      secretAccessKey: minioadmin\n      accessKeyId: minioadmin\n      s3ForcePathStyle: true\n      insecure: true\n</code></pre>"},{"location":"storage/#tempo-tracing-backend","title":"Tempo Tracing Backend","text":"<p>Tempo uses MinIO for storing distributed traces:</p> <pre><code>tempo:\n  storage:\n    trace:\n      backend: s3\n      s3:\n        endpoint: &lt;docker-host-ip&gt;:9000\n        bucket: tempo\n        access_key: minioadmin\n        secret_key: minioadmin\n        insecure: true\n</code></pre>"},{"location":"storage/#victoria-metrics-long-term-storage","title":"Victoria Metrics Long-term Storage","text":"<p>Victoria Metrics uses MinIO for long-term metrics storage:</p> <pre><code>vmstorage:\n  persistentVolume:\n    enabled: true\n  extraArgs:\n    storageDataPath: \"/storage\"\n    retentionPeriod: \"3m\"\n  s3:\n    enabled: true\n    endpoint: \"http://&lt;docker-host-ip&gt;:9000\"\n    bucket: \"vm\"\n    accessKey: \"minioadmin\"\n    secretKey: \"minioadmin\"\n    region: \"minio\"\n</code></pre>"},{"location":"storage/#references","title":"References","text":"<ul> <li>MinIO Documentation</li> <li>Velero with MinIO</li> <li>Loki Storage</li> <li>Tempo Storage</li> <li>Victoria Metrics Storage</li> </ul>"},{"location":"streams/","title":"Message Streaming and Event Brokers","text":"<p>This document provides an overview of the message streaming and event broker options supported in the DoKa Seca. We support three major messaging systems via their respective Kubernetes operators: Strimzi (for Apache Kafka), NATS, and RabbitMQ.</p>"},{"location":"streams/#overview","title":"Overview","text":"<p>Messaging systems play a critical role in modern distributed architectures, enabling asynchronous communication, event-driven patterns, and data streaming. DoKa Seca provides support for three complementary messaging systems, each with different strengths:</p> System Best For Key Features Performance Profile Apache Kafka (Strimzi) High-throughput data streaming, event sourcing Strong ordering, persistence, replay capability High throughput, higher latency NATS Lightweight messaging, request/reply, pub/sub Simple, fast, large fan-out Ultra-low latency, moderate throughput RabbitMQ Traditional message queueing, complex routing Rich routing capabilities, flexible delivery guarantees Moderate latency, good throughput"},{"location":"streams/#strimzi-apache-kafka","title":"Strimzi (Apache Kafka)","text":"<p>Strimzi provides a way to run an Apache Kafka cluster on Kubernetes using custom resources and operators.</p>"},{"location":"streams/#installation","title":"Installation","text":"<pre><code># Add the Strimzi Helm repository\nhelm repo add strimzi https://strimzi.io/charts/\nhelm repo update\n\n# Install the Strimzi operator\nhelm install strimzi-kafka-operator strimzi/strimzi-kafka-operator \\\n  --namespace strimzi-system \\\n  --create-namespace \\\n  --version 0.39.0\n</code></pre>"},{"location":"streams/#deploying-a-kafka-cluster","title":"Deploying a Kafka Cluster","text":"<p>Create a Kafka cluster using the following Custom Resource:</p> <pre><code>apiVersion: kafka.strimzi.io/v1beta2\nkind: Kafka\nmetadata:\n  name: my-kafka-cluster\n  namespace: kafka\nspec:\n  kafka:\n    version: 3.6.0\n    replicas: 3\n    listeners:\n      - name: plain\n        port: 9092\n        type: internal\n        tls: false\n      - name: tls\n        port: 9093\n        type: internal\n        tls: true\n      - name: external\n        port: 9094\n        type: nodeport\n        tls: false\n    config:\n      offsets.topic.replication.factor: 3\n      transaction.state.log.replication.factor: 3\n      transaction.state.log.min.isr: 2\n      default.replication.factor: 3\n      min.insync.replicas: 2\n      inter.broker.protocol.version: \"3.6\"\n    storage:\n      type: jbod\n      volumes:\n      - id: 0\n        type: persistent-claim\n        size: 100Gi\n        deleteClaim: false\n  zookeeper:\n    replicas: 3\n    storage:\n      type: persistent-claim\n      size: 10Gi\n      deleteClaim: false\n  entityOperator:\n    topicOperator: {}\n    userOperator: {}\n</code></pre>"},{"location":"streams/#creating-topics","title":"Creating Topics","text":"<pre><code>apiVersion: kafka.strimzi.io/v1beta2\nkind: KafkaTopic\nmetadata:\n  name: my-topic\n  namespace: kafka\n  labels:\n    strimzi.io/cluster: my-kafka-cluster\nspec:\n  partitions: 10\n  replicas: 3\n  config:\n    retention.ms: 604800000\n    segment.bytes: 1073741824\n</code></pre>"},{"location":"streams/#accessing-kafka","title":"Accessing Kafka","text":"<pre><code># Port forward the Kafka service\nkubectl -n kafka port-forward svc/my-kafka-cluster-kafka-bootstrap 9092:9092\n\n# Use kafkacat to produce messages\necho \"Hello, Kafka!\" | kcat -b localhost:9092 -t my-topic -P\n\n# Consume messages\nkcat -b localhost:9092 -t my-topic -C\n</code></pre>"},{"location":"streams/#nats","title":"NATS","text":"<p>NATS is a simple, secure and high-performance messaging system designed for cloud native applications, IoT messaging, and microservices architectures.</p>"},{"location":"streams/#installing-nats-operator","title":"Installing NATS Operator","text":"<pre><code># Add the NATS Helm repository\nhelm repo add nats https://nats-io.github.io/k8s/helm/charts/\nhelm repo update\n\n# Install the NATS operator\nhelm install nats-operator nats/nats-operator \\\n  --namespace nats-system \\\n  --create-namespace\n</code></pre>"},{"location":"streams/#deploying-a-nats-cluster","title":"Deploying a NATS Cluster","text":"<p>Create a NATS cluster with the following manifest:</p> <pre><code>apiVersion: nats.io/v1alpha2\nkind: NatsCluster\nmetadata:\n  name: nats-cluster\n  namespace: nats\nspec:\n  size: 3\n  version: \"2.10.7\"\n  pod:\n    enableConfigReload: true\n  auth:\n    enableServiceAccounts: true\n  jetstream:\n    enabled: true\n    memStorage:\n      enabled: true\n      size: 1Gi\n    fileStorage:\n      enabled: true\n      storageClassName: standard\n      size: 10Gi\n</code></pre>"},{"location":"streams/#creating-streams-and-consumers","title":"Creating Streams and Consumers","text":"<p>With JetStream enabled, you can create streams and consumers:</p> <pre><code># Port forward the NATS service\nkubectl -n nats port-forward svc/nats-cluster 4222:4222\n\n# Create a stream\nnats stream add my-stream \\\n  --subjects=\"orders.*\" \\\n  --retention=limits \\\n  --storage=file \\\n  --replicas=3 \\\n  --max-msgs=-1 \\\n  --max-bytes=-1 \\\n  --max-age=1y \\\n  --max-msg-size=-1 \\\n  --dupe-window=2m\n\n# Create a consumer\nnats consumer add my-stream my-consumer \\\n  --filter=\"orders.created\" \\\n  --ack=explicit \\\n  --pull \\\n  --deliver=all \\\n  --max-deliver=10 \\\n  --replay=instant\n</code></pre>"},{"location":"streams/#rabbitmq","title":"RabbitMQ","text":"<p>RabbitMQ is a widely deployed open source message broker that implements several messaging protocols.</p>"},{"location":"streams/#installing-rabbitmq-operator","title":"Installing RabbitMQ Operator","text":"<pre><code># Add the RabbitMQ Helm repository\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n\n# Install the RabbitMQ Cluster Operator\nkubectl apply -f \"https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml\"\n</code></pre>"},{"location":"streams/#deploying-a-rabbitmq-cluster","title":"Deploying a RabbitMQ Cluster","text":"<p>Create a RabbitMQ cluster using the following manifest:</p> <pre><code>apiVersion: rabbitmq.com/v1beta1\nkind: RabbitmqCluster\nmetadata:\n  name: rabbitmq-cluster\n  namespace: rabbitmq\nspec:\n  replicas: 3\n  resources:\n    requests:\n      cpu: 500m\n      memory: 1Gi\n    limits:\n      cpu: 1\n      memory: 2Gi\n  rabbitmq:\n    additionalConfig: |\n      cluster_partition_handling = autoheal\n      vm_memory_high_watermark.relative = 0.8\n  persistence:\n    storageClassName: standard\n    storage: 10Gi\n  service:\n    type: ClusterIP\n  terminationGracePeriodSeconds: 180\n</code></pre>"},{"location":"streams/#creating-vhosts-queues-and-exchanges","title":"Creating Vhosts, Queues, and Exchanges","text":"<p>After the cluster is deployed, you can create vhosts, queues, exchanges, and bindings using the RabbitMQ management UI or the CLI:</p> <pre><code># Port forward the RabbitMQ management UI\nkubectl -n rabbitmq port-forward svc/rabbitmq-cluster 15672:15672\n\n# Get the admin credentials\nRABBITMQ_USER=\"$(kubectl -n rabbitmq get secret rabbitmq-cluster-default-user -o jsonpath='{.data.username}' | base64 --decode)\"\nRABBITMQ_PASS=\"$(kubectl -n rabbitmq get secret rabbitmq-cluster-default-user -o jsonpath='{.data.password}' | base64 --decode)\"\n\n# Use the RabbitMQ CLI\nkubectl -n rabbitmq exec rabbitmq-cluster-server-0 -- rabbitmqadmin \\\n  -u \"$RABBITMQ_USER\" -p \"$RABBITMQ_PASS\" \\\n  declare vhost name=my-vhost\n\n# Create a queue\nkubectl -n rabbitmq exec rabbitmq-cluster-server-0 -- rabbitmqadmin \\\n  -u \"$RABBITMQ_USER\" -p \"$RABBITMQ_PASS\" \\\n  declare queue name=my-queue vhost=my-vhost durable=true\n\n# Create an exchange\nkubectl -n rabbitmq exec rabbitmq-cluster-server-0 -- rabbitmqadmin \\\n  -u \"$RABBITMQ_USER\" -p \"$RABBITMQ_PASS\" \\\n  declare exchange name=my-exchange type=topic vhost=my-vhost durable=true\n</code></pre>"},{"location":"streams/#integration-with-applications","title":"Integration with Applications","text":""},{"location":"streams/#application-example-using-kafka","title":"Application Example Using Kafka","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kafka-consumer-app\n  namespace: kafka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kafka-consumer-app\n  template:\n    metadata:\n      labels:\n        app: kafka-consumer-app\n    spec:\n      containers:\n      - name: consumer\n        image: myapp/kafka-consumer:latest\n        env:\n        - name: KAFKA_BOOTSTRAP_SERVERS\n          value: my-kafka-cluster-kafka-bootstrap:9092\n        - name: KAFKA_TOPIC\n          value: my-topic\n</code></pre>"},{"location":"streams/#application-example-using-nats","title":"Application Example Using NATS","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nats-publisher-app\n  namespace: nats\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nats-publisher-app\n  template:\n    metadata:\n      labels:\n        app: nats-publisher-app\n    spec:\n      containers:\n      - name: publisher\n        image: myapp/nats-publisher:latest\n        env:\n        - name: NATS_URL\n          value: nats://nats-cluster:4222\n        - name: NATS_SUBJECT\n          value: orders.created\n</code></pre>"},{"location":"streams/#application-example-using-rabbitmq","title":"Application Example Using RabbitMQ","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rabbitmq-worker-app\n  namespace: rabbitmq\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: rabbitmq-worker-app\n  template:\n    metadata:\n      labels:\n        app: rabbitmq-worker-app\n    spec:\n      containers:\n      - name: worker\n        image: myapp/rabbitmq-worker:latest\n        env:\n        - name: RABBITMQ_HOST\n          value: rabbitmq-cluster\n        - name: RABBITMQ_USER\n          valueFrom:\n            secretKeyRef:\n              name: rabbitmq-cluster-default-user\n              key: username\n        - name: RABBITMQ_PASS\n          valueFrom:\n            secretKeyRef:\n              name: rabbitmq-cluster-default-user\n              key: password\n</code></pre>"},{"location":"streams/#monitoring-and-management","title":"Monitoring and Management","text":""},{"location":"streams/#accessing-management-uis","title":"Accessing Management UIs","text":"<pre><code># Access RabbitMQ management UI\nkubectl -n rabbitmq port-forward svc/rabbitmq-cluster 15672:15672\n\n# Access NATS Dashboard\nkubectl -n nats port-forward svc/nats-cluster 8222:8222\n</code></pre>"},{"location":"streams/#best-practices","title":"Best Practices","text":"<ol> <li>Selecting the Right Messaging System:</li> <li>For high-throughput data pipelines: Kafka</li> <li>For low-latency service-to-service communication: NATS</li> <li> <p>For complex routing needs and traditional message queueing: RabbitMQ</p> </li> <li> <p>Resource Allocation:</p> </li> <li>Kafka: Allocate more disk and memory, especially for high-volume workloads</li> <li>NATS: Optimize for network I/O, requires less memory</li> <li> <p>RabbitMQ: Balance between memory and CPU, monitor queue depths</p> </li> <li> <p>High Availability:</p> </li> <li>Deploy at least 3 nodes for each broker in production</li> <li>Use proper anti-affinity rules to distribute across nodes</li> <li> <p>Configure appropriate replication factors</p> </li> <li> <p>Monitoring:</p> </li> <li>Set up Prometheus exporters for each messaging system</li> <li>Create Grafana dashboards to monitor throughput, latency, and errors</li> <li>Configure alerts for queue depth, consumer lag, and broker health</li> </ol>"},{"location":"streams/#references","title":"References","text":"<ul> <li>Strimzi Documentation</li> <li>Apache Kafka Documentation</li> <li>NATS Documentation</li> <li>NATS Kubernetes Operator</li> <li>RabbitMQ Documentation</li> <li>RabbitMQ Cluster Operator</li> <li>Event-Driven Architecture Patterns</li> </ul>"},{"location":"vault-setup/","title":"HashiCorp Vault Setup Guide","text":"<p>This document provides step-by-step instructions for setting up HashiCorp Vault using Docker Compose with persistent storage.</p>"},{"location":"vault-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose installed</li> <li>Basic understanding of Vault concepts</li> <li>Terminal access</li> </ul>"},{"location":"vault-setup/#architecture","title":"Architecture","text":"<ul> <li>Storage Backend: File-based storage with Docker volume persistence</li> <li>Network: HTTP (TLS disabled for local development)</li> <li>UI: Enabled and accessible via web browser</li> <li>Persistence: All secrets stored in <code>vault-data</code> Docker volume</li> </ul>"},{"location":"vault-setup/#setup-instructions","title":"Setup Instructions","text":""},{"location":"vault-setup/#1-start-vault-service","title":"1. Start Vault Service","text":"<pre><code>docker compose up -d vault\n</code></pre>"},{"location":"vault-setup/#2-verify-vault-is-running","title":"2. Verify Vault is Running","text":"<p>Check the logs to ensure Vault started successfully:</p> <pre><code>docker logs vault\n</code></pre> <p>You should see logs indicating the server started, but will show \"security barrier not initialized\" - this is expected.</p>"},{"location":"vault-setup/#3-initialize-vault","title":"3. Initialize Vault","text":"<p>Initialize Vault to generate unseal keys and root token:</p> <pre><code>docker exec -it vault sh -c 'VAULT_ADDR=http://127.0.0.1:8200 vault operator init'\n</code></pre> <p>Important: Save the output securely! You'll get:</p> <ul> <li>5 unseal keys (need 3 to unseal)</li> <li>1 root token (for initial admin access)</li> </ul>"},{"location":"vault-setup/#4-unseal-vault","title":"4. Unseal Vault","text":"<p>Vault needs to be unsealed with 3 of the 5 keys every time it starts:</p> <pre><code># First unseal key\ndocker exec -it vault sh -c 'VAULT_ADDR=http://127.0.0.1:8200 vault operator unseal &lt;UNSEAL_KEY_1&gt;'\n\n# Second unseal key\ndocker exec -it vault sh -c 'VAULT_ADDR=http://127.0.0.1:8200 vault operator unseal &lt;UNSEAL_KEY_2&gt;'\n\n# Third unseal key (Vault will be unsealed after this)\ndocker exec -it vault sh -c 'VAULT_ADDR=http://127.0.0.1:8200 vault operator unseal &lt;UNSEAL_KEY_3&gt;'\n</code></pre>"},{"location":"vault-setup/#5-verify-vault-status","title":"5. Verify Vault Status","text":"<p>Check that Vault is fully operational:</p> <pre><code>docker exec -it vault sh -c 'VAULT_ADDR=http://127.0.0.1:8200 vault status'\n</code></pre> <p>Look for <code>Sealed: false</code> in the output.</p>"},{"location":"vault-setup/#access-methods","title":"Access Methods","text":""},{"location":"vault-setup/#web-ui","title":"Web UI","text":"<ul> <li>URL: http://localhost:8200</li> <li>Login: Use the root token from initialization</li> </ul>"},{"location":"vault-setup/#command-line","title":"Command Line","text":"<pre><code># Set environment variable for convenience\nexport VAULT_ADDR=http://localhost:8200\n\n# Login with root token\nvault auth -method=token token=&lt;ROOT_TOKEN&gt;\n\n# Or run commands directly in container\ndocker exec -it vault sh -c 'VAULT_ADDR=http://127.0.0.1:8200 vault &lt;COMMAND&gt;'\n</code></pre>"},{"location":"vault-setup/#configuration-files","title":"Configuration Files","text":""},{"location":"vault-setup/#docker-composeyml","title":"docker-compose.yml","text":"<p>Located at project root, contains:</p> <ul> <li>Vault service definition</li> <li>Volume mounts for data persistence</li> <li>Network configuration</li> </ul>"},{"location":"vault-setup/#dockervaultlocaljson","title":"docker/vault/local.json","text":"<p>Vault configuration file with:</p> <ul> <li>File storage backend</li> <li>HTTP listener (TLS disabled)</li> <li>UI enabled</li> <li>Logging configuration</li> </ul>"},{"location":"vault-setup/#maintenance-tasks","title":"Maintenance Tasks","text":""},{"location":"vault-setup/#stop-vault","title":"Stop Vault","text":"<pre><code>docker compose down vault\n</code></pre>"},{"location":"vault-setup/#start-existing-vault","title":"Start Existing Vault","text":"<pre><code>docker compose up -d vault\n# Then unseal with 3 keys (same process as step 4 above)\n</code></pre>"},{"location":"vault-setup/#view-logs","title":"View Logs","text":"<pre><code>docker logs vault -f\n</code></pre>"},{"location":"vault-setup/#backup-vault-data","title":"Backup Vault Data","text":"<pre><code># Create backup of vault data volume\ndocker run --rm -v vault-data:/data -v $(pwd):/backup ubuntu tar czf /backup/vault-backup.tar.gz -C /data .\n</code></pre>"},{"location":"vault-setup/#restore-vault-data","title":"Restore Vault Data","text":"<pre><code># Restore from backup\ndocker run --rm -v vault-data:/data -v $(pwd):/backup ubuntu tar xzf /backup/vault-backup.tar.gz -C /data\n</code></pre>"},{"location":"vault-setup/#security-considerations","title":"Security Considerations","text":"<p>\u26a0\ufe0f Important Security Notes:</p> <ol> <li>Unseal Keys: Store the 5 unseal keys securely and separately</li> <li>Root Token: Change or revoke the initial root token after setup</li> <li>TLS: This setup uses HTTP for local development - enable TLS for production</li> <li>Access Control: Implement proper authentication methods beyond root token</li> <li>Backups: Regularly backup the vault data volume</li> <li>Network: Restrict network access in production environments</li> </ol>"},{"location":"vault-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"vault-setup/#common-issues","title":"Common Issues","text":"<p>Vault won't start:</p> <ul> <li>Check Docker logs: <code>docker logs vault</code></li> <li>Verify configuration file syntax</li> <li>Ensure ports aren't in use</li> </ul> <p>Cannot connect to Vault:</p> <ul> <li>Verify Vault is unsealed: <code>vault status</code></li> <li>Check VAULT_ADDR environment variable</li> <li>Ensure using HTTP not HTTPS for local setup</li> </ul> <p>Lost unseal keys:</p> <ul> <li>If you have root token, you can rekey: <code>vault operator rekey</code></li> <li>Otherwise, you'll need to reinitialize (data loss)</li> </ul> <p>Permission errors:</p> <ul> <li>Check Docker volume permissions</li> <li>Verify container user has access to mounted directories</li> </ul>"},{"location":"vault-setup/#useful-commands","title":"Useful Commands","text":"<pre><code># Check seal status\nvault status\n\n# List enabled auth methods\nvault auth list\n\n# List enabled secret engines\nvault secrets list\n\n# Read vault configuration\nvault read sys/config/state/sanitized\n\n# Generate new unseal keys (requires existing keys)\nvault operator rekey\n</code></pre>"},{"location":"vault-setup/#production-considerations","title":"Production Considerations","text":"<p>For production use, consider:</p> <ol> <li>TLS Configuration: Enable HTTPS with proper certificates</li> <li>High Availability: Use Consul or other HA storage backend</li> <li>Auto-unseal: Configure auto-unseal with cloud HSM or transit</li> <li>Monitoring: Set up monitoring and alerting</li> <li>Access Policies: Implement least-privilege access policies</li> <li>Regular Backups: Automated backup strategy</li> <li>Network Security: Firewall rules and network segmentation</li> </ol>"},{"location":"vault-setup/#example-initial-setup-script","title":"Example Initial Setup Script","text":"<pre><code>#!/bin/bash\nset -e\n\necho \"Starting Vault...\"\ndocker compose up -d vault\n\necho \"Waiting for Vault to start...\"\nsleep 10\n\necho \"Initializing Vault...\"\nINIT_OUTPUT=$(docker exec -it vault sh -c 'VAULT_ADDR=http://127.0.0.1:8200 vault operator init')\necho \"$INIT_OUTPUT\"\n\n# Extract keys and token (you should do this more securely)\nUNSEAL_KEY_1=$(echo \"$INIT_OUTPUT\" | grep \"Unseal Key 1:\" | awk '{print $4}')\nUNSEAL_KEY_2=$(echo \"$INIT_OUTPUT\" | grep \"Unseal Key 2:\" | awk '{print $4}')\nUNSEAL_KEY_3=$(echo \"$INIT_OUTPUT\" | grep \"Unseal Key 3:\" | awk '{print $4}')\n\necho \"Unsealing Vault...\"\ndocker exec -it vault sh -c \"VAULT_ADDR=http://127.0.0.1:8200 vault operator unseal $UNSEAL_KEY_1\"\ndocker exec -it vault sh -c \"VAULT_ADDR=http://127.0.0.1:8200 vault operator unseal $UNSEAL_KEY_2\"\ndocker exec -it vault sh -c \"VAULT_ADDR=http://127.0.0.1:8200 vault operator unseal $UNSEAL_KEY_3\"\n\necho \"Vault is ready!\"\ndocker exec -it vault sh -c 'VAULT_ADDR=http://127.0.0.1:8200 vault status'\n</code></pre>"},{"location":"adr/","title":"Architecture Decision Records (ADRs)","text":"<p>This directory contains Architecture Decision Records (ADRs) for the DoKa Seca project.</p>"},{"location":"adr/#what-are-adrs","title":"What are ADRs?","text":"<p>Architecture Decision Records are short text documents that capture an important architectural decision made along with its context and consequences. They help teams understand why certain decisions were made and provide historical context for future changes.</p>"},{"location":"adr/#adr-list","title":"ADR List","text":"<ul> <li>Use Architecture Decision Records</li> <li>External Secrets Operator Multi-tenancy</li> <li>ArgoCD Hub and Spoke Configuration</li> <li>Use ArgoCD, vCluster, and GitHub Actions for Preview Environments</li> </ul>"},{"location":"adr/#adr-template","title":"ADR Template","text":"<p>When creating new ADRs, use the following template:</p> <pre><code># {title}\n\nDate: {date}\n\n## Status\n\n{status}\n\n## Context\n\nWhat is the issue that we're seeing that is motivating this decision or change?\n\n## Decision\n\nWhat is the change that we're proposing and/or doing?\n\n## Consequences\n\nWhat becomes easier or more difficult to do because of this change?\n\n## References\n</code></pre>"},{"location":"adr/#guidelines","title":"Guidelines","text":"<ol> <li>ADRs should be numbered sequentially (001, 002, etc.) in the filename</li> <li>Titles should be descriptive and action-oriented</li> <li>Include the date when the decision was made</li> <li>Keep ADRs concise but complete</li> <li>Update the index when adding new ADRs</li> <li>ADRs are immutable once accepted - create new ADRs to supersede old ones</li> </ol>"},{"location":"adr/001-use-architecture-decision-records/","title":"Use Architecture Decision Records","text":"<p>Date: 2025-07-07</p>"},{"location":"adr/001-use-architecture-decision-records/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/001-use-architecture-decision-records/#context","title":"Context","text":"<p>The DoKa Seca project is a comprehensive framework for bootstrapping cloud-native platforms, involving numerous architectural decisions around infrastructure choices, tooling selections, deployment patterns, and platform configurations. As the project evolves and the team grows, we're seeing several issues:</p> <ol> <li>Loss of knowledge: Understanding why certain architectural choices were made becomes unclear over time</li> <li>Repeated discussions: The same decisions get debated multiple times without remembering previous conclusions</li> <li>Onboarding difficulty: New team members and contributors struggle to understand the reasoning behind current architecture</li> <li>Inconsistent decisions: Without documented principles, new decisions may conflict with established patterns</li> <li>Review challenges: Architectural changes lack structured evaluation criteria</li> </ol> <p>Without a systematic approach to documenting architectural decisions, the project risks loss of institutional knowledge, repeated discussions about previously settled matters, inconsistent architectural approaches, difficulty onboarding new contributors, and unclear rationale for existing design choices.</p>"},{"location":"adr/001-use-architecture-decision-records/#decision","title":"Decision","text":"<p>We will use Architecture Decision Records (ADRs) to document all significant architectural decisions in the DoKa Seca project.</p> <p>What qualifies as an architectural decision:</p> <ul> <li>Choice of technologies, frameworks, or tools</li> <li>Infrastructure patterns and deployment strategies</li> <li>Security policies and implementation approaches</li> <li>GitOps workflow and repository structure decisions</li> <li>Platform configuration standards and conventions</li> <li>Integration patterns between components</li> <li>Breaking changes to existing interfaces or workflows</li> </ul> <p>ADR Process:</p> <ol> <li>Create ADRs for all new significant architectural decisions</li> <li>Use the standard ADR format (Status, Context, Decision, Consequences)</li> <li>Store ADRs in the <code>/docs/adr/</code> directory</li> <li>Number ADRs sequentially (001, 002, etc.)</li> <li>Include ADRs in pull request reviews for architectural changes</li> <li>Update the ADR index when adding new records</li> </ol>"},{"location":"adr/001-use-architecture-decision-records/#consequences","title":"Consequences","text":"<p>What becomes easier:</p> <ul> <li>New contributors can understand the reasoning behind current architecture through documented context</li> <li>Team members avoid revisiting previously decided matters, reducing decision fatigue</li> <li>Architectural decisions maintain consistency through explicit documentation of principles and patterns</li> <li>Code reviews include structured evaluation of architectural impacts</li> <li>Historical evolution of the platform architecture is preserved and traceable</li> </ul> <p>What becomes more difficult:</p> <ul> <li>Writing ADRs requires additional time and effort from the team during development</li> <li>Team members must remember to create ADRs for qualifying decisions, adding process overhead</li> <li>ADR index and cross-references need ongoing maintenance to stay current</li> <li>Risk of over-documenting minor decisions if scope isn't well-defined, potentially creating bureaucracy</li> </ul>"},{"location":"adr/001-use-architecture-decision-records/#references","title":"References","text":"<ul> <li>Documenting Architecture Decisions by Michael Nygard</li> <li>ADR GitHub Organization</li> <li>Architecture Decision Records: A Practical Guide</li> </ul>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/","title":"External Secrets Operator Multi-Tenancy Configuration","text":"<p>Date: 2025-07-11</p>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#context","title":"Context","text":"<p>DoKa Seca requires a secure and scalable secret management solution that supports multi-tenancy across different teams and environments. The current challenges include:</p> <ul> <li>Secret Management Complexity: Manual secret management across multiple namespaces and environments</li> <li>Security Isolation: Need for proper tenant isolation to prevent cross-namespace secret access</li> <li>Zero Secret Problem: Chicken-and-egg problem where External Secrets Operator needs initial secrets to access external secret stores</li> <li>Vault Integration: Requirement to integrate with HashiCorp Vault as the central secret store</li> <li>Scalability: Solution must scale across multiple teams, namespaces, and environments</li> <li>Compliance: Need for proper audit trails and access controls for secret access</li> </ul> <p>The platform needs a solution that provides tenant isolation while solving the bootstrap problem of initial secret provisioning.</p>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#decision","title":"Decision","text":"<p>We will implement External Secrets Operator (ESO) with a multi-tenancy configuration using the following architecture:</p>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#1-cluster-level-bootstrap-configuration","title":"1. Cluster-Level Bootstrap Configuration","text":"<p>Deploy a single Cluster Secret Store (<code>vault-infra</code>) that has elevated privileges to:</p> <ul> <li>Access a dedicated bootstrap path in Vault (<code>/secret/platform/bootstrap/</code>)</li> <li>Create and manage initial authentication secrets for tenant-specific Secret Stores</li> <li>Provide the initial secrets needed to solve the zero secret problem</li> </ul>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#2-namespace-level-tenant-isolation","title":"2. Namespace-Level Tenant Isolation","text":"<p>Deploy a Secret Store (<code>vault-{tenant-name}</code>) in each tenant namespace that:</p> <ul> <li>Has access only to tenant-specific paths in Vault (<code>/secret/tenants/{namespace}/</code>)</li> <li>Uses authentication credentials provided by the Cluster Secret Store</li> <li>Provides isolated secret management for each tenant</li> </ul>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#3-vault-path-structure","title":"3. Vault Path Structure","text":"<p>Organize Vault secrets with clear tenant boundaries:</p> <pre><code>/secret/\n\u251c\u2500\u2500 tenants/\n\u2502   \u251c\u2500\u2500 tenant-a/\n\u2502   \u2502   \u251c\u2500\u2500 database-credentials\n\u2502   \u2502   \u2514\u2500\u2500 api-keys\n\u2502   \u251c\u2500\u2500 tenant-b/\n\u2502   \u2502   \u251c\u2500\u2500 database-credentials\n\u2502   \u2502   \u2514\u2500\u2500 certificates\n\u2502   \u2514\u2500\u2500 tenant-c/\n\u2502       \u251c\u2500\u2500 database-credentials\n\u2502       \u2514\u2500\u2500 certificates\n\u2514\u2500\u2500 platform/                    # Platform team exclusive access\n    \u251c\u2500\u2500 bootstrap/               # Contains tenant Secret Store auth credentials\n    \u2502   \u251c\u2500\u2500 tenant-a-auth        # Auth credentials for tenant-a Secret Store\n    \u2502   \u251c\u2500\u2500 tenant-b-auth        # Auth credentials for tenant-b Secret Store\n    \u2502   \u2514\u2500\u2500 tenant-c-auth        # Auth credentials for tenant-c Secret Store\n    \u251c\u2500\u2500 shared-services/  \n    \u2502   \u251c\u2500\u2500 monitoring-config\n    \u2502   \u251c\u2500\u2500 backup-credentials\n    \u2502   \u2514\u2500\u2500 cluster-certificates\n    \u2514\u2500\u2500 infrastructure/\n        \u251c\u2500\u2500 terraform-state-backend\n        \u2514\u2500\u2500 registry-credentials\n</code></pre> <p>Note: The <code>bootstrap/</code> folder is nested under the <code>platform/</code> path, ensuring that only the platform team has access to tenant authentication credentials. This maintains the security boundary between platform operations and tenant operations.</p>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#4-naming-conventions","title":"4. Naming Conventions","text":"<p>The multi-tenancy configuration uses a consistent naming scheme:</p> <ul> <li>Cluster Secret Store: <code>vault-infra</code> - Manages platform-level infrastructure secrets and bootstrap credentials</li> <li>Tenant Secret Stores: <code>vault-{tenant-name}</code> - Each tenant has a dedicated Secret Store named after the tenant</li> <li>Service Accounts:</li> <li>Infrastructure: <code>external-secrets-infra</code> (in <code>external-secrets</code> namespace)</li> <li>Tenant: <code>external-secrets-{tenant-name}</code> (in respective tenant namespace)</li> <li>Vault Roles:</li> <li>Bootstrap: <code>bootstrap-role</code> (for infrastructure access)</li> <li>Tenant: <code>{tenant-namespace}-role</code> (for tenant-specific access)</li> </ul> <p>This naming convention ensures clear separation between infrastructure and tenant resources while maintaining consistency across the platform.</p>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#5-implementation-components","title":"5. Implementation Components","text":""},{"location":"adr/002-external-secrets-operator-multi-tenancy/#cluster-secret-store-configuration","title":"Cluster Secret Store Configuration","text":"<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ClusterSecretStore\nmetadata:\n  name: vault-infra\nspec:\n  provider:\n    vault:\n      server: \"https://vault.dokaseca.local\"\n      path: \"secret\"\n      version: \"v2\"\n      auth:\n        kubernetes:\n          mountPath: \"kubernetes\"\n          role: \"bootstrap-role\"\n          serviceAccountRef:\n            name: \"external-secrets-infra\"\n            namespace: \"external-secrets\"\n</code></pre>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#namespace-secret-store-template","title":"Namespace Secret Store Template","text":"<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: vault-${TENANT_NAME}\n  namespace: ${TENANT_NAMESPACE}\nspec:\n  provider:\n    vault:\n      server: \"https://vault.dokaseca.local\"\n      path: \"secret\"\n      version: \"v2\"\n      auth:\n        kubernetes:\n          mountPath: \"kubernetes\"\n          role: \"${TENANT_NAMESPACE}-role\"\n          serviceAccountRef:\n            name: \"external-secrets-${TENANT_NAME}\"\n            namespace: ${TENANT_NAMESPACE}\n</code></pre>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#6-vault-policies-and-roles","title":"6. Vault Policies and Roles","text":""},{"location":"adr/002-external-secrets-operator-multi-tenancy/#platform-team-policy","title":"Platform Team Policy","text":"<pre><code># Platform team policy for full Vault access\npath \"secret/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"]\n}\npath \"secret/data/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"delete\"]\n}\npath \"secret/metadata/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"]\n}\n</code></pre>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#bootstrap-policy","title":"Bootstrap Policy","text":"<pre><code># Bootstrap policy for cluster-wide secret store\npath \"secret/data/platform/bootstrap/*\" {\n  capabilities = [\"read\"]\n}\npath \"secret/metadata/platform/bootstrap/*\" {\n  capabilities = [\"list\", \"read\"]\n}\n</code></pre>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#tenant-policy-template","title":"Tenant Policy Template","text":"<pre><code># Policy for tenant-specific access\npath \"secret/data/tenants/{{identity.entity.aliases.MOUNT_ACCESSOR.metadata.service_account_namespace}}/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"delete\"]\n}\npath \"secret/metadata/tenants/{{identity.entity.aliases.MOUNT_ACCESSOR.metadata.service_account_namespace}}/*\" {\n  capabilities = [\"list\", \"read\", \"delete\"]\n}\n</code></pre>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#shared-services-policy","title":"Shared Services Policy","text":"<pre><code># Policy for shared services (platform team exclusive)\npath \"secret/data/platform/bootstrap/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"delete\"]\n}\npath \"secret/metadata/platform/bootstrap/*\" {\n  capabilities = [\"list\", \"read\", \"delete\"]\n}\npath \"secret/data/platform/shared-services/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"delete\"]\n}\npath \"secret/metadata/platform/shared-services/*\" {\n  capabilities = [\"list\", \"read\", \"delete\"]\n}\npath \"secret/data/platform/infrastructure/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"delete\"]\n}\npath \"secret/metadata/platform/infrastructure/*\" {\n  capabilities = [\"list\", \"read\", \"delete\"]\n}\n</code></pre>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#7-automated-deployment-process","title":"7. Automated Deployment Process","text":"<ol> <li>Bootstrap Phase: Deploy <code>vault-infra</code> Cluster Secret Store with elevated Vault access to platform paths</li> <li>Credential Generation: <code>vault-infra</code> creates authentication credentials for each tenant in <code>/secret/platform/bootstrap/</code></li> <li>Tenant Deployment: Deploy namespace-specific <code>vault-{tenant-name}</code> Secret Stores using bootstrap credentials</li> <li>Secret Synchronization: External Secrets begin syncing tenant-specific secrets from <code>/secret/tenants/{namespace}/</code></li> </ol>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#consequences","title":"Consequences","text":""},{"location":"adr/002-external-secrets-operator-multi-tenancy/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Security Isolation: Each tenant can only access their designated Vault paths</li> <li>Zero Secret Resolution: Bootstrap mechanism solves the initial secret problem</li> <li>Scalability: New tenants can be onboarded without manual secret management</li> <li>Audit Trail: Complete audit trail of secret access through Vault logs</li> <li>GitOps Integration: Secret Store configurations can be managed through GitOps</li> <li>Disaster Recovery: Secrets can be restored from Vault during cluster rebuilds</li> <li>Compliance: Proper separation of concerns meets security compliance requirements</li> </ul>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Vault Dependency: Strong dependency on Vault availability for secret operations</li> <li>Bootstrap Security: Bootstrap credentials become a critical security component</li> <li>Monitoring Overhead: Need to monitor multiple Secret Stores across namespaces</li> </ul>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#configuration-examples","title":"Configuration Examples","text":""},{"location":"adr/002-external-secrets-operator-multi-tenancy/#bootstrap-external-secret","title":"Bootstrap External Secret","text":"<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: tenant-a-bootstrap\n  namespace: external-secrets\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: vault-infra\n    kind: ClusterSecretStore\n  target:\n    name: tenant-a-vault-auth\n    namespace: tenant-a\n  data:\n  - secretKey: token\n    remoteRef:\n      key: platform/bootstrap/tenant-a-auth\n      property: token\n</code></pre>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#tenant-external-secret","title":"Tenant External Secret","text":"<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: database-credentials\n  namespace: tenant-a\nspec:\n  refreshInterval: 15m\n  secretStoreRef:\n    name: vault-tenant-a\n    kind: SecretStore\n  target:\n    name: database-secret\n    namespace: tenant-a\n  data:\n  - secretKey: username\n    remoteRef:\n      key: tenants/tenant-a/database-credentials\n      property: username\n  - secretKey: password\n    remoteRef:\n      key: tenants/tenant-a/database-credentials\n      property: password\n</code></pre>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"adr/002-external-secrets-operator-multi-tenancy/#key-metrics","title":"Key Metrics","text":"<ul> <li>Secret Store connection status</li> <li>Secret synchronization success/failure rates</li> <li>Vault authentication errors</li> <li>Secret refresh latency</li> </ul>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#alert-rules","title":"Alert Rules","text":"<pre><code>groups:\n- name: external-secrets\n  rules:\n  - alert: SecretStoreDown\n    expr: external_secrets_sync_calls_error &gt; 0\n    for: 5m\n    labels:\n      severity: critical\n    annotations:\n      summary: \"Secret Store {{ $labels.name }} is failing\"\n\n  - alert: SecretSyncFailed\n    expr: external_secrets_sync_calls_error &gt; external_secrets_sync_calls_total * 0.1\n    for: 2m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"High error rate for secret synchronization\"\n</code></pre>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#security-considerations","title":"Security Considerations","text":""},{"location":"adr/002-external-secrets-operator-multi-tenancy/#access-control","title":"Access Control","text":"<ul> <li>Platform Team: Full administrative access to all Vault paths for platform management and operations, including:</li> <li>Bootstrap credentials (<code>/secret/platform/bootstrap/</code>) for tenant Secret Store authentication</li> <li>Shared services infrastructure (<code>/secret/platform/shared-services/</code>)</li> <li>Core infrastructure secrets (<code>/secret/platform/infrastructure/</code>)</li> <li>Vault Policies: Strict path-based access control for tenant isolation</li> <li>Kubernetes RBAC: Controls Secret Store access within Kubernetes clusters</li> <li>Service Accounts: Minimal required permissions for tenant-specific operations</li> <li>Bootstrap Isolation: Bootstrap credentials stored under platform namespace ensure only the platform team can manage tenant onboarding</li> </ul>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#secret-rotation","title":"Secret Rotation","text":"<ul> <li>Automated rotation of Vault authentication tokens</li> <li>Regular rotation of bootstrap credentials</li> <li>Monitoring for expired or expiring secrets</li> </ul>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#audit-and-compliance","title":"Audit and Compliance","text":"<ul> <li>All secret access logged in Vault audit logs</li> <li>External Secrets Operator provides operation metrics</li> <li>Regular security reviews of access patterns</li> <li>Platform team actions audited through Vault's comprehensive logging</li> </ul>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#references","title":"References","text":"<ul> <li>External Secrets Operator Multi Tenancy Documentation</li> <li>HashiCorp Vault Kubernetes Auth Method</li> <li>Kubernetes Multi-Tenancy Best Practices</li> </ul>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#related-adrs","title":"Related ADRs","text":"<ul> <li>ADR-001: Use Architecture Decision Records</li> </ul>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#authors","title":"Authors","text":"<ul> <li>Platform Engineering Team</li> <li>Security Team</li> </ul>"},{"location":"adr/002-external-secrets-operator-multi-tenancy/#review-history","title":"Review History","text":"Date Reviewer Status Comments 2025-07-11 Initial Draft Initial proposal for review"},{"location":"adr/003-argocd-hub-and-spoke-configuration/","title":"ArgoCD Hub and Spoke Configuration for Multi-Cluster GitOps","text":"<p>Date: 2025-07-11</p>"},{"location":"adr/003-argocd-hub-and-spoke-configuration/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/003-argocd-hub-and-spoke-configuration/#context","title":"Context","text":"<p>DoKa Seca requires a scalable GitOps solution to manage applications and configurations across multiple Kubernetes clusters in different environments (dev, staging, production) and serving different teams. The platform needs to support:</p> <ul> <li>Multi-Cluster Management: Centralized management of multiple Kubernetes clusters</li> <li>Team Isolation: Separate development teams (team-a, team-b, team-c) with isolated workspaces</li> <li>Environment Promotion: Consistent application promotion from <code>dev \u2192 staging \u2192 production</code></li> <li>Security Boundaries: Proper RBAC and network isolation between clusters</li> <li>Operational Efficiency: Centralized monitoring and management while maintaining cluster autonomy</li> </ul> <p>The main architectural patterns considered were:</p> <ol> <li>Single ArgoCD per Cluster: Independent ArgoCD instance in each cluster</li> <li>Centralized ArgoCD: Single ArgoCD managing all clusters from one location</li> <li>Hub and Spoke: Central ArgoCD hub with satellite clusters registered as remote targets</li> </ol> <p>Each approach has different trade-offs in terms of complexity, security, and operational overhead.</p>"},{"location":"adr/003-argocd-hub-and-spoke-configuration/#decision","title":"Decision","text":"<p>DoKa Seca will implement ArgoCD in a hub and spoke configuration with the following architecture:</p>"},{"location":"adr/003-argocd-hub-and-spoke-configuration/#1-hub-cluster-control-plane","title":"1. Hub Cluster (Control Plane)","text":"<p>Deploy a central ArgoCD instance in the control-plane cluster that acts as the management hub:</p> <ul> <li>Central GitOps Controller: Single point of GitOps orchestration</li> <li>Cluster Registry: Manages connections to all spoke clusters</li> <li>Application Catalog: Centralized application definitions and policies</li> <li>Monitoring Dashboard: Unified view of all cluster deployments</li> </ul>"},{"location":"adr/003-argocd-hub-and-spoke-configuration/#2-spoke-clusters-workload-clusters","title":"2. Spoke Clusters (Workload Clusters)","text":"<p>Register workload clusters as remote targets with the hub:</p> <ul> <li>workloads-dev: Development environment cluster with team-specific namespaces</li> <li>workloads-stg: Staging environment cluster with team-specific namespaces  </li> <li>workloads-prod: Production environment cluster with team-specific namespaces</li> </ul> <p>Each workload cluster contains dedicated namespaces for team isolation:</p> <ul> <li>team-a: Team A's namespaces across environments</li> <li>team-b: Team B's namespaces across environments</li> <li>team-c: Team C's namespaces across environments</li> <li>Namespace-level RBAC: Team isolation through Kubernetes RBAC and network policies</li> </ul>"},{"location":"adr/003-argocd-hub-and-spoke-configuration/#3-gitops-repository-structure","title":"3. GitOps Repository Structure","text":"<p>Organize Git repositories to support the hub and spoke model with namespace-based team isolation:</p> <p>Workloads Repository (DoKa-seca-workloads):</p> <pre><code>\u251c\u2500\u2500 environments/\n\u2502   \u251c\u2500\u2500 dev/                     # Development environment\n\u2502   \u2502   \u251c\u2500\u2500 team-a/              # Team A applications in dev\n\u2502   \u2502   \u251c\u2500\u2500 team-b/              # Team B applications in dev\n\u2502   \u2502   \u2514\u2500\u2500 team-c/              # Team C applications in dev\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 stg/                     # Staging environment\n\u2502   \u2502   \u251c\u2500\u2500 team-a/              # Team A applications in staging\n\u2502   \u2502   \u251c\u2500\u2500 team-b/              # Team B applications in staging\n\u2502   \u2502   \u2514\u2500\u2500 team-c/              # Team C applications in staging\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 prod/                    # Production environment\n\u2502       \u251c\u2500\u2500 team-a/              # Team A applications in production\n\u2502       \u251c\u2500\u2500 team-b/              # Team B applications in production\n\u2502       \u2514\u2500\u2500 team-c/              # Team C applications in production\n\u2502\n\u2514\u2500\u2500 kustomize/                   # Kustomize overlays for environments\n</code></pre>"},{"location":"adr/003-argocd-hub-and-spoke-configuration/#4-security-model","title":"4. Security Model","text":"<p>Implement proper security boundaries:</p> <ul> <li>Service Accounts: Dedicated service accounts per cluster with minimal required permissions</li> <li>ArgoCD Projects: Team-specific projects with RBAC enforcement</li> <li>Network Policies: Secure communication between hub and spokes</li> <li>Secret Management: External Secrets Operator with Vault integration</li> </ul>"},{"location":"adr/003-argocd-hub-and-spoke-configuration/#5-application-management","title":"5. Application Management","text":"<p>Use the \"App of Apps\" pattern for scalable application management:</p> <ul> <li>Root Applications: Hub manages high-level application definitions</li> <li>Team Applications: Teams manage their specific application configurations</li> <li>Platform Applications: Shared infrastructure components managed centrally</li> </ul>"},{"location":"adr/003-argocd-hub-and-spoke-configuration/#consequences","title":"Consequences","text":""},{"location":"adr/003-argocd-hub-and-spoke-configuration/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Centralized Management: Single pane of glass for all GitOps operations across clusters</li> <li>Operational Efficiency: Reduced operational overhead compared to multiple ArgoCD instances</li> <li>Consistent Policies: Platform-wide policies and standards enforced centrally</li> <li>Team Autonomy: Teams can manage their applications while respecting platform boundaries</li> <li>Scalability: Easy to add new clusters and teams to the hub</li> <li>Monitoring: Unified monitoring and alerting for all GitOps operations</li> <li>Disaster Recovery: Hub cluster can be restored independently of workload clusters</li> <li>Cost Optimization: Single ArgoCD deployment reduces resource overhead</li> </ul>"},{"location":"adr/003-argocd-hub-and-spoke-configuration/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Single Point of Failure: Hub cluster failure affects GitOps operations across all clusters</li> <li>Network Dependency: Requires stable network connectivity between hub and spokes</li> <li>Complexity: More complex initial setup compared to single-cluster deployments</li> <li>Hub Cluster Sizing: Hub cluster must be sized to handle load from all spokes</li> <li>Security Scope: Compromise of hub cluster could affect all managed clusters</li> <li>Troubleshooting: Cross-cluster issues may be more complex to diagnose</li> </ul>"},{"location":"adr/003-argocd-hub-and-spoke-configuration/#mitigation-strategies","title":"Mitigation Strategies","text":"<ul> <li>Hub High Availability: Deploy ArgoCD hub in HA configuration with multiple replicas</li> <li>Network Resilience: Implement robust networking with failover mechanisms</li> <li>Cluster Autonomy: Spoke clusters can operate independently for critical services</li> <li>Security Hardening: Implement comprehensive security controls and monitoring</li> <li>Documentation: Maintain detailed runbooks for troubleshooting cross-cluster issues</li> <li>Backup Strategy: Regular backups of ArgoCD configurations and Git repositories</li> </ul>"},{"location":"adr/003-argocd-hub-and-spoke-configuration/#implementation-details","title":"Implementation Details","text":""},{"location":"adr/003-argocd-hub-and-spoke-configuration/#team-project-configuration","title":"Team Project Configuration","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\n  name: team-a\n  namespace: argocd\nspec:\n  description: Team A applications across all environments\n  sourceRepos:\n  - 'https://github.com/dokaseca/doka-seca-workloads/environments/*/team-a/*'\n  destinations:\n  - namespace: 'team-a'\n    server: https://workloads-dev.dokaseca.local:6443\n  clusterResourceWhitelist:\n  - group: ''\n    kind: Namespace\n  namespaceResourceWhitelist:\n  - group: '*'\n    kind: '*'\n</code></pre>"},{"location":"adr/003-argocd-hub-and-spoke-configuration/#references","title":"References","text":"<ul> <li>Distribute Your Argo CD Applications to Different Kubernetes Clusters Using Application Sets</li> <li>Amazon EKS multi-cluster gitops-bridge</li> <li>Multi-Cluster centralized hub-spoke topology</li> <li>DoKa Seca GitOps Documentation</li> <li>DoKa Seca Multi-Cluster Architecture</li> </ul>"},{"location":"adr/003-argocd-hub-and-spoke-configuration/#authors","title":"Authors","text":"<ul> <li>Platform Engineering Team</li> <li>DevOps Team</li> </ul>"},{"location":"adr/003-argocd-hub-and-spoke-configuration/#review-history","title":"Review History","text":"Date Reviewer Status Comments 2025-07-11 Initial Draft Initial proposal for ArgoCD hub and spoke architecture"},{"location":"adr/004-preview-env-argocd-vcluster/","title":"Use ArgoCD, vCluster, and GitHub Actions for Preview Environments","text":"<p>Date: 2025-07-12</p>"},{"location":"adr/004-preview-env-argocd-vcluster/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"adr/004-preview-env-argocd-vcluster/#context","title":"Context","text":"<p>We want to provide on-demand preview environments for every pull request (PR) to improve testing, validation, and feedback cycles. The preview environment should be isolated, ephemeral, and closely match production. Manual setup is slow and error-prone, and static environments are limited and can cause conflicts between PRs.</p>"},{"location":"adr/004-preview-env-argocd-vcluster/#decision","title":"Decision","text":"<p>We will use the following approach for preview environments:</p> <ul> <li>ArgoCD Pull Request Generator: Detects new PRs and generates ArgoCD Application manifests for each PR.</li> <li>vCluster: Each preview environment runs in its own lightweight virtual Kubernetes cluster (vCluster), providing strong isolation and a production-like environment without the overhead of full clusters.</li> <li>GitHub Actions: Automates the workflow, triggering on PR events to provision the vCluster, deploy the application using ArgoCD, and clean up resources when the PR is closed or merged.</li> </ul> <p>This setup ensures that every PR gets a fresh, isolated environment for testing, and the process is fully automated.</p>"},{"location":"adr/004-preview-env-argocd-vcluster/#consequences","title":"Consequences","text":"<ul> <li>Easier to test PRs: Developers and reviewers can validate changes in a realistic environment before merging.</li> <li>Automated lifecycle: Environments are created and destroyed automatically, reducing manual work and resource waste.</li> <li>Strong isolation: vCluster provides isolation between preview environments, reducing conflicts and side effects.</li> <li>Increased resource usage: Running multiple vClusters may increase resource consumption, so cluster sizing and quotas must be considered.</li> <li>Complexity: The workflow introduces more moving parts (ArgoCD, vCluster, GitHub Actions), requiring maintenance and monitoring.</li> </ul>"},{"location":"adr/004-preview-env-argocd-vcluster/#references","title":"References","text":"<ul> <li>ArgoCD Pull Request Generator</li> <li>vCluster</li> <li>GitHub Actions</li> </ul>"},{"location":"adr/adr_template/","title":"{title}","text":"<p>Date: {date}</p>"},{"location":"adr/adr_template/#status","title":"Status","text":"<p>{status}</p>"},{"location":"adr/adr_template/#context","title":"Context","text":"<p>What is the issue that we're seeing that is motivating this decision or change?</p>"},{"location":"adr/adr_template/#decision","title":"Decision","text":"<p>What is the change that we're proposing and/or doing?</p>"},{"location":"adr/adr_template/#consequences","title":"Consequences","text":"<p>What becomes easier or more difficult to do because of this change?</p>"},{"location":"adr/adr_template/#references","title":"References","text":""},{"location":"getting_started/architecture/","title":"DoKa Seca Architecture","text":""},{"location":"getting_started/architecture/#overview","title":"Overview","text":"<p>DoKa Seca is a comprehensive platform engineering framework designed around the \"dry dock\" concept - providing a controlled, isolated environment where complete cloud-native platforms can be rapidly assembled, configured, and tested. The architecture follows modern platform engineering principles with a strong emphasis on GitOps, Infrastructure as Code, and automated platform bootstrapping.</p>"},{"location":"getting_started/architecture/#core-architecture-principles","title":"Core Architecture Principles","text":""},{"location":"getting_started/architecture/#1-dry-dock-pattern","title":"1. Dry Dock Pattern","text":"<p>Just as ships are built and maintained in dry docks, DoKa Seca creates isolated local environments where entire Kubernetes platforms can be constructed with all necessary tooling and infrastructure readily available.</p>"},{"location":"getting_started/architecture/#2-gitops-first-design","title":"2. GitOps-First Design","text":"<p>All platform configuration, application deployments, and infrastructure changes are managed through Git repositories, ensuring reproducibility, auditability, and collaborative development.</p>"},{"location":"getting_started/architecture/#3-opinionated-platform-stack","title":"3. Opinionated Platform Stack","text":"<p>Provides curated, production-ready components that work together seamlessly, reducing decision fatigue and accelerating time-to-productivity.</p>"},{"location":"getting_started/architecture/#high-level-architecture","title":"High-Level Architecture","text":"<p>The DoKa Seca platform consists of several interconnected layers:</p> <pre><code>graph TB\n    Dev[Developer Workstation] --&gt; TF[Terraform/OpenTofu]\n    TF --&gt; Kind[Kind Cluster]\n    Kind --&gt; GB[GitOps Bridge]\n    GB --&gt; ArgoCD[ArgoCD]\n    ArgoCD --&gt; Apps[Platform Applications]\n\n    subgraph \"Local Environment\"\n        Kind\n        GB\n        ArgoCD\n        Apps\n    end\n\n    subgraph \"GitOps Repositories\"\n        AR[Addons Repo]\n        CR[Clusters Repo]\n        WR[Workloads Repo]\n        RR[Resources Repo]\n    end\n\n    ArgoCD --&gt; AR\n    ArgoCD --&gt; CR\n    ArgoCD --&gt; WR\n    ArgoCD --&gt; RR</code></pre>"},{"location":"getting_started/architecture/#platform-components","title":"Platform Components","text":""},{"location":"getting_started/architecture/#infrastructure-layer","title":"Infrastructure Layer","text":""},{"location":"getting_started/architecture/#kind-kubernetes-in-docker","title":"Kind (Kubernetes in Docker)","text":"<ul> <li>Purpose: Provides the foundational Kubernetes runtime</li> <li>Configuration: Multi-node clusters with custom networking</li> <li>Features:</li> <li>Port forwarding for local access</li> <li>Custom image mounting</li> <li>Ingress controller support</li> <li>Load balancer simulation</li> </ul>"},{"location":"getting_started/architecture/#terraformopentofu","title":"Terraform/OpenTofu","text":"<ul> <li>Purpose: Infrastructure as Code orchestration</li> <li>Scope: Cluster provisioning, GitOps bridge configuration, addon management</li> <li>Workspaces: Environment-specific configurations (dev, staging, prod)</li> </ul>"},{"location":"getting_started/architecture/#gitops-layer","title":"GitOps Layer","text":""},{"location":"getting_started/architecture/#gitops-bridge","title":"GitOps Bridge","text":"<ul> <li>Purpose: Connects infrastructure provisioning with GitOps workflows</li> <li>Function: Injects cluster metadata and addon configurations into ArgoCD</li> <li>Metadata: Cluster information, addon enablement flags, repository references</li> </ul>"},{"location":"getting_started/architecture/#argocd","title":"ArgoCD","text":"<ul> <li>Purpose: GitOps continuous delivery engine</li> <li>Pattern: App of Apps pattern for hierarchical application management</li> <li>Features:</li> <li>Automated synchronization with Git repositories</li> <li>Multi-repository support</li> <li>Environment-specific application sets</li> <li>Drift detection and remediation</li> </ul>"},{"location":"getting_started/architecture/#application-layer","title":"Application Layer","text":""},{"location":"getting_started/architecture/#platform-applications","title":"Platform Applications","text":"<p>The platform includes several categories of applications:</p> <p>Observability Stack</p> <ul> <li>Victoria Metrics for monitoring and alerting</li> <li>Grafana for visualization</li> <li>Alloy for data collection</li> <li>Prometheus ecosystem compatibility</li> </ul> <p>Security &amp; Compliance</p> <ul> <li>Kyverno for policy enforcement</li> <li>Pod Security Standards</li> <li>Falco for runtime security</li> <li>Trivy for vulnerability scanning</li> </ul> <p>Developer Experience</p> <ul> <li>Backstage for developer portal</li> <li>ArgoCD Image Updater for automated image updates</li> <li>Argo Rollouts for progressive delivery</li> <li>Argo Workflows for pipeline orchestration</li> </ul> <p>Platform Utilities</p> <ul> <li>Reloader for automatic restarts</li> <li>Reflector for resource synchronization</li> <li>Cert-Manager for certificate management</li> <li>External Secrets Operator</li> </ul>"},{"location":"getting_started/install/","title":"DoKa Seca Installation","text":"<p>Welcome to DoKa Seca! This guide will help you set up the complete framework for bootstrapping cloud-native platforms using Kubernetes in Docker (Kind). DoKa Seca provides an opinionated, production-ready approach to rapidly deploying a full platform stack with GitOps, observability, and security built-in.</p>"},{"location":"getting_started/install/#prerequisites","title":"Prerequisites","text":"<p>Before installing DoKa Seca, ensure you have the following prerequisites installed on your system:</p>"},{"location":"getting_started/install/#required-tools","title":"Required Tools","text":"<ul> <li>Docker - Container runtime for Kind clusters</li> <li>Kind - Kubernetes in Docker for local clusters</li> <li>Terraform or OpenTofu - Infrastructure as Code tool</li> <li>kubectl - Kubernetes command-line tool</li> <li>Helm - Kubernetes package manager</li> <li>jq - JSON processor for configuration management</li> <li>Kustomize - Kubernetes configuration management</li> </ul>"},{"location":"getting_started/install/#optional-tools-recommended","title":"Optional Tools (Recommended)","text":"<ul> <li>k9s or Lens - Visual cluster inspection</li> <li>ArgoCD CLI - GitOps workflow management</li> <li>Cosign - Container image signing</li> <li>Velero - Backup and restore</li> </ul>"},{"location":"getting_started/install/#system-requirements","title":"System Requirements","text":"<ul> <li>OS: Linux, macOS, or Windows (with WSL2)</li> <li>Memory: Minimum 8GB RAM (16GB recommended)</li> <li>CPU: 4+ cores recommended</li> <li>Disk: 20GB+ free space</li> <li>Network: Internet access for pulling container images</li> </ul>"},{"location":"getting_started/install/#quick-start-installation","title":"Quick Start Installation","text":"<p>DoKa Seca provides a streamlined installation process that bootstraps your entire platform with a single command:</p>"},{"location":"getting_started/install/#1-fork-the-repository","title":"1. Fork the Repository","text":"<pre><code>git clone https://github.com/org/dokaseca-control-plane.git\ncd dokaseca-control-plane\n</code></pre>"},{"location":"getting_started/install/#2-bootstrap-your-platform","title":"2. Bootstrap Your Platform","text":"<p>Create a Kind cluster with the complete DoKa Seca platform stack:</p> <pre><code>./scripts/terraform.sh hub dev apply\n</code></pre> <p>This command will:</p> <ul> <li>Create a Kind cluster named <code>hub-dev</code></li> <li>Install the core platform components</li> <li>Configure GitOps workflows with ArgoCD (if enabled)</li> <li>Set up observability stack</li> <li>Apply security policies</li> </ul>"},{"location":"getting_started/install/#3-verify-installation","title":"3. Verify Installation","text":"<p>Check that your cluster is running:</p> <pre><code>kind get clusters\n# Output: hub-dev\n\nkubectl cluster-info\nkubectl get nodes\n</code></pre>"},{"location":"getting_started/install/#4-access-platform-components","title":"4. Access Platform Components","text":"<p>If GitOps is enabled (<code>enable_gitops_bridge = true</code> in terraform.tfvars), you can access various platform components:</p> <pre><code># Get ArgoCD admin password\nmake argo-cd-password\n\n# Port-forward to access ArgoCD UI\nkubectl port-forward svc/argocd-server -n argocd 8080:443\n# Access: https://localhost:8080\n</code></pre>"},{"location":"getting_started/install/#configuration-options","title":"Configuration Options","text":"<p>DoKa Seca supports various configuration options through <code>terraform.tfvars</code>:</p> <pre><code># Enable GitOps workflow\nenable_gitops_bridge = true\n\n# Configure cluster settings\ncluster_name = \"hub-dev\"\nenvironment = \"dev\"\n\n# Enable specific addons\nenable_argo_cd = true\nenable_victoria_metrics_k8s_stack = true\nenable_vault = false\n# ... additional addon configurations\n</code></pre>"},{"location":"getting_started/install/#platform-components","title":"Platform Components","text":"<p>Once installed, DoKa Seca provides:</p> <ul> <li>GitOps: ArgoCD for declarative application deployment</li> <li>Observability: Victoria Metrics stack for monitoring and alerting</li> <li>Security: Pod Security Standards and admission controllers</li> <li>Networking: Ingress controllers and service mesh (optional)</li> <li>Storage: Persistent volume management</li> <li>Backup: Velero for disaster recovery (optional)</li> </ul>"},{"location":"getting_started/install/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting_started/install/#increase-file-limits","title":"Increase File Limits","text":"<p>If you encounter \"too many open files\" errors:</p> <pre><code>sudo sysctl fs.inotify.max_user_watches=1048576\nsudo sysctl fs.inotify.max_user_instances=8192\n</code></pre>"},{"location":"getting_started/install/#clean-installation","title":"Clean Installation","text":"<p>To completely remove and reinstall:</p> <pre><code>./scripts/terraform.sh hub dev destroy\n# Wait for cleanup to complete\n./scripts/terraform.sh hub dev apply\n</code></pre>"},{"location":"getting_started/install/#next-steps","title":"Next Steps","text":"<p>After installation:</p> <ol> <li>Explore the Platform: Check the Architecture guide</li> <li>Deploy Applications: Learn about the Catalog</li> <li>Configure GitOps: Set up your Repository workflows</li> <li>Monitor: Access Observability dashboards</li> <li>Secure: Review Security policies</li> </ol> <p>For more detailed configuration and usage, continue to the Architecture section.</p>"},{"location":"getting_started/repo_structure/","title":"Repo Structure","text":""},{"location":"getting_started/repo_structure/#repository-structure","title":"Repository Structure","text":"<p>DoKa Seca follows a multi-repository GitOps pattern:</p>"},{"location":"getting_started/repo_structure/#control-plane-repository-this-repository","title":"Control Plane Repository (This Repository)","text":"<pre><code>dokaseca-control-plane/\n\u251c\u2500\u2500 terraform/           # Infrastructure as Code\n\u251c\u2500\u2500 charts/             # Helm charts for custom applications\n\u251c\u2500\u2500 gitops/             # GitOps configurations\n\u251c\u2500\u2500 docs/               # Documentation\n\u2514\u2500\u2500 scripts/            # Automation scripts\n</code></pre>"},{"location":"getting_started/repo_structure/#addons-repository","title":"Addons Repository","text":"<p>Contains ArgoCD ApplicationSets for platform addons:</p> <pre><code>dokaseca-addons/\n\u251c\u2500\u2500 appsets/            # ArgoCD ApplicationSets\n\u2514\u2500\u2500 values/             # Environment-specific values\n</code></pre>"},{"location":"getting_started/repo_structure/#clusters-repository","title":"Clusters Repository","text":"<p>Manages cluster-specific configurations:</p> <pre><code>dokaseca-clusters/\n\u251c\u2500\u2500 clusters/           # Per-cluster configurations\n\u251c\u2500\u2500 environments/       # Environment-specific settings\n\u2514\u2500\u2500 policies/           # Cluster policies\n</code></pre>"},{"location":"getting_started/repo_structure/#workloads-repository","title":"Workloads Repository","text":"<p>Contains application workloads:</p> <pre><code>dokaseca-workloads/\n\u251c\u2500\u2500 applications/       # Application manifests\n\u2514\u2500\u2500 environments/       # Environment-specific configs\n</code></pre>"},{"location":"getting_started/repo_structure/#multi-repository-architecture","title":"Multi-Repository Architecture","text":"<p>DoKa Seca is designed as a distributed system composed of multiple specialized repositories, each serving a specific purpose in the platform ecosystem. This approach provides clear separation of concerns, enables team autonomy, and supports scalable platform operations.</p>"},{"location":"getting_started/repo_structure/#core-repositories","title":"Core Repositories","text":""},{"location":"getting_started/repo_structure/#1-control-plane-repository-dokaseca-control-plane","title":"1. Control Plane Repository (<code>dokaseca-control-plane</code>)","text":"<p>Purpose: Central orchestration and infrastructure management</p> <p>Key Components:</p> <ul> <li>Terraform Modules: Infrastructure as Code for cluster provisioning</li> <li>GitOps Bridge: Integration layer between infrastructure and applications</li> <li>Documentation: Comprehensive platform documentation</li> <li>Scripts: Automation tools and utilities</li> </ul> <p>Responsibilities:</p> <ul> <li>Kubernetes cluster lifecycle management</li> <li>Platform infrastructure provisioning</li> <li>GitOps workflow orchestration</li> <li>Cross-repository coordination</li> <li>Platform documentation and guides</li> </ul> <pre><code>dokaseca-control-plane/\n\u251c\u2500\u2500 terraform/\n\u2502   \u251c\u2500\u2500 modules/         # Reusable Terraform modules\n\u2502   \u251c\u2500\u2500 environments/    # Environment-specific configurations\n\u2502   \u2514\u2500\u2500 providers.tf     # Cloud provider configurations\n\u251c\u2500\u2500 docs/               # Platform documentation\n\u2514\u2500\u2500 scripts/\n    \u251c\u2500\u2500 bootstrap.sh    # Platform bootstrap script\n    \u2514\u2500\u2500 utils/          # Utility scripts\n</code></pre>"},{"location":"getting_started/repo_structure/#2-addons-repository-dokaseca-addons","title":"2. Addons Repository (<code>dokaseca-addons</code>)","text":"<p>Purpose: Platform addon management and configuration</p> <p>Key Components:</p> <ul> <li>ApplicationSets: ArgoCD ApplicationSets for addon deployment</li> <li>Value Templates: Environment-specific addon configurations</li> </ul> <p>Responsibilities:</p> <ul> <li>Platform addon lifecycle management</li> <li>Environment-specific addon configuration</li> <li>Addon dependency management</li> <li>Platform capability enablement</li> </ul> <pre><code>dokaseca-addons/\n\u251c\u2500\u2500 appsets/\n\u2502   \u251c\u2500\u2500 observability/   # Monitoring and logging addons\n\u2502   \u251c\u2500\u2500 security/        # Security and compliance addons\n\u2502   \u251c\u2500\u2500 networking/      # Network and ingress addons\n\u2502   \u2514\u2500\u2500 developer-tools/ # Developer experience addons\n\u2514\u2500\u2500 values/\n    \u251c\u2500\u2500 dev/            # Development environment values\n    \u251c\u2500\u2500 staging/        # Staging environment values\n    \u2514\u2500\u2500 production/     # Production environment values\n</code></pre>"},{"location":"getting_started/repo_structure/#3-helm-charts-repository-helm-charts","title":"3. Helm Charts Repository (<code>helm-charts</code>)","text":"<p>Purpose: Centralized Helm chart library and distribution</p> <p>Key Components:</p> <ul> <li>Application Charts: Charts for common applications</li> <li>Library Charts: Reusable chart components and helpers</li> <li>Chart Testing: Automated chart validation and testing</li> <li>Chart Registry: Package distribution and versioning</li> </ul> <p>Responsibilities:</p> <ul> <li>Helm chart development and maintenance</li> <li>Chart versioning and release management</li> <li>Chart testing and quality assurance</li> <li>Chart distribution and packaging</li> </ul>"},{"location":"getting_started/repo_structure/#4-cluster-configuration-repository-dokaseca-clusters","title":"4. Cluster Configuration Repository (<code>dokaseca-clusters</code>)","text":"<p>Purpose: Cluster-specific configuration and policies</p> <p>Key Components:</p> <ul> <li>Cluster Manifests: Per-cluster Kubernetes configurations</li> <li>Environment Configs: Environment-specific settings</li> <li>Policy Definitions: Cluster governance and security policies</li> <li>Resource Quotas: Cluster resource management</li> </ul> <p>Responsibilities:</p> <ul> <li>Cluster-specific configuration management</li> <li>Environment isolation and configuration</li> <li>Cluster policy enforcement</li> <li>Resource allocation and quotas</li> </ul> <pre><code>dokaseca-clusters/\n\u251c\u2500\u2500 clusters/\n\u2502   \u251c\u2500\u2500 dev-cluster/     # Development cluster config\n\u2502   \u251c\u2500\u2500 staging-cluster/ # Staging cluster config\n\u2502   \u2514\u2500\u2500 prod-cluster/   # Production cluster config\n\u251c\u2500\u2500 environments/\n\u2502   \u251c\u2500\u2500 base/           # Base environment configuration\n\u2502   \u251c\u2500\u2500 overlays/       # Environment-specific overlays\n\u2502   \u2514\u2500\u2500 secrets/        # Environment secret templates\n\u2514\u2500\u2500 rbac/\n    \u251c\u2500\u2500 teams/            # Team-based RBAC\n    \u2514\u2500\u2500 service-accounts/ # Service account configurations\n</code></pre>"},{"location":"getting_started/repo_structure/#optional-repositories","title":"Optional Repositories","text":""},{"location":"getting_started/repo_structure/#5-backstage-setup-repository-dokaseca-backstage-optional","title":"5. Backstage Setup Repository (<code>dokaseca-backstage</code>) - Optional","text":"<p>Purpose: Developer portal configuration and customization</p> <p>Key Components:</p> <ul> <li>Backstage Configuration: Developer portal setup</li> <li>Custom Plugins: Organization-specific Backstage plugins</li> <li>Templates: Software scaffolding templates</li> <li>Catalog Definitions: Service catalog configurations</li> </ul> <p>Responsibilities:</p> <ul> <li>Developer portal deployment and configuration</li> <li>Custom plugin development and integration</li> <li>Service catalog management</li> <li>Developer experience customization</li> </ul>"},{"location":"getting_started/repo_structure/#6-software-templates-repository-dokaseca-templates-optional","title":"6. Software Templates Repository (<code>dokaseca-templates</code>) - Optional","text":"<p>Purpose: Software scaffolding and development templates</p> <p>Key Components:</p> <ul> <li>Application Templates: Microservice and application templates</li> <li>Infrastructure Templates: Infrastructure scaffolding templates</li> <li>Pipeline Templates: CI/CD pipeline templates</li> <li>Documentation Templates: Project documentation templates</li> </ul> <p>Responsibilities:</p> <ul> <li>Software project scaffolding</li> <li>Development best practices enforcement</li> <li>Standardized project structure</li> <li>Template lifecycle management</li> </ul>"},{"location":"getting_started/repo_structure/#repository-relationships","title":"Repository Relationships","text":""},{"location":"getting_started/repo_structure/#inter-repository-dependencies","title":"Inter-Repository Dependencies","text":"<pre><code>graph TB\n    CP[Control Plane] --&gt; A[Addons]\n    CP --&gt; CC[Cluster Config]\n    CP --&gt; C[Charts]\n    A --&gt; C\n    A --&gt; CC\n    BS[Backstage] -.-&gt; ST[Software Templates]\n    BS -.-&gt; CP\n    ST -.-&gt; A\n\n    subgraph \"Core Repositories\"\n        CP\n        A\n        C\n        CC\n    end\n\n    subgraph \"Optional Repositories\"\n        BS\n        ST\n    end</code></pre>"},{"location":"getting_started/repo_structure/#repository-access-patterns","title":"Repository Access Patterns","text":"<ul> <li>Control Plane orchestrates and references all other repositories</li> <li>Addons consumes charts and applies cluster configurations</li> <li>Cluster Config provides environment-specific overrides</li> <li>Charts serves as a shared library for all repositories</li> <li>Backstage (optional) integrates with templates and platform repos</li> <li>Software Templates (optional) consumed by Backstage and development teams</li> </ul>"},{"location":"getting_started/repo_structure/#configuration-strategy","title":"Configuration Strategy","text":""},{"location":"getting_started/repo_structure/#repository-configuration-variables","title":"Repository Configuration Variables","text":"<p>Each repository can be configured independently:</p> <pre><code># Control plane repository configuration\ngitops_control_plane_repo = \"https://github.com/org/dokaseca-control-plane\"\ngitops_control_plane_revision = \"main\"\n\n# Addons repository configuration\ngitops_addons_repo = \"https://github.com/org/dokaseca-addons\"\ngitops_addons_revision = \"v1.0.0\"\n\n# Charts repository configuration\ngitops_charts_repo = \"https://github.com/org/dokaseca-charts\"\ngitops_charts_revision = \"main\"\n\n# Cluster configuration repository\ngitops_clusters_repo = \"https://github.com/org/dokaseca-clusters\"\ngitops_clusters_revision = \"main\"\n\n# Optional: Backstage repository\ngitops_backstage_repo = \"https://github.com/org/dokaseca-backstage\"\ngitops_backstage_revision = \"main\"\n\n# Optional: Software templates repository\ngitops_templates_repo = \"https://github.com/org/dokaseca-templates\"\ngitops_templates_revision = \"main\"\n</code></pre>"},{"location":"getting_started/repo_structure/#repository-branching-strategy","title":"Repository Branching Strategy","text":"<ul> <li>main: Production-ready configurations</li> <li>develop: Integration and testing branch</li> <li>feature/*: Feature development branches</li> <li>release/*: Release preparation branches</li> <li>hotfix/*: Critical fixes for production</li> </ul>"},{"location":"getting_started/repo_structure/#benefits-of-multi-repository-architecture","title":"Benefits of Multi-Repository Architecture","text":"<ol> <li>Separation of Concerns: Each repository has a clear, focused responsibility</li> <li>Team Autonomy: Teams can work independently on their respective repositories</li> <li>Scalability: Repositories can be scaled and managed independently</li> <li>Security: Fine-grained access control per repository</li> <li>Versioning: Independent versioning and release cycles</li> <li>Flexibility: Optional repositories can be enabled based on organizational needs</li> <li>Reusability: Charts and templates can be shared across multiple platform instances</li> </ol>"},{"location":"getting_started/repo_structure/#data-flow","title":"Data Flow","text":""},{"location":"getting_started/repo_structure/#1-bootstrap-process","title":"1. Bootstrap Process","text":"<pre><code>Developer \u2192 terraform apply \u2192 Kind cluster creation \u2192 GitOps Bridge \u2192 ArgoCD installation \u2192 App of Apps deployment\n</code></pre>"},{"location":"getting_started/repo_structure/#2-gitops-sync-process","title":"2. GitOps Sync Process","text":"<pre><code>Git commit \u2192 ArgoCD detection \u2192 Manifest retrieval \u2192 Kubernetes API \u2192 Resource creation/update\n</code></pre>"},{"location":"getting_started/repo_structure/#3-addon-management","title":"3. Addon Management","text":"<pre><code>Terraform variables \u2192 GitOps Bridge metadata \u2192 ArgoCD cluster secret \u2192 ApplicationSet evaluation \u2192 Addon deployment\n</code></pre>"},{"location":"getting_started/repo_structure/#configuration-management","title":"Configuration Management","text":""},{"location":"getting_started/repo_structure/#environment-strategy","title":"Environment Strategy","text":"<ul> <li>Development: Full feature enablement for testing</li> <li>Staging: Production-like configuration for validation</li> <li>Production: Optimized, secure, and monitored deployment</li> </ul>"},{"location":"getting_started/repo_structure/#addon-configuration","title":"Addon Configuration","text":"<p>Addons are managed through Terraform variables:</p> <pre><code>addons = {\n  enable_argo_cd = true\n  enable_victoria_metrics_k8s_stack = true\n  enable_kyverno = true\n  enable_backstage = false\n}\n</code></pre>"},{"location":"getting_started/repo_structure/#git-repository-configuration","title":"Git Repository Configuration","text":"<p>Repository references are configurable per environment:</p> <pre><code>gitops_addons_repo = \"dokaseca-addons\"\ngitops_addons_revision = \"main\"\ngitops_workload_repo = \"dokaseca-workloads\"\ngitops_workload_revision = \"dev\"\n</code></pre>"},{"location":"getting_started/repo_structure/#integration-points","title":"Integration Points","text":""},{"location":"getting_started/repo_structure/#external-systems","title":"External Systems","text":"<ul> <li>GitHub: Source code and GitOps repositories</li> <li>Container Registries: Image storage and distribution</li> <li>Identity Providers: Authentication and authorization</li> <li>Monitoring Systems: External observability integration</li> </ul>"},{"location":"getting_started/repo_structure/#development-workflow","title":"Development Workflow","text":"<ul> <li>Local Development: Kind-based development clusters</li> <li>CI/CD Integration: GitHub Actions for automation</li> <li>Testing: Automated testing with platform validation</li> <li>Deployment: GitOps-driven continuous delivery</li> </ul>"},{"location":"getting_started/teams/","title":"Teams and Roles","text":""},{"location":"getting_started/teams/#overview","title":"Overview","text":"<p>Modern platform engineering initiatives typically involve three core roles that work together to deliver and maintain cloud-native platforms:</p> <ul> <li>Platform Engineers: Design and build the platform infrastructure and developer experience</li> <li>DevOps Engineers: Bridge development and operations with automation and reliability practices</li> <li>Developers: Build and deploy applications using the platform's capabilities</li> </ul> <p>Each role has distinct responsibilities, but successful platform engineering requires close collaboration and shared understanding of goals.</p>"},{"location":"getting_started/teams/#platform-engineer","title":"Platform Engineer","text":"<p>Platform engineers are responsible for building and maintaining the underlying platform infrastructure that enables developer productivity and operational excellence.</p>"},{"location":"getting_started/teams/#platform-engineer-responsibilities","title":"Platform Engineer Responsibilities","text":""},{"location":"getting_started/teams/#infrastructure-design-and-implementation","title":"Infrastructure Design and Implementation","text":"<ul> <li>Design platform architecture and technology stack decisions</li> <li>Implement Infrastructure as Code (IaC) using tools like Terraform and Crossplane</li> <li>Manage Kubernetes clusters and container orchestration</li> <li>Design and implement CI/CD pipelines and GitOps workflows</li> <li>Establish networking, security, and observability foundations</li> </ul>"},{"location":"getting_started/teams/#developer-experience-dx","title":"Developer Experience (DX)","text":"<ul> <li>Build self-service capabilities and developer portals (e.g., Backstage)</li> <li>Create standardized templates and scaffolding tools</li> <li>Design APIs and abstractions that hide infrastructure complexity</li> <li>Implement golden paths for common development workflows</li> <li>Provide documentation and training materials</li> </ul>"},{"location":"getting_started/teams/#platform-operations","title":"Platform Operations","text":"<ul> <li>Monitor platform health and performance metrics</li> <li>Implement platform SLAs and reliability measures</li> <li>Manage platform upgrades and lifecycle management</li> <li>Establish backup and disaster recovery procedures</li> <li>Handle platform-level incident response and troubleshooting</li> </ul>"},{"location":"getting_started/teams/#governance-and-standards","title":"Governance and Standards","text":"<ul> <li>Define platform policies and compliance requirements</li> <li>Implement security controls and access management</li> <li>Establish cost optimization and resource management practices</li> <li>Create platform roadmaps and technical decision records</li> <li>Ensure platform scalability and maintainability</li> </ul>"},{"location":"getting_started/teams/#required-skills","title":"Required Skills","text":""},{"location":"getting_started/teams/#technical-skills","title":"Technical Skills","text":"<ul> <li>Deep Kubernetes and container technology expertise</li> <li>Infrastructure as Code (Terraform, Crossplane, Helm)</li> <li>Cloud platform knowledge (AWS, Azure, GCP)</li> <li>GitOps and CI/CD pipeline design</li> <li>Observability and monitoring systems</li> <li>Security and compliance frameworks</li> <li>Programming skills (Go, Python, shell scripting)</li> </ul>"},{"location":"getting_started/teams/#soft-skills","title":"Soft Skills","text":"<ul> <li>System thinking and architectural design</li> <li>Cross-team collaboration and communication</li> <li>Product mindset for internal platforms</li> <li>Technical writing and documentation</li> <li>Mentoring and knowledge sharing</li> </ul>"},{"location":"getting_started/teams/#doka-seca-platform-engineer-workflow","title":"DoKa Seca Platform Engineer Workflow","text":"<pre><code>graph TD\n    A[Design Platform Architecture] --&gt; B[Implement Infrastructure]\n    B --&gt; C[Configure GitOps Workflows]\n    C --&gt; D[Setup Developer Portal]\n    D --&gt; E[Define Golden Paths]\n    E --&gt; F[Monitor Platform Health]\n    F --&gt; G[Gather Developer Feedback]\n    G --&gt; A</code></pre>"},{"location":"getting_started/teams/#devops-engineer","title":"DevOps Engineer","text":"<p>DevOps engineers focus on automating delivery pipelines, ensuring system reliability, and bridging the gap between development and operations teams.</p>"},{"location":"getting_started/teams/#core-responsibilities","title":"Core Responsibilities","text":"<p>Automation and Tooling</p> <ul> <li>Build and maintain CI/CD pipelines for applications</li> <li>Implement automated testing and quality gates</li> <li>Create deployment automation and rollback procedures</li> <li>Develop infrastructure automation and configuration management</li> <li>Implement secrets management and security scanning</li> </ul> <p>Reliability Engineering</p> <ul> <li>Design and implement monitoring and alerting systems</li> <li>Establish SLOs/SLIs for applications and services</li> <li>Implement chaos engineering and resilience testing</li> <li>Perform capacity planning and performance optimization</li> <li>Handle incident response and post-mortem processes</li> </ul> <p>Security and Compliance</p> <ul> <li>Implement security scanning in pipelines</li> <li>Manage vulnerability assessments and remediation</li> <li>Ensure compliance with organizational policies</li> <li>Implement access controls and identity management</li> <li>Conduct security reviews and threat modeling</li> </ul> <p>Collaboration and Process</p> <ul> <li>Work with developers to optimize deployment workflows</li> <li>Collaborate with platform engineers on infrastructure requirements</li> <li>Establish development and deployment best practices</li> <li>Provide training on DevOps tools and practices</li> <li>Participate in architecture and design reviews</li> </ul>"},{"location":"getting_started/teams/#required-skills_1","title":"Required Skills","text":"<p>Technical Skills</p> <ul> <li>CI/CD pipeline tools (Jenkins, GitLab CI, GitHub Actions)</li> <li>Configuration management (Ansible, Chef, Puppet)</li> <li>Monitoring and observability (Prometheus, Grafana, ELK stack)</li> <li>Scripting and automation (Python, Bash, PowerShell)</li> <li>Container technologies and orchestration</li> <li>Cloud services and APIs</li> <li>Security tools and practices</li> </ul> <p>Soft Skills</p> <ul> <li>Problem-solving and analytical thinking</li> <li>Collaboration across development and operations teams</li> <li>Process improvement and optimization mindset</li> <li>Incident management and crisis communication</li> <li>Continuous learning and adaptation</li> </ul>"},{"location":"getting_started/teams/#doka-seca-devops-engineer-workflow","title":"DoKa Seca DevOps Engineer Workflow","text":"<pre><code>graph TD\n    A[Define Pipeline Requirements] --&gt; B[Implement CI/CD Pipelines]\n    B --&gt; C[Configure Monitoring]\n    C --&gt; D[Setup Security Scanning]\n    D --&gt; E[Deploy Applications]\n    E --&gt; F[Monitor Performance]\n    F --&gt; G[Handle Incidents]\n    G --&gt; H[Optimize Processes]\n    H --&gt; A</code></pre>"},{"location":"getting_started/teams/#developer","title":"Developer","text":"<p>Developers focus on building applications and services using the platform's capabilities, following established patterns and practices.</p>"},{"location":"getting_started/teams/#core-responsibilities_1","title":"Core Responsibilities","text":"<p>Application Development</p> <ul> <li>Build applications using platform-provided templates and tools</li> <li>Follow established coding standards and best practices</li> <li>Implement application-level monitoring and logging</li> <li>Write comprehensive tests (unit, integration, e2e)</li> <li>Document application architecture and APIs</li> </ul> <p>Platform Adoption</p> <ul> <li>Use platform self-service capabilities and developer portal</li> <li>Follow golden paths and established deployment patterns</li> <li>Leverage platform-provided services and abstractions</li> <li>Provide feedback on developer experience and tooling</li> <li>Participate in platform evolution discussions</li> </ul> <p>Operational Awareness</p> <ul> <li>Understand application deployment and runtime characteristics</li> <li>Monitor application performance and user experience</li> <li>Participate in incident response for application issues</li> <li>Implement proper error handling and resilience patterns</li> <li>Consider security implications in application design</li> </ul> <p>Collaboration</p> <ul> <li>Work with DevOps engineers on deployment strategies</li> <li>Collaborate with platform engineers on requirements</li> <li>Share knowledge and best practices with team members</li> <li>Participate in code reviews and technical discussions</li> <li>Contribute to platform documentation and examples</li> </ul>"},{"location":"getting_started/teams/#required-skills_2","title":"Required Skills","text":"<p>Technical Skills</p> <ul> <li>Programming languages and frameworks</li> <li>Application architecture and design patterns</li> <li>Testing frameworks and methodologies</li> <li>API design and integration</li> <li>Database technologies and data modeling</li> <li>Frontend and/or backend development expertise</li> <li>Basic understanding of containers and Kubernetes</li> </ul> <p>Soft Skills</p> <ul> <li>Problem-solving and debugging abilities</li> <li>Collaboration and teamwork</li> <li>User-focused thinking and empathy</li> <li>Adaptability to new tools and processes</li> <li>Communication of technical concepts</li> </ul>"},{"location":"getting_started/teams/#doka-seca-developer-workflow","title":"DoKa Seca Developer Workflow","text":"<pre><code>graph TD\n    A[Access Developer Portal] --&gt; B[Choose Application Template]\n    B --&gt; C[Develop Application]\n    C --&gt; D[Run Local Tests]\n    D --&gt; E[Commit to Git]\n    E --&gt; F[Automated CI/CD Pipeline]\n    F --&gt; G[Monitor Application]\n    G --&gt; H[Gather User Feedback]\n    H --&gt; C</code></pre>"},{"location":"getting_started/teams/#team-collaboration-patterns","title":"Team Collaboration Patterns","text":"<p>Platform Team Metrics</p> <ul> <li>Platform uptime and reliability (99.9%+ SLA)</li> <li>Developer onboarding time (&lt; 1 day)</li> <li>Self-service adoption rate (80%+)</li> <li>Mean time to recovery (MTTR) for platform issues</li> </ul> <p>Developer Team Metrics</p> <ul> <li>Application deployment frequency</li> <li>Lead time from commit to production</li> <li>Application reliability and performance</li> <li>Developer satisfaction scores</li> </ul>"},{"location":"getting_started/teams/#doka-seca-team-support","title":"DoKa Seca Team Support","text":"<p>DoKa Seca provides tools and patterns to support effective team collaboration:</p>"},{"location":"getting_started/teams/#for-platform-engineers","title":"For Platform Engineers","text":"<ul> <li>Crossplane compositions for standardized infrastructure</li> <li>ArgoCD applications for GitOps workflows</li> <li>Backstage catalog for service discovery and documentation</li> <li>Monitoring stack with Prometheus, Grafana, and alerting</li> </ul>"},{"location":"getting_started/teams/#for-devops-engineers","title":"For DevOps Engineers","text":"<ul> <li>CI/CD templates with GitHub Actions and GitLab CI</li> <li>Security scanning with Cosign and policy enforcement</li> <li>Observability tools for monitoring and troubleshooting</li> <li>Infrastructure automation with Terraform and Helm</li> </ul>"},{"location":"getting_started/teams/#for-developers","title":"For Developers","text":"<ul> <li>Developer portal with Backstage for self-service capabilities</li> <li>Application templates for quick project scaffolding</li> <li>Golden paths for common deployment patterns</li> <li>Documentation and examples for platform usage</li> </ul>"},{"location":"getting_started/teams/#best-practices","title":"Best Practices","text":""},{"location":"getting_started/teams/#cross-team-collaboration","title":"Cross-Team Collaboration","text":"<ol> <li>Establish Clear Interfaces: Define APIs and contracts between teams</li> <li>Implement Feedback Loops: Regular communication and feedback mechanisms</li> <li>Share Responsibility: Platform and application teams share ownership of outcomes</li> <li>Invest in Documentation: Comprehensive, up-to-date documentation for all stakeholders</li> <li>Measure and Improve: Continuous measurement and improvement of team effectiveness</li> </ol>"},{"location":"getting_started/teams/#platform-adoption","title":"Platform Adoption","text":"<ol> <li>Start Small: Begin with pilot projects and gradually expand</li> <li>Provide Training: Invest in training and support for platform adoption</li> <li>Gather Feedback: Continuously collect and act on user feedback</li> <li>Iterate Quickly: Rapid iteration based on user needs and feedback</li> <li>Celebrate Success: Recognize and celebrate platform adoption wins</li> </ol> <p>The success of platform engineering initiatives depends on effective collaboration between these roles, clear communication, and a shared commitment to improving developer productivity and operational excellence.</p>"}]}